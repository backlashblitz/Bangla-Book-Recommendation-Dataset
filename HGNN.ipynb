{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082e26e-d0f1-48aa-b24f-2b2aa9827d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "#pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1257-f807-4d8d-a579-26d4ef9bed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Mock DGL Graphbolt\n",
    "# ==============================================================================\n",
    "import sys\n",
    "from unittest.mock import MagicMock\n",
    "sys.modules['dgl.graphbolt'] = MagicMock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3520a72-e748-4859-868f-1595ebe51f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 2: Imports & GPU Setup ==========\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GPU Memory Management\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.2f}GB total\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "# Import DGL after torch\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv, GraphConv\n",
    "print(f\"DGL version: {dgl.__version__}\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e822f3-8d64-4ebc-9d04-231b5bca04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 3: Configuration ==========\n",
    "class Config:\n",
    "    \"\"\"Configuration for FULL DATA on RTX 4060 Laptop GPU (8GB VRAM)\"\"\"\n",
    "    \n",
    "    # [!] UPDATE THIS PATH!\n",
    "    BASE_INPUT_PATH = 'E:/RokomariBG_Dataset'\n",
    "    \n",
    "    # === FULL DATA SETTINGS ===\n",
    "    MAX_BOOKS = None  # Use ALL books\n",
    "    MAX_AUTHORS = None  # Use ALL authors\n",
    "    MAX_USERS = None  # Use ALL users\n",
    "    MAX_CATEGORIES = None  # Use ALL categories\n",
    "    MAX_PUBLISHERS = None  # Use ALL publishers\n",
    "    \n",
    "    # === MODEL SETTINGS ===\n",
    "    EMBED_DIM = 64\n",
    "    HIDDEN_DIM = 128\n",
    "    OUT_DIM = 64\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    # === TRAINING SETTINGS ===\n",
    "    LEARNING_RATE = 0.001\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 512\n",
    "    NEG_SAMPLES = 4\n",
    "    \n",
    "    # === EARLY STOPPING ===\n",
    "    PATIENCE = 15\n",
    "    MIN_DELTA = 0.0005\n",
    "    \n",
    "    # === SPLIT RATIOS ===\n",
    "    TRAIN_RATIO = 0.70\n",
    "    VAL_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "    \n",
    "    # === MEMORY MANAGEMENT ===\n",
    "    EVAL_EVERY = 2\n",
    "    CLEAR_CACHE_EVERY = 3\n",
    "    \n",
    "    # === OUTPUT ===\n",
    "    OUTPUT_DIR = 'outputs'\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define paths\n",
    "paths = {\n",
    "    'book': f'{config.BASE_INPUT_PATH}/book.json',\n",
    "    'author': f'{config.BASE_INPUT_PATH}/author.json',\n",
    "    'category': f'{config.BASE_INPUT_PATH}/category.json',\n",
    "    'publisher': f'{config.BASE_INPUT_PATH}/publisher.json',\n",
    "    'review': f'{config.BASE_INPUT_PATH}/review.json',\n",
    "    'book_to_author': f'{config.BASE_INPUT_PATH}/book_to_author.json',\n",
    "    'book_to_publisher': f'{config.BASE_INPUT_PATH}/book_to_publisher.json',\n",
    "    'book_to_category': f'{config.BASE_INPUT_PATH}/book_to_category.json',\n",
    "    'author_to_category': f'{config.BASE_INPUT_PATH}/author_to_category.json',\n",
    "    'author_to_publisher': f'{config.BASE_INPUT_PATH}/author_to_publisher.json',\n",
    "    'publisher_to_category': f'{config.BASE_INPUT_PATH}/publisher_to_category.json',\n",
    "    'user_to_review': f'{config.BASE_INPUT_PATH}/user_to_review.json',\n",
    "    'book_to_review': f'{config.BASE_INPUT_PATH}/book_to_review.json',\n",
    "}\n",
    "\n",
    "print(\"[OK] Configuration loaded\")\n",
    "print(f\"Data path: {config.BASE_INPUT_PATH}\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3e31-dfc2-4f58-92cd-cf0a9c9adf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 4: Data Loading Functions ==========\n",
    "def load_json_file(filepath):\n",
    "    \"\"\"Load a single JSON file\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"[OK] Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcb78b-afe1-4590-8813-ba5a168d8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 5: Data Loader Class (NO LEAKAGE - WITH REVIEWS) ==========\n",
    "class HGNNDataLoader:\n",
    "    \"\"\"Data loader - graph built ONLY from training edges (NO LEAKAGE)\"\"\"\n",
    "    \n",
    "    def __init__(self, paths, config):\n",
    "        self.paths = paths\n",
    "        self.config = config\n",
    "        self.entity_maps = {}\n",
    "        self.reverse_maps = {}\n",
    "        self.entity_data = {}\n",
    "        self.edge_data = {}\n",
    "    \n",
    "    def load_entity_metadata(self):\n",
    "        \"\"\"Load entity metadata\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"LOADING ENTITY METADATA\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load books\n",
    "        print(\"\\n1. Loading books...\")\n",
    "        self.entity_data['book'] = load_json_file(self.paths['book']) or []\n",
    "        print(f\" [OK] {len(self.entity_data['book']):,} books loaded (may contain duplicates)\")\n",
    "        \n",
    "        # Load authors\n",
    "        print(\"\\n2. Loading authors...\")\n",
    "        self.entity_data['author'] = load_json_file(self.paths['author']) or []\n",
    "        print(f\" [OK] {len(self.entity_data['author']):,} authors loaded\")\n",
    "        \n",
    "        # Load categories\n",
    "        print(\"\\n3. Loading categories...\")\n",
    "        self.entity_data['category'] = load_json_file(self.paths['category']) or []\n",
    "        print(f\" [OK] {len(self.entity_data['category']):,} categories loaded\")\n",
    "        \n",
    "        # Load publishers\n",
    "        print(\"\\n4. Loading publishers...\")\n",
    "        self.entity_data['publisher'] = load_json_file(self.paths['publisher']) or []\n",
    "        print(f\" [OK] {len(self.entity_data['publisher']):,} publishers loaded\")\n",
    "        \n",
    "        # Load reviews\n",
    "        print(\"\\n5. Loading reviews...\")\n",
    "        self.entity_data['review'] = load_json_file(self.paths['review']) or []\n",
    "        print(f\" [OK] {len(self.entity_data['review']):,} reviews loaded\")\n",
    "        \n",
    "        # Create mappings\n",
    "        self._create_entity_mappings()\n",
    "    \n",
    "    def _create_entity_mappings(self):\n",
    "        \"\"\"Create ID mappings - DEDUPLICATES properly\"\"\"\n",
    "        \n",
    "        def create_mapping(data_list, id_key):\n",
    "            \"\"\"Generic mapping with deduplication\"\"\"\n",
    "            seen_ids = set()\n",
    "            unique_items = []\n",
    "            for item in data_list:\n",
    "                entity_id = str(item.get(id_key, ''))\n",
    "                if entity_id and entity_id not in seen_ids:\n",
    "                    seen_ids.add(entity_id)\n",
    "                    unique_items.append(item)\n",
    "            \n",
    "            entity_map = {}\n",
    "            reverse_map = {}\n",
    "            for idx, item in enumerate(unique_items):\n",
    "                entity_id = str(item.get(id_key, ''))\n",
    "                entity_map[entity_id] = idx\n",
    "                reverse_map[idx] = entity_id\n",
    "            \n",
    "            return entity_map, reverse_map, unique_items\n",
    "        \n",
    "        print(\"\\nDeduplicating and creating entity mappings...\")\n",
    "        \n",
    "        # Update entity_data with deduplicated items\n",
    "        self.entity_maps['book'], self.reverse_maps['book'], self.entity_data['book'] = create_mapping(\n",
    "            self.entity_data['book'], 'book_id')\n",
    "        self.entity_maps['author'], self.reverse_maps['author'], self.entity_data['author'] = create_mapping(\n",
    "            self.entity_data['author'], 'author_id')\n",
    "        self.entity_maps['category'], self.reverse_maps['category'], self.entity_data['category'] = create_mapping(\n",
    "            self.entity_data['category'], 'category_id')\n",
    "        self.entity_maps['publisher'], self.reverse_maps['publisher'], self.entity_data['publisher'] = create_mapping(\n",
    "            self.entity_data['publisher'], 'publisher_id')\n",
    "        self.entity_maps['review'], self.reverse_maps['review'], self.entity_data['review'] = create_mapping(\n",
    "            self.entity_data['review'], 'review_id')\n",
    "        \n",
    "        # For users: extract from user_to_review\n",
    "        print(\"Extracting user IDs...\")\n",
    "        user_to_review = load_json_file(self.paths['user_to_review']) or []\n",
    "        unique_users = sorted(set(str(item.get('user_id', '')) for item in user_to_review if item.get('user_id')))\n",
    "        self.entity_maps['user'] = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "        self.reverse_maps['user'] = {idx: uid for uid, idx in self.entity_maps['user'].items()}\n",
    "        \n",
    "        print(f\"\\nâœ… Deduplicated Entity Counts:\")\n",
    "        print(f\"   Users:      {len(self.entity_maps['user']):,}\")\n",
    "        print(f\"   Books:      {len(self.entity_maps['book']):,} (unique)\")\n",
    "        print(f\"   Authors:    {len(self.entity_maps['author']):,}\")\n",
    "        print(f\"   Categories: {len(self.entity_maps['category']):,}\")\n",
    "        print(f\"   Publishers: {len(self.entity_maps['publisher']):,}\")\n",
    "        print(f\"   Reviews:    {len(self.entity_maps['review']):,}\")\n",
    "    \n",
    "    def load_edge_data(self):\n",
    "        \"\"\"Load ALL edge data (user-book interactions extracted)\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"LOADING EDGE DATA (FULL DATASET)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # User-Review-Book interactions\n",
    "        print(\"\\n1. Building User-Book interactions (via reviews)...\")\n",
    "        user_to_review = load_json_file(self.paths['user_to_review']) or []\n",
    "        book_to_review = load_json_file(self.paths['book_to_review']) or []\n",
    "        \n",
    "        # Create review-to-book mapping\n",
    "        print(\"   Creating review-to-book mapping...\")\n",
    "        review_to_book = {}\n",
    "        for item in book_to_review:\n",
    "            review_id = str(item.get('review_id', ''))\n",
    "            book_id = str(item.get('book_id', ''))\n",
    "            if review_id and book_id and book_id in self.entity_maps['book']:\n",
    "                review_to_book[review_id] = book_id\n",
    "        print(f\"   [OK] {len(review_to_book):,} review-to-book mappings\")\n",
    "        \n",
    "        # Create review-to-rating mapping\n",
    "        print(\"   Creating review-to-rating mapping...\")\n",
    "        review_to_rating = {}\n",
    "        for review in self.entity_data['review']:\n",
    "            review_id = str(review.get('review_id', ''))\n",
    "            rating = review.get('user_rating', None)\n",
    "            if rating is not None:\n",
    "                try:\n",
    "                    rating = float(rating)\n",
    "                except (ValueError, TypeError):\n",
    "                    rating = 5.0\n",
    "            else:\n",
    "                rating = 5.0\n",
    "            review_to_rating[review_id] = rating\n",
    "        print(f\"   [OK] {len(review_to_rating):,} review-to-rating mappings\")\n",
    "        \n",
    "        # Build DEDUPLICATED user-book pairs\n",
    "        print(\"   Building user-book pairs (deduplicating)...\")\n",
    "        user_book_set = set()\n",
    "        user_book_list = []\n",
    "        \n",
    "        for item in tqdm(user_to_review, desc=\"   Processing\", leave=False):\n",
    "            user_id = str(item.get('user_id', ''))\n",
    "            review_id = str(item.get('review_id', ''))\n",
    "            \n",
    "            if user_id in self.entity_maps['user'] and review_id in review_to_book:\n",
    "                book_id = review_to_book[review_id]\n",
    "                user_idx = self.entity_maps['user'][user_id]\n",
    "                book_idx = self.entity_maps['book'][book_id]\n",
    "                \n",
    "                # Only add if (user, book) pair is unique\n",
    "                pair = (user_idx, book_idx)\n",
    "                if pair not in user_book_set:\n",
    "                    user_book_set.add(pair)\n",
    "                    rating = review_to_rating.get(review_id, 5.0)\n",
    "                    user_book_list.append((user_idx, book_idx, rating))\n",
    "        \n",
    "        print(f\" âœ… {len(user_book_list):,} UNIQUE user-book interactions (NO FILTERING)\")\n",
    "        \n",
    "        # Convert to edge data format\n",
    "        self.edge_data['user_book'] = {\n",
    "            'user_indices': np.array([p[0] for p in user_book_list]),\n",
    "            'book_indices': np.array([p[1] for p in user_book_list]),\n",
    "            'ratings': np.array([p[2] for p in user_book_list])\n",
    "        }\n",
    "        \n",
    "        # Helper for loading bidirectional metadata edges\n",
    "        def load_bidir_edges(filepath, src_key, dst_key, src_type, dst_type, edge_name):\n",
    "            data = load_json_file(filepath) or []\n",
    "            src_list, dst_list = [], []\n",
    "            for item in data:\n",
    "                src_id = str(item.get(src_key, ''))\n",
    "                dst_id = str(item.get(dst_key, ''))\n",
    "                if src_id in self.entity_maps[src_type] and dst_id in self.entity_maps[dst_type]:\n",
    "                    src_list.append(self.entity_maps[src_type][src_id])\n",
    "                    dst_list.append(self.entity_maps[dst_type][dst_id])\n",
    "            print(f\" [OK] {len(src_list):,} {edge_name} edges\")\n",
    "            return np.array(src_list), np.array(dst_list)\n",
    "        \n",
    "        # Book-Author\n",
    "        print(\"\\n2. Loading book-author edges...\")\n",
    "        b_src, a_dst = load_bidir_edges(\n",
    "            self.paths['book_to_author'], 'book_id', 'author_id',\n",
    "            'book', 'author', 'book-author'\n",
    "        )\n",
    "        self.edge_data['book_author'] = (b_src, a_dst)\n",
    "        self.edge_data['author_book'] = (a_dst, b_src)\n",
    "        \n",
    "        # Book-Publisher\n",
    "        print(\"\\n3. Loading book-publisher edges...\")\n",
    "        b_src, p_dst = load_bidir_edges(\n",
    "            self.paths['book_to_publisher'], 'book_id', 'publisher_id',\n",
    "            'book', 'publisher', 'book-publisher'\n",
    "        )\n",
    "        self.edge_data['book_publisher'] = (b_src, p_dst)\n",
    "        self.edge_data['publisher_book'] = (p_dst, b_src)\n",
    "        \n",
    "        # Book-Category\n",
    "        print(\"\\n4. Loading book-category edges...\")\n",
    "        b_src, c_dst = load_bidir_edges(\n",
    "            self.paths['book_to_category'], 'book_id', 'category_id',\n",
    "            'book', 'category', 'book-category'\n",
    "        )\n",
    "        self.edge_data['book_category'] = (b_src, c_dst)\n",
    "        self.edge_data['category_book'] = (c_dst, b_src)\n",
    "        \n",
    "        # Author-Category\n",
    "        print(\"\\n5. Loading author-category edges...\")\n",
    "        a_src, c_dst = load_bidir_edges(\n",
    "            self.paths['author_to_category'], 'author_id', 'category_id',\n",
    "            'author', 'category', 'author-category'\n",
    "        )\n",
    "        self.edge_data['author_category'] = (a_src, c_dst)\n",
    "        self.edge_data['category_author'] = (c_dst, a_src)\n",
    "        \n",
    "        # Author-Publisher\n",
    "        print(\"\\n6. Loading author-publisher edges...\")\n",
    "        a_src, p_dst = load_bidir_edges(\n",
    "            self.paths['author_to_publisher'], 'author_id', 'publisher_id',\n",
    "            'author', 'publisher', 'author-publisher'\n",
    "        )\n",
    "        self.edge_data['author_publisher'] = (a_src, p_dst)\n",
    "        self.edge_data['publisher_author'] = (p_dst, a_src)\n",
    "        \n",
    "        # Publisher-Category\n",
    "        print(\"\\n7. Loading publisher-category edges...\")\n",
    "        p_src, c_dst = load_bidir_edges(\n",
    "            self.paths['publisher_to_category'], 'publisher_id', 'category_id',\n",
    "            'publisher', 'category', 'publisher-category'\n",
    "        )\n",
    "        self.edge_data['publisher_category'] = (p_src, c_dst)\n",
    "        self.edge_data['category_publisher'] = (c_dst, p_src)\n",
    "        \n",
    "        # User-Review and Review-Book edges for graph\n",
    "        print(\"\\n8. Loading user-review edges...\")\n",
    "        user_src, review_dst = [], []\n",
    "        for item in user_to_review:\n",
    "            user_id = str(item.get('user_id', ''))\n",
    "            review_id = str(item.get('review_id', ''))\n",
    "            if user_id in self.entity_maps['user'] and review_id in self.entity_maps['review']:\n",
    "                user_src.append(self.entity_maps['user'][user_id])\n",
    "                review_dst.append(self.entity_maps['review'][review_id])\n",
    "        self.edge_data['user_review'] = (np.array(user_src), np.array(review_dst))\n",
    "        self.edge_data['review_user'] = (np.array(review_dst), np.array(user_src))\n",
    "        print(f\" [OK] {len(user_src):,} user-review edges\")\n",
    "        \n",
    "        print(\"\\n9. Loading review-book edges...\")\n",
    "        review_src, book_dst = [], []\n",
    "        for item in book_to_review:\n",
    "            review_id = str(item.get('review_id', ''))\n",
    "            book_id = str(item.get('book_id', ''))\n",
    "            if review_id in self.entity_maps['review'] and book_id in self.entity_maps['book']:\n",
    "                review_src.append(self.entity_maps['review'][review_id])\n",
    "                book_dst.append(self.entity_maps['book'][book_id])\n",
    "        self.edge_data['review_book'] = (np.array(review_src), np.array(book_dst))\n",
    "        self.edge_data['book_review'] = (np.array(book_dst), np.array(review_src))\n",
    "        print(f\" [OK] {len(review_src):,} review-book edges\")\n",
    "        \n",
    "        return self.edge_data\n",
    "    \n",
    "    def build_heterograph(self, train_user_reviews, train_review_books):\n",
    "        \"\"\"Build graph using ONLY training edges (NO LEAKAGE)\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"BUILDING HETEROGENEOUS GRAPH (TRAINING EDGES ONLY)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        graph_data = {}\n",
    "        \n",
    "        # USER-REVIEW edges: TRAINING ONLY (NO LEAKAGE)\n",
    "        print(\"\\nðŸ“Š User-Review edges: TRAINING ONLY (no val/test)\")\n",
    "        graph_data[('user', 'writes', 'review')] = (\n",
    "            train_user_reviews['user'],\n",
    "            train_user_reviews['review']\n",
    "        )\n",
    "        graph_data[('review', 'written_by', 'user')] = (\n",
    "            train_user_reviews['review'],\n",
    "            train_user_reviews['user']\n",
    "        )\n",
    "        print(f\"   âœ“ {len(train_user_reviews['user']):,} training user-review edges\")\n",
    "        \n",
    "        # REVIEW-BOOK edges: TRAINING ONLY (NO LEAKAGE)\n",
    "        print(\"\\nðŸ“Š Review-Book edges: TRAINING ONLY (no val/test)\")\n",
    "        graph_data[('review', 'reviews', 'book')] = (\n",
    "            train_review_books['review'],\n",
    "            train_review_books['book']\n",
    "        )\n",
    "        graph_data[('book', 'reviewed_by', 'review')] = (\n",
    "            train_review_books['book'],\n",
    "            train_review_books['review']\n",
    "        )\n",
    "        print(f\"   âœ“ {len(train_review_books['review']):,} training review-book edges\")\n",
    "        \n",
    "        # All other metadata edges (no leakage concern)\n",
    "        print(\"\\nðŸ“š Metadata edges (all):\")\n",
    "        \n",
    "        # Book-Author\n",
    "        graph_data[('book', 'written_by', 'author')] = self.edge_data['book_author']\n",
    "        graph_data[('author', 'writes', 'book')] = self.edge_data['author_book']\n",
    "        print(f\"   âœ“ Book-Author: {len(self.edge_data['book_author'][0]):,} edges\")\n",
    "        \n",
    "        # Book-Publisher\n",
    "        graph_data[('book', 'published_by', 'publisher')] = self.edge_data['book_publisher']\n",
    "        graph_data[('publisher', 'publishes', 'book')] = self.edge_data['publisher_book']\n",
    "        print(f\"   âœ“ Book-Publisher: {len(self.edge_data['book_publisher'][0]):,} edges\")\n",
    "        \n",
    "        # Book-Category\n",
    "        graph_data[('book', 'belongs_to', 'category')] = self.edge_data['book_category']\n",
    "        graph_data[('category', 'contains', 'book')] = self.edge_data['category_book']\n",
    "        print(f\"   âœ“ Book-Category: {len(self.edge_data['book_category'][0]):,} edges\")\n",
    "        \n",
    "        # Author-Category\n",
    "        graph_data[('author', 'writes_in', 'category')] = self.edge_data['author_category']\n",
    "        graph_data[('category', 'has_author', 'author')] = self.edge_data['category_author']\n",
    "        print(f\"   âœ“ Author-Category: {len(self.edge_data['author_category'][0]):,} edges\")\n",
    "        \n",
    "        # Author-Publisher\n",
    "        graph_data[('author', 'published_with', 'publisher')] = self.edge_data['author_publisher']\n",
    "        graph_data[('publisher', 'publishes_author', 'author')] = self.edge_data['publisher_author']\n",
    "        print(f\"   âœ“ Author-Publisher: {len(self.edge_data['author_publisher'][0]):,} edges\")\n",
    "        \n",
    "        # Publisher-Category\n",
    "        graph_data[('publisher', 'publishes_in', 'category')] = self.edge_data['publisher_category']\n",
    "        graph_data[('category', 'has_publisher', 'publisher')] = self.edge_data['category_publisher']\n",
    "        print(f\"   âœ“ Publisher-Category: {len(self.edge_data['publisher_category'][0]):,} edges\")\n",
    "        \n",
    "        # Create graph\n",
    "        g = dgl.heterograph(graph_data)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"GRAPH STRUCTURE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Node types: {g.ntypes}\")\n",
    "        print(f\"Edge types: {len(g.canonical_etypes)}\")\n",
    "        print(\"\\nNode counts:\")\n",
    "        for ntype in g.ntypes:\n",
    "            print(f\"  {ntype}: {g.num_nodes(ntype):,}\")\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def extract_node_features(self, g):\n",
    "        \"\"\"Extract node features with safe handling\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EXTRACTING NODE FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        def safe_float(value, default=0.0):\n",
    "            \"\"\"Safely convert to float\"\"\"\n",
    "            try:\n",
    "                return max(0.0, float(value)) if value is not None else default\n",
    "            except (ValueError, TypeError):\n",
    "                return default\n",
    "        \n",
    "        # Book features (4 dims)\n",
    "        print(\"\\n1. Book features (4 dims: price, rating, review_count, pages)...\")\n",
    "        book_feats = []\n",
    "        for i in range(g.num_nodes('book')):\n",
    "            book_id = self.reverse_maps['book'].get(i, '')\n",
    "            book = next((b for b in self.entity_data['book']\n",
    "                        if str(b.get('book_id', '')) == book_id), None)\n",
    "            \n",
    "            if book:\n",
    "                price = safe_float(book.get('book_price', 0))\n",
    "                rating = safe_float(book.get('average_rating', 0))\n",
    "                review_count = safe_float(book.get('review_count', 0))\n",
    "                pages = safe_float(book.get('book_pages', 0))\n",
    "                \n",
    "                feats = [\n",
    "                    np.log1p(price) / 10.0,\n",
    "                    rating / 5.0,\n",
    "                    np.log1p(review_count) / 10.0,\n",
    "                    np.log1p(pages) / 10.0\n",
    "                ]\n",
    "            else:\n",
    "                feats = [0.0, 0.0, 0.0, 0.0]\n",
    "            \n",
    "            book_feats.append(feats)\n",
    "        \n",
    "        features['book'] = torch.tensor(book_feats, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['book'].shape}\")\n",
    "        \n",
    "        # Author features (1 dim)\n",
    "        print(\"\\n2. Author features (1 dim: follower_count)...\")\n",
    "        author_feats = []\n",
    "        for i in range(g.num_nodes('author')):\n",
    "            author_id = self.reverse_maps['author'].get(i, '')\n",
    "            author = next((a for a in self.entity_data['author']\n",
    "                          if str(a.get('author_id', '')) == author_id), None)\n",
    "            \n",
    "            if author:\n",
    "                follower_count = safe_float(author.get('follower_count', 0))\n",
    "                feats = [np.log1p(follower_count) / 10.0]\n",
    "            else:\n",
    "                feats = [0.0]\n",
    "            \n",
    "            author_feats.append(feats)\n",
    "        \n",
    "        features['author'] = torch.tensor(author_feats, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['author'].shape}\")\n",
    "        \n",
    "        # Category features (1 dim)\n",
    "        print(\"\\n3. Category features (1 dim: books_count)...\")\n",
    "        cat_feats = []\n",
    "        for i in range(g.num_nodes('category')):\n",
    "            cat_id = self.reverse_maps['category'].get(i, '')\n",
    "            cat = next((c for c in self.entity_data['category']\n",
    "                       if str(c.get('category_id', '')) == cat_id), None)\n",
    "            \n",
    "            if cat:\n",
    "                books_count = safe_float(cat.get('books_count', 0))\n",
    "                feats = [np.log1p(books_count) / 10.0]\n",
    "            else:\n",
    "                feats = [0.0]\n",
    "            \n",
    "            cat_feats.append(feats)\n",
    "        \n",
    "        features['category'] = torch.tensor(cat_feats, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['category'].shape}\")\n",
    "        \n",
    "        # Publisher features (2 dims)\n",
    "        print(\"\\n4. Publisher features (2 dims: authors, categories)...\")\n",
    "        pub_feats = []\n",
    "        for i in range(g.num_nodes('publisher')):\n",
    "            pub_id = self.reverse_maps['publisher'].get(i, '')\n",
    "            pub = next((p for p in self.entity_data['publisher']\n",
    "                       if str(p.get('publisher_id', '')) == pub_id), None)\n",
    "            \n",
    "            if pub:\n",
    "                authors_count = safe_float(pub.get('total_authors', 0))\n",
    "                cats_count = safe_float(pub.get('total_categories', 0))\n",
    "                feats = [np.log1p(authors_count) / 10.0, np.log1p(cats_count) / 10.0]\n",
    "            else:\n",
    "                feats = [0.0, 0.0]\n",
    "            \n",
    "            pub_feats.append(feats)\n",
    "        \n",
    "        features['publisher'] = torch.tensor(pub_feats, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['publisher'].shape}\")\n",
    "        \n",
    "        # Review features (3 dims)\n",
    "        print(\"\\n5. Review features (3 dims: rating, pos_vote, neg_vote)...\")\n",
    "        review_feats = []\n",
    "        for i in range(g.num_nodes('review')):\n",
    "            review_id = self.reverse_maps['review'].get(i, '')\n",
    "            review = next((r for r in self.entity_data['review']\n",
    "                          if str(r.get('review_id', '')) == review_id), None)\n",
    "            \n",
    "            if review:\n",
    "                rating = safe_float(review.get('user_rating', 0))\n",
    "                pos_vote = safe_float(review.get('positive_vote', 0))\n",
    "                neg_vote = safe_float(review.get('negative_vote', 0))\n",
    "                \n",
    "                feats = [\n",
    "                    rating / 5.0,\n",
    "                    np.log1p(pos_vote) / 10.0,\n",
    "                    np.log1p(neg_vote) / 10.0\n",
    "                ]\n",
    "            else:\n",
    "                feats = [0.0, 0.0, 0.0]\n",
    "            \n",
    "            review_feats.append(feats)\n",
    "        \n",
    "        features['review'] = torch.tensor(review_feats, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['review'].shape}\")\n",
    "        \n",
    "        # User features (1 dim placeholder)\n",
    "        print(\"\\n6. User features (1 dim: placeholder)...\")\n",
    "        features['user'] = torch.zeros(g.num_nodes('user'), 1, dtype=torch.float32)\n",
    "        print(f\"   âœ… {features['user'].shape}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        \n",
    "        return features\n",
    "\n",
    "print(\"[OK] HGNNDataLoader defined (NO LEAKAGE version with reviews)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f2127-ec8d-442c-9207-1d33440fb642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 6: Model Classes ==========\n",
    "class NodeFeatureProjection(nn.Module):\n",
    "    \"\"\"Project features to common dimension\"\"\"\n",
    "    def __init__(self, feature_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.projections = nn.ModuleDict()\n",
    "        for ntype, dim in feature_dims.items():\n",
    "            self.projections[ntype] = nn.Sequential(\n",
    "                nn.Linear(dim, embed_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        return {k: self.projections[k](v) for k, v in features.items()}\n",
    "\n",
    "class HeteroRGCNLayer(nn.Module):\n",
    "    \"\"\"Heterogeneous R-GCN layer\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, rel_names, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = HeteroGraphConv({\n",
    "            rel: GraphConv(in_dim, out_dim, norm='both', allow_zero_in_degree=True)\n",
    "            for rel in rel_names\n",
    "        }, aggregate='mean')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        h_out = self.conv(g, h)\n",
    "        return {k: self.dropout(F.relu(v)) if v is not None else v for k, v in h_out.items()}\n",
    "\n",
    "class LightweightHGNN(nn.Module):\n",
    "    \"\"\"Lightweight HGNN encoder\"\"\"\n",
    "    def __init__(self, g, feature_dims, embed_dim=64, hidden_dim=128,\n",
    "                 out_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.ntypes = g.ntypes\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = NodeFeatureProjection(feature_dims, embed_dim)\n",
    "        \n",
    "        # Learnable embeddings\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for ntype in g.ntypes:\n",
    "            self.embeddings[ntype] = nn.Embedding(g.num_nodes(ntype), embed_dim)\n",
    "        \n",
    "        # Combine layer\n",
    "        self.combine = nn.ModuleDict()\n",
    "        for ntype in g.ntypes:\n",
    "            self.combine[ntype] = nn.Linear(embed_dim * 2, hidden_dim)\n",
    "        \n",
    "        # R-GCN layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            out_d = out_dim if i == num_layers - 1 else hidden_dim\n",
    "            self.layers.append(HeteroRGCNLayer(hidden_dim, out_d, g.canonical_etypes, dropout))\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        # Project features\n",
    "        h_feat = self.feature_proj(features)\n",
    "        \n",
    "        # Get learnable embeddings\n",
    "        h_emb = {}\n",
    "        for ntype in self.ntypes:\n",
    "            indices = torch.arange(g.num_nodes(ntype), device=features[ntype].device)\n",
    "            h_emb[ntype] = self.embeddings[ntype](indices)\n",
    "        \n",
    "        # Combine\n",
    "        h = {}\n",
    "        for ntype in self.ntypes:\n",
    "            combined = torch.cat([h_feat[ntype], h_emb[ntype]], dim=-1)\n",
    "            h[ntype] = F.relu(self.combine[ntype](combined))\n",
    "        \n",
    "        # GNN layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class NegativeSampler:\n",
    "    \"\"\"Efficient negative sampler\"\"\"\n",
    "    def __init__(self, num_books, train_data):\n",
    "        self.num_books = num_books\n",
    "        self.user_pos = defaultdict(set)\n",
    "        for u, b in zip(train_data['user'], train_data['book']):\n",
    "            self.user_pos[int(u)].add(int(b))\n",
    "    \n",
    "    def sample(self, user_batch, num_neg):\n",
    "        neg_samples = []\n",
    "        for u in user_batch.cpu().numpy():\n",
    "            pos_items = self.user_pos[int(u)]\n",
    "            negs = []\n",
    "            while len(negs) < num_neg:\n",
    "                neg = np.random.randint(0, self.num_books)\n",
    "                if neg not in pos_items:\n",
    "                    negs.append(neg)\n",
    "            neg_samples.append(negs)\n",
    "        return torch.tensor(neg_samples, dtype=torch.long)\n",
    "\n",
    "class BookRecommendationModel(nn.Module):\n",
    "    \"\"\"Complete recommendation model\"\"\"\n",
    "    def __init__(self, g, feature_dims, config):\n",
    "        super().__init__()\n",
    "        self.encoder = LightweightHGNN(\n",
    "            g, feature_dims,\n",
    "            embed_dim=config.EMBED_DIM,\n",
    "            hidden_dim=config.HIDDEN_DIM,\n",
    "            out_dim=config.OUT_DIM,\n",
    "            num_layers=config.NUM_LAYERS,\n",
    "            dropout=config.DROPOUT\n",
    "        )\n",
    "    \n",
    "    def forward(self, g, features, user_ids, pos_book_ids, neg_book_ids):\n",
    "        h = self.encoder(g, features)\n",
    "        \n",
    "        user_emb = h['user'][user_ids]\n",
    "        pos_emb = h['book'][pos_book_ids]\n",
    "        neg_emb = h['book'][neg_book_ids]\n",
    "        \n",
    "        pos_score = (user_emb.unsqueeze(1) * pos_emb.unsqueeze(1)).sum(dim=-1)\n",
    "        neg_score = (user_emb.unsqueeze(1) * neg_emb).sum(dim=-1)\n",
    "        \n",
    "        return pos_score, neg_score\n",
    "    \n",
    "    def get_all_embeddings(self, g, features):\n",
    "        return self.encoder(g, features)\n",
    "\n",
    "print(\"[OK] Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa55ab-81a2-4539-822a-155800ace4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 7: Training Utilities (NO FILTERING - ALL USERS) ==========\n",
    "def split_edges_user_based(edge_data, train_ratio=0.70, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"Split edges by users - NO FILTERING, ALL USERS INCLUDED\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SPLITTING DATA (ALL USERS - NO FILTERING)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Group by users\n",
    "    user_items = defaultdict(list)\n",
    "    for i in range(len(edge_data['user_indices'])):\n",
    "        user_idx = edge_data['user_indices'][i]\n",
    "        book_idx = edge_data['book_indices'][i]\n",
    "        rating = edge_data['ratings'][i]\n",
    "        user_items[user_idx].append((book_idx, rating))\n",
    "    \n",
    "       # Filter users with at least 3 interactions\n",
    "    valid_users = [u for u, items in user_items.items() if len(items) >= 3]\n",
    "    print(f\"\\nðŸ“Š Total users: {len(valid_users):,} (â‰¥3 interactions)\")\n",
    "    \n",
    "    if len(valid_users) == 0:\n",
    "        raise ValueError(\"No users with at least 3 interactions found!\")\n",
    "    \n",
    "    # Shuffle and split users\n",
    "    random.shuffle(valid_users)\n",
    "    n_train = int(len(valid_users) * train_ratio)\n",
    "    n_val = int(len(valid_users) * val_ratio)\n",
    "    \n",
    "    train_users = valid_users[:n_train]\n",
    "    val_users = valid_users[n_train:n_train + n_val]\n",
    "    test_users = valid_users[n_train + n_val:]\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ User Split:\")\n",
    "    print(f\"   Train users: {len(train_users):,} ({len(train_users)/len(valid_users)*100:.1f}%)\")\n",
    "    print(f\"   Val users:   {len(val_users):,} ({len(val_users)/len(valid_users)*100:.1f}%)\")\n",
    "    print(f\"   Test users:  {len(test_users):,} ({len(test_users)/len(valid_users)*100:.1f}%)\")\n",
    "    # Create splits\n",
    "    def create_split(users):\n",
    "        user_list, book_list, rating_list = [], [], []\n",
    "        for u in users:\n",
    "            for b, r in user_items[u]:\n",
    "                user_list.append(u)\n",
    "                book_list.append(b)\n",
    "                rating_list.append(r)\n",
    "        return {\n",
    "            'user': np.array(user_list),\n",
    "            'book': np.array(book_list),\n",
    "            'rating': np.array(rating_list)\n",
    "        }\n",
    "    \n",
    "    splits = {\n",
    "        'train': create_split(train_users),\n",
    "        'val': create_split(val_users),\n",
    "        'test': create_split(test_users)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Interaction Counts:\")\n",
    "    print(f\"   Train: {len(splits['train']['user']):,}\")\n",
    "    print(f\"   Val:   {len(splits['val']['user']):,}\")\n",
    "    print(f\"   Test:  {len(splits['test']['user']):,}\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def get_per_user_items(splits):\n",
    "    \"\"\"Convert splits to per-user dictionaries\"\"\"\n",
    "    def to_dict(split):\n",
    "        user_dict = defaultdict(list)\n",
    "        for u, b, r in zip(split['user'], split['book'], split['rating']):\n",
    "            user_dict[int(u)].append((int(b), float(r)))\n",
    "        return dict(user_dict)\n",
    "    \n",
    "    return (\n",
    "        to_dict(splits['train']),\n",
    "        to_dict(splits['val']),\n",
    "        to_dict(splits['test'])\n",
    "    )\n",
    "\n",
    "def create_train_graph_edges(splits, loader):\n",
    "    \"\"\"Create training-only graph edges (user-review-book path)\"\"\"\n",
    "    \n",
    "    # Get train user-book pairs\n",
    "    train_user_book = set(zip(splits['train']['user'], splits['train']['book']))\n",
    "    \n",
    "    # Filter user-review edges to training users only\n",
    "    user_to_review = load_json_file(loader.paths['user_to_review']) or []\n",
    "    book_to_review = load_json_file(loader.paths['book_to_review']) or []\n",
    "    \n",
    "    # Create review-to-book mapping\n",
    "    review_to_book_id = {}\n",
    "    for item in book_to_review:\n",
    "        review_id = str(item.get('review_id', ''))\n",
    "        book_id = str(item.get('book_id', ''))\n",
    "        if review_id in loader.entity_maps['review'] and book_id in loader.entity_maps['book']:\n",
    "            review_to_book_id[review_id] = loader.entity_maps['book'][book_id]\n",
    "    \n",
    "    # Filter user-review edges\n",
    "    train_user_review_src = []\n",
    "    train_user_review_dst = []\n",
    "    train_review_ids = set()\n",
    "    \n",
    "    for item in user_to_review:\n",
    "        user_id = str(item.get('user_id', ''))\n",
    "        review_id = str(item.get('review_id', ''))\n",
    "        \n",
    "        if user_id in loader.entity_maps['user'] and review_id in loader.entity_maps['review']:\n",
    "            user_idx = loader.entity_maps['user'][user_id]\n",
    "            review_idx = loader.entity_maps['review'][review_id]\n",
    "            book_idx = review_to_book_id.get(review_id, None)\n",
    "            \n",
    "            # Only include if this user-book pair is in training set\n",
    "            if book_idx is not None and (user_idx, book_idx) in train_user_book:\n",
    "                train_user_review_src.append(user_idx)\n",
    "                train_user_review_dst.append(review_idx)\n",
    "                train_review_ids.add(review_idx)\n",
    "    \n",
    "    # Filter review-book edges  \n",
    "    train_review_book_src = []\n",
    "    train_review_book_dst = []\n",
    "    \n",
    "    for item in book_to_review:\n",
    "        review_id = str(item.get('review_id', ''))\n",
    "        book_id = str(item.get('book_id', ''))\n",
    "        \n",
    "        if review_id in loader.entity_maps['review'] and book_id in loader.entity_maps['book']:\n",
    "            review_idx = loader.entity_maps['review'][review_id]\n",
    "            book_idx = loader.entity_maps['book'][book_id]\n",
    "            \n",
    "            # Only include reviews that are in training set\n",
    "            if review_idx in train_review_ids:\n",
    "                train_review_book_src.append(review_idx)\n",
    "                train_review_book_dst.append(book_idx)\n",
    "    \n",
    "    return {\n",
    "        'user_review': {\n",
    "            'user': np.array(train_user_review_src),\n",
    "            'review': np.array(train_user_review_dst)\n",
    "        },\n",
    "        'review_book': {\n",
    "            'review': np.array(train_review_book_src),\n",
    "            'book': np.array(train_review_book_dst)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"[OK] Training utilities defined (NO filtering, ALL users)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65472aa-96c0-490c-8b33-85ab89da1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 8: Evaluation Function (with NDCG@50) ==========\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(model, g, features, user_items, user_known, device, ks=[5, 10, 50]):\n",
    "    \"\"\"Evaluate recommendation metrics including NDCG@50\"\"\"\n",
    "    model.eval()\n",
    "    h = model.get_all_embeddings(g, features)\n",
    "    \n",
    "    users = list(user_items.keys())\n",
    "    max_k = max(ks)\n",
    "    \n",
    "    hit_rates = {k: [] for k in ks}\n",
    "    mrrs = []\n",
    "    ndcgs = {k: [] for k in ks}\n",
    "    \n",
    "    batch_size = 256\n",
    "    for i in range(0, len(users), batch_size):\n",
    "        batch_users = users[i:i+batch_size]\n",
    "        batch_indices = torch.tensor(batch_users, device=device)\n",
    "        \n",
    "        user_embs = h['user'][batch_indices]\n",
    "        book_embs = h['book']\n",
    "        scores = user_embs @ book_embs.T\n",
    "        \n",
    "        # Mask known items\n",
    "        for j, u in enumerate(batch_users):\n",
    "            known = user_known.get(u, set())\n",
    "            for kb in known:\n",
    "                scores[j, kb] = -float('inf')\n",
    "        \n",
    "        # Get top-K\n",
    "        _, top_indices = torch.topk(scores, k=max_k, dim=-1)\n",
    "        \n",
    "        # Compute metrics\n",
    "        for j, u in enumerate(batch_users):\n",
    "            gt_items = set(b for b, _ in user_items[u])\n",
    "            top_k_items = top_indices[j].cpu().tolist()\n",
    "            \n",
    "            # Hit@K\n",
    "            for k in ks:\n",
    "                hit_rates[k].append(1.0 if any(item in gt_items for item in top_k_items[:k]) else 0.0)\n",
    "            \n",
    "            # MRR\n",
    "            for rank, item in enumerate(top_k_items, 1):\n",
    "                if item in gt_items:\n",
    "                    mrrs.append(1.0 / rank)\n",
    "                    break\n",
    "            else:\n",
    "                mrrs.append(0.0)\n",
    "            \n",
    "            # NDCG@K (including NDCG@50)\n",
    "            for k in ks:\n",
    "                dcg = sum((1.0 / np.log2(rank + 1)) for rank, item in enumerate(top_k_items[:k], 1) if item in gt_items)\n",
    "                idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(len(gt_items), k) + 1))\n",
    "                ndcgs[k].append(dcg / max(idcg, 1e-8))\n",
    "    \n",
    "    metrics = {\n",
    "        'hit_rate': {k: np.mean(v) for k, v in hit_rates.items()},\n",
    "        'mrr': np.mean(mrrs),\n",
    "        'ndcg': {k: np.mean(v) for k, v in ndcgs.items()}\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"[OK] Evaluation function defined (with NDCG@50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be92ce-fc5e-4e66-99f4-3c37090bd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 9: Training Loop (with NDCG@50) ==========\n",
    "def train(model, g, features, splits, user_train, user_val, user_test, config, device):\n",
    "    \"\"\"Training loop with NDCG@50 tracking\"\"\"\n",
    "    \n",
    "    # Move to device\n",
    "    g = g.to(device)\n",
    "    features = {k: v.to(device) for k, v in features.items()}\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    neg_sampler = NegativeSampler(g.num_nodes('book'), splits['train'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE,\n",
    "                                  weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    best_mrr = 0\n",
    "    best_epoch = 0\n",
    "    patience = 0\n",
    "    best_state = None\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_hit_rate_5': [],\n",
    "        'val_hit_rate_10': [],\n",
    "        'val_hit_rate_50': [],\n",
    "        'val_mrr': [],\n",
    "        'val_ndcg_10': [],\n",
    "        'val_ndcg_50': [],  # Added NDCG@50\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # For val: known = train\n",
    "    user_known_val = {u: set(b for b, _ in user_train.get(u, [])) for u in user_val}\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Training {config.EPOCHS} epochs with early stopping (patience={config.PATIENCE})\")\n",
    "    print(f\"ðŸ“Š Best model selection: MRR (Mean Reciprocal Rank)\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        # Train\n",
    "        model.train()\n",
    "        user_ids = torch.from_numpy(splits['train']['user']).to(device)\n",
    "        book_ids = torch.from_numpy(splits['train']['book']).to(device)\n",
    "        perm = torch.randperm(len(user_ids))\n",
    "        user_ids, book_ids = user_ids[perm], book_ids[perm]\n",
    "        \n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        for i in range(0, len(user_ids), config.BATCH_SIZE):\n",
    "            batch_user = user_ids[i:i+config.BATCH_SIZE]\n",
    "            batch_book = book_ids[i:i+config.BATCH_SIZE]\n",
    "            neg_books = neg_sampler.sample(batch_user.cpu(), config.NEG_SAMPLES).to(device)\n",
    "            \n",
    "            pos_score, neg_score = model(g, features, batch_user, batch_book, neg_books)\n",
    "            loss = -torch.log(torch.sigmoid(pos_score - neg_score) + 1e-8).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        if (epoch + 1) % config.EVAL_EVERY == 0:\n",
    "            val_metrics = evaluate_metrics(model, g, features, user_val, user_known_val, device)\n",
    "            \n",
    "            history['val_hit_rate_5'].append(val_metrics['hit_rate'][5])\n",
    "            history['val_hit_rate_10'].append(val_metrics['hit_rate'][10])\n",
    "            history['val_hit_rate_50'].append(val_metrics['hit_rate'][50])\n",
    "            history['val_mrr'].append(val_metrics['mrr'])\n",
    "            history['val_ndcg_10'].append(val_metrics['ndcg'][10])\n",
    "            history['val_ndcg_50'].append(val_metrics['ndcg'][50])  # Track NDCG@50\n",
    "            history['epochs'].append(epoch + 1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Hit@5: {val_metrics['hit_rate'][5]:.4f} | \"\n",
    "                  f\"Hit@10: {val_metrics['hit_rate'][10]:.4f} | \"\n",
    "                  f\"MRR: {val_metrics['mrr']:.4f} | \"\n",
    "                  f\"NDCG@10: {val_metrics['ndcg'][10]:.4f} | \"\n",
    "                  f\"NDCG@50: {val_metrics['ndcg'][50]:.4f}\")\n",
    "            \n",
    "            # Model selection\n",
    "            if val_metrics['mrr'] > best_mrr + config.MIN_DELTA:\n",
    "                best_mrr = val_metrics['mrr']\n",
    "                best_epoch = epoch + 1\n",
    "                patience = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                print(f\"  â­ New best MRR: {best_mrr:.4f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= config.PATIENCE:\n",
    "                    print(f\"\\nâ¹ï¸  Early stopping at epoch {epoch+1} (best: {best_epoch})\")\n",
    "                    break\n",
    "            \n",
    "            scheduler.step(val_metrics['mrr'])\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1:3d} | Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Memory\n",
    "        if (epoch + 1) % config.CLEAR_CACHE_EVERY == 0:\n",
    "            clear_gpu_memory()\n",
    "    \n",
    "    # Load best\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nâœ… Loaded best model from epoch {best_epoch}\")\n",
    "    \n",
    "    return history, best_epoch, g, features, model\n",
    "\n",
    "print(\"[OK] Training loop defined (with NDCG@50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025f643-8541-48e4-98a3-032e8041da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 10: Visualization (with NDCG@50) ==========\n",
    "def plot_training_history(history, config):\n",
    "    \"\"\"Plot training metrics including NDCG@50\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('HGNN Training History', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history['loss'], linewidth=2, color='#e74c3c')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hit@5\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(history['epochs'], history['val_hit_rate_5'], linewidth=2, marker='o', color='#3498db')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Hit@5', fontsize=12)\n",
    "    ax.set_title('Validation Hit@5', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hit@10\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(history['epochs'], history['val_hit_rate_10'], linewidth=2, marker='o', color='#2ecc71')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Hit@10', fontsize=12)\n",
    "    ax.set_title('Validation Hit@10', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # MRR\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(history['epochs'], history['val_mrr'], linewidth=2, marker='o', color='#9b59b6')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('MRR', fontsize=12)\n",
    "    ax.set_title('Validation MRR â­', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # NDCG@10\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(history['epochs'], history['val_ndcg_10'], linewidth=2, marker='o', color='#1abc9c')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('NDCG@10', fontsize=12)\n",
    "    ax.set_title('Validation NDCG@10', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # NDCG@50\n",
    "    ax = axes[1, 2]\n",
    "    ax.plot(history['epochs'], history['val_ndcg_50'], linewidth=2, marker='o', color='#f39c12')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('NDCG@50', fontsize=12)\n",
    "    ax.set_title('Validation NDCG@50', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.OUTPUT_DIR}/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nâœ… Plot saved: {config.OUTPUT_DIR}/training_history.png\")\n",
    "\n",
    "print(\"[OK] Visualization function defined (with NDCG@50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81fed3-b431-4567-b813-c3d218345cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 11: Test Evaluation ==========\n",
    "@torch.no_grad()\n",
    "def test_evaluation(model, g, features, user_test, user_train, user_val, device, loader):\n",
    "    \"\"\"Test set evaluation with ground truth comparison\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST SET EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Known items = train + val\n",
    "    user_known_test = {}\n",
    "    for u in user_test:\n",
    "        known = set(b for b, _ in user_train.get(u, []))\n",
    "        known.update(b for b, _ in user_val.get(u, []))\n",
    "        user_known_test[u] = known\n",
    "    \n",
    "    # Evaluate\n",
    "    test_metrics = evaluate_metrics(model, g, features, user_test, user_known_test, device)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Test Set Results:\")\n",
    "    print(f\"  Hit@5:    {test_metrics['hit_rate'][5]:.4f}\")\n",
    "    print(f\"  Hit@10:   {test_metrics['hit_rate'][10]:.4f}\")\n",
    "    print(f\"  Hit@50:   {test_metrics['hit_rate'][50]:.4f}\")\n",
    "    print(f\"  MRR:      {test_metrics['mrr']:.4f}\")\n",
    "    print(f\"  NDCG@10:  {test_metrics['ndcg'][10]:.4f}\")\n",
    "    print(f\"  NDCG@50:  {test_metrics['ndcg'][50]:.4f}\")\n",
    "    \n",
    "    return test_metrics\n",
    "\n",
    "print(\"[OK] Test evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab41bf-1dd5-4afb-9740-9cb3e12d7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CELL 12: Main Execution (NO LEAKAGE) ==========\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline - NO DATA LEAKAGE\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HETEROGENEOUS GNN BOOK RECOMMENDATION SYSTEM\")\n",
    "    print(\"NO DATA LEAKAGE - Graph built from TRAINING edges only\")\n",
    "    print(\"ALL USERS INCLUDED - No filtering\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load data\n",
    "    loader = HGNNDataLoader(paths, config)\n",
    "    loader.load_entity_metadata()\n",
    "    edge_data = loader.load_edge_data()\n",
    "    \n",
    "    # Split data FIRST (before building graph)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 1: SPLIT DATA (before graph construction)\")\n",
    "    print(\"=\" * 80)\n",
    "    splits = split_edges_user_based(edge_data['user_book'],\n",
    "                                     config.TRAIN_RATIO,\n",
    "                                     config.VAL_RATIO,\n",
    "                                     config.TEST_RATIO)\n",
    "    user_train, user_val, user_test = get_per_user_items(splits)\n",
    "    \n",
    "    # Create training-only graph edges\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: CREATE TRAINING-ONLY EDGES (NO LEAKAGE)\")\n",
    "    print(\"=\" * 80)\n",
    "    train_edges = create_train_graph_edges(splits, loader)\n",
    "    print(f\"\\nâœ“ Training user-review edges: {len(train_edges['user_review']['user']):,}\")\n",
    "    print(f\"âœ“ Training review-book edges: {len(train_edges['review_book']['review']):,}\")\n",
    "    \n",
    "    # Build graph using ONLY training edges\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: BUILD GRAPH (training edges only - NO LEAKAGE)\")\n",
    "    print(\"=\" * 80)\n",
    "    g = loader.build_heterograph(train_edges['user_review'], train_edges['review_book'])\n",
    "    \n",
    "    # Extract features\n",
    "    features = loader.extract_node_features(g)\n",
    "    feature_dims = {k: v.shape[1] for k, v in features.items()}\n",
    "    \n",
    "    # Create model\n",
    "    model = BookRecommendationModel(g, feature_dims, config).to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nðŸ“Š Model: {total_params:,} parameters\")\n",
    "    \n",
    "    # Train\n",
    "    history, best_epoch, g, features, model = train(\n",
    "        model, g, features, splits, user_train, user_val, user_test, config, device\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    plot_training_history(history, config)\n",
    "    \n",
    "    # Test\n",
    "    test_metrics = test_evaluation(model, g, features, user_test, user_train, user_val, device, loader)\n",
    "    \n",
    "    # Completion\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return model, g, features, loader, splits, history, test_metrics\n",
    "\n",
    "# Run main\n",
    "if __name__ == \"__main__\":\n",
    "    model, g, features, loader, splits, history, test_metrics = main()\n",
    "\n",
    "print(\"\\nâœ… All cells defined! Run this cell to execute the complete pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd05a8e-37fe-4b2b-8e0b-cdd0475b7a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
