{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge implicit -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bd9912-42b3-4cd6-8ffe-5a80b3011e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utility Loader\n",
    "# ---------------------------------------\n",
    "\n",
    "def load_json(path: str):\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicitMFRecommender():\n",
    "    \"\"\"\n",
    "    Implicit Matrix Factorization using ALS (via implicit).\n",
    "    Train-only interactions. Guaranteed shape-safe.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_user_book: Dict[str, List[str]],\n",
    "        n_factors: int = 64,\n",
    "        n_iters: int = 20,\n",
    "        reg: float = 0.01,\n",
    "    ):\n",
    "        self.train_user_book = {u: list(set(b)) for u, b in train_user_book.items()}\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iters = n_iters\n",
    "        self.reg = reg\n",
    "\n",
    "    def fit(self):\n",
    "        print(\"Training Implicit MF (ALS)...\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Collect users and books\n",
    "        # -----------------------------\n",
    "        self.user_ids = sorted(self.train_user_book.keys())\n",
    "        \n",
    "        all_books = set()\n",
    "        for u, bl in self.train_user_book.items():\n",
    "            for b in bl:\n",
    "                all_books.add(b)\n",
    "                \n",
    "        self.book_ids = sorted(list(all_books))\n",
    "\n",
    "        self.user_id_map = {u: i for i, u in enumerate(self.user_ids)}\n",
    "        self.book_id_map = {b: i for i, b in enumerate(self.book_ids)}\n",
    "        self.reverse_book_id_map = {i: b for i, b in enumerate(self.book_ids)}\n",
    "\n",
    "        # -----------------------------\n",
    "        # Build USER–ITEM matrix\n",
    "        # -----------------------------\n",
    "        rows, cols = [], []\n",
    "        for u, books in self.train_user_book.items():\n",
    "            ui = self.user_id_map[u]\n",
    "            for b in books:\n",
    "                bi = self.book_id_map[b]\n",
    "                rows.append(ui)\n",
    "                cols.append(bi)\n",
    "\n",
    "        data = np.ones(len(rows), dtype=np.float32)\n",
    "\n",
    "        # USER–ITEM\n",
    "        self.user_item_matrix = csr_matrix(\n",
    "            (data, (rows, cols)),\n",
    "            shape=(len(self.user_ids), len(self.book_ids)),\n",
    "        )\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train ALS\n",
    "        # -----------------------------\n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=self.n_factors,\n",
    "            iterations=self.n_iters,\n",
    "            regularization=self.reg,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        self.model.fit(self.user_item_matrix)\n",
    "\n",
    "    def recommend(self, user_id: str, k: int = 10):\n",
    "        if user_id not in self.user_id_map:\n",
    "            return []\n",
    "\n",
    "        uidx = self.user_id_map[user_id]\n",
    "    \n",
    "        user_row = self.user_item_matrix[uidx]\n",
    "    \n",
    "        scores = self.model.recommend(\n",
    "            userid=uidx,\n",
    "            user_items=user_row,\n",
    "            N=k,\n",
    "            filter_already_liked_items=True,\n",
    "        )\n",
    "    \n",
    "        item_indices, _ = scores\n",
    "\n",
    "        recs = []\n",
    "        for i in item_indices:\n",
    "            recs.append(self.reverse_book_id_map[i])\n",
    "       \n",
    "        return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates ranking-based recommendation metrics.\n",
    "    Computes: Hit@K, MRR@K, NDCG@K for various K values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def hit_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Hit@K: 1 if at least one relevant item is in top-K, else 0\n",
    "        \"\"\"\n",
    "        top_k = predictions[:k]\n",
    "        return 1.0 if any(item in ground_truth for item in top_k) else 0.0\n",
    "    \n",
    "    def mrr_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        MRR@K: Reciprocal rank of first relevant item in top-K\n",
    "        \"\"\"\n",
    "        top_k = predictions[:k]\n",
    "        for rank, item in enumerate(top_k, start=1):\n",
    "            if item in ground_truth:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "    \n",
    "    def dcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        DCG@K: Discounted Cumulative Gain\n",
    "        \"\"\"\n",
    "        dcg = 0.0\n",
    "        top_k = predictions[:k]\n",
    "        for rank, item in enumerate(top_k, start=1):\n",
    "            if item in ground_truth:\n",
    "                dcg += 1.0 / np.log2(rank + 1)\n",
    "        return dcg\n",
    "    \n",
    "    def idcg_at_k(self, ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        IDCG@K: Ideal DCG (best possible DCG)\n",
    "        \"\"\"\n",
    "        ideal_k = min(len(ground_truth), k)\n",
    "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_k + 1))\n",
    "        return idcg\n",
    "    \n",
    "    def ndcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        NDCG@K: Normalized Discounted Cumulative Gain\n",
    "        \"\"\"\n",
    "        dcg = self.dcg_at_k(predictions, ground_truth, k)\n",
    "        idcg = self.idcg_at_k(ground_truth, k)\n",
    "        \n",
    "        if idcg == 0.0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dcg / idcg\n",
    "    \n",
    "    def evaluate(self, predictions: Dict[str, List[str]], ground_truth: Dict[str, Set[str]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate predictions against ground truth.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Dict mapping user_id -> list of recommended item_ids (ranked)\n",
    "            ground_truth: Dict mapping user_id -> set of relevant item_ids\n",
    "            \n",
    "        Returns:\n",
    "            Dict of metric_name -> average_score\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'Hit@5': [],\n",
    "            'Hit@10': [],\n",
    "            'Hit@50': [],\n",
    "            'MRR@10': [],\n",
    "            'NDCG@10': [],\n",
    "            'NDCG@50': []\n",
    "        }\n",
    "        \n",
    "        # Only evaluate users present in both predictions and ground_truth\n",
    "        common_users = set(predictions.keys()) & set(ground_truth.keys())\n",
    "        \n",
    "        for user_id in common_users:\n",
    "            preds = predictions[user_id]\n",
    "            gt = ground_truth[user_id]\n",
    "            \n",
    "            # Skip users with no ground truth items\n",
    "            if len(gt) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics['Hit@5'].append(self.hit_at_k(preds, gt, 5))\n",
    "            metrics['Hit@10'].append(self.hit_at_k(preds, gt, 10))\n",
    "            metrics['Hit@50'].append(self.hit_at_k(preds, gt, 50))\n",
    "            metrics['MRR@10'].append(self.mrr_at_k(preds, gt, 10))\n",
    "            metrics['NDCG@10'].append(self.ndcg_at_k(preds, gt, 10))\n",
    "            metrics['NDCG@50'].append(self.ndcg_at_k(preds, gt, 50))\n",
    "        \n",
    "        # Average across all users\n",
    "        results = {}\n",
    "        for metric_name, values in metrics.items():\n",
    "            if len(values) > 0:\n",
    "                results[metric_name] = np.mean(values)\n",
    "            else:\n",
    "                results[metric_name] = 0.0\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_book_interactions(\n",
    "    user_to_review_path: str,\n",
    "    book_to_review_path: str,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "        user_id -> [book_id, book_id, ...]\n",
    "    \"\"\"\n",
    "    user_to_review = load_json(user_to_review_path)\n",
    "    book_to_review = load_json(book_to_review_path)\n",
    "\n",
    "    # Normalize IDs to string\n",
    "    review_to_user = {\n",
    "        str(x[\"review_id\"]): str(x[\"user_id\"])\n",
    "        for x in user_to_review\n",
    "    }\n",
    "\n",
    "    review_to_book = {\n",
    "        str(x[\"review_id\"]): str(x[\"book_id\"])\n",
    "        for x in book_to_review\n",
    "    }\n",
    "\n",
    "    user_book = defaultdict(list)\n",
    "\n",
    "    for rid, user_id in review_to_user.items():\n",
    "        if rid in review_to_book:\n",
    "            book_id = review_to_book[rid]\n",
    "            user_book[user_id].append(book_id)\n",
    "\n",
    "    return user_book\n",
    "\n",
    "\n",
    "def split_user_interactions(\n",
    "    user_book: Dict[str, List[str]],\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        train_user_book\n",
    "        val_user_book\n",
    "        test_user_book\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    train = {}\n",
    "    val = {}\n",
    "    test = {}\n",
    "\n",
    "    for user, books in user_book.items():\n",
    "        books = list(set(books))  # remove duplicates\n",
    "        random.shuffle(books)\n",
    "\n",
    "        n = len(books)\n",
    "        if n < 3:\n",
    "            train[user] = books\n",
    "            val[user] = []\n",
    "            test[user] = []\n",
    "            continue\n",
    "\n",
    "        n_train = int(0.7 * n)\n",
    "        n_val = int(0.15 * n)\n",
    "\n",
    "        train[user] = books[:n_train]\n",
    "        val[user] = books[n_train : n_train + n_val]\n",
    "        test[user] = books[n_train + n_val :]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def build_ground_truth(test_user_book: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    return {\n",
    "        user: set(books)\n",
    "        for user, books in test_user_book.items()\n",
    "        if len(books) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_predictions(model, users: List[str], k: int):\n",
    "    predictions = {}\n",
    "\n",
    "    for u in tqdm(users, desc=\"Generating Predictions\"):\n",
    "        preds = model.recommend(u, k)\n",
    "        predictions[u] = preds\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users with interactions: 63721\n",
      "Users in test set: 15427\n",
      "\n",
      "Training Implicit MF (ALS)...\n",
      "Training Implicit MF (ALS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "C:\\Users\\Admin\\miniconda3\\envs\\recsys\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f3f2c9590b48a491cd2d972e66e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8a0546d0f4417382d7eb27ec5f5698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Predictions:   0%|          | 0/15427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Implicit MF Results =====\n",
      "Hit@5: 0.0606\n",
      "Hit@10: 0.0826\n",
      "Hit@50: 0.0826\n",
      "MRR@10: 0.0410\n",
      "NDCG@10: 0.0347\n",
      "NDCG@50: 0.0345\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # -------------------------------\n",
    "    # Paths\n",
    "    # -------------------------------\n",
    "    DATA_FOLDER = \"RokomariBG_Dataset/\"\n",
    "    USER_TO_REVIEW = DATA_FOLDER+\"user_to_review.json.gz\"\n",
    "    BOOK_TO_REVIEW = DATA_FOLDER+\"book_to_review.json.gz\"\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    # -------------------------------\n",
    "    # Build user-book interactions\n",
    "    # -------------------------------\n",
    "    user_book = build_user_book_interactions(\n",
    "        USER_TO_REVIEW,\n",
    "        BOOK_TO_REVIEW,\n",
    "    )\n",
    "\n",
    "    print(f\"Total users with interactions: {len(user_book)}\")\n",
    "\n",
    "    evaluator = RankingEvaluator()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Split 70/15/15\n",
    "    # -------------------------------\n",
    "    train_user_book, val_user_book, test_user_book = split_user_interactions(\n",
    "        user_book\n",
    "    )\n",
    "\n",
    "    ground_truth = build_ground_truth(test_user_book)\n",
    "    test_users = list(ground_truth.keys())\n",
    "\n",
    "    print(f\"Users in test set: {len(test_users)}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # IMPLICIT MF (ALS)\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nTraining Implicit MF (ALS)...\")\n",
    "\n",
    "    implicit_mf = ImplicitMFRecommender(\n",
    "        train_user_book=train_user_book,\n",
    "        n_factors=64,\n",
    "        n_iters=20,\n",
    "    )\n",
    "\n",
    "    implicit_mf.fit()\n",
    "\n",
    "    implicit_preds = generate_predictions(implicit_mf, test_users, K)\n",
    "    implicit_metrics = evaluator.evaluate(implicit_preds, ground_truth)\n",
    "\n",
    "    print(\"\\n===== Implicit MF Results =====\")\n",
    "    for m, v in implicit_metrics.items():\n",
    "        print(f\"{m}: {v:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aba5a5-55a4-468d-83a1-0496b29c913b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
