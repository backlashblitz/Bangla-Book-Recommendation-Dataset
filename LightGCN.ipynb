{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom typing import Dict, List, Set\n\nclass RankingEvaluator:\n    \"\"\"\n    Evaluates ranking-based recommendation metrics.\n    Computes: Hit@K, MRR@K, NDCG@K for various K values.\n    \"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def hit_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n        \"\"\"\n        Hit@K: 1 if at least one relevant item is in top-K, else 0\n        \"\"\"\n        top_k = predictions[:k]\n        return 1.0 if any(item in ground_truth for item in top_k) else 0.0\n    \n    def mrr_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n        \"\"\"\n        MRR@K: Reciprocal rank of first relevant item in top-K\n        \"\"\"\n        top_k = predictions[:k]\n        for rank, item in enumerate(top_k, start=1):\n            if item in ground_truth:\n                return 1.0 / rank\n        return 0.0\n    \n    def dcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n        \"\"\"\n        DCG@K: Discounted Cumulative Gain\n        \"\"\"\n        dcg = 0.0\n        top_k = predictions[:k]\n        for rank, item in enumerate(top_k, start=1):\n            if item in ground_truth:\n                dcg += 1.0 / np.log2(rank + 1)\n        return dcg\n    \n    def idcg_at_k(self, ground_truth: Set[str], k: int) -> float:\n        \"\"\"\n        IDCG@K: Ideal DCG (best possible DCG)\n        \"\"\"\n        ideal_k = min(len(ground_truth), k)\n        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_k + 1))\n        return idcg\n    \n    def ndcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n        \"\"\"\n        NDCG@K: Normalized Discounted Cumulative Gain\n        \"\"\"\n        dcg = self.dcg_at_k(predictions, ground_truth, k)\n        idcg = self.idcg_at_k(ground_truth, k)\n        \n        if idcg == 0.0:\n            return 0.0\n        \n        return dcg / idcg\n    \n    def evaluate(self, predictions: Dict[str, List[str]], ground_truth: Dict[str, Set[str]]) -> Dict[str, float]:\n        \"\"\"\n        Evaluate predictions against ground truth.\n        \n        Args:\n            predictions: Dict mapping user_id -> list of recommended item_ids (ranked)\n            ground_truth: Dict mapping user_id -> set of relevant item_ids\n            \n        Returns:\n            Dict of metric_name -> average_score\n        \"\"\"\n        metrics = {\n            'Hit@5': [],\n            'Hit@10': [],\n            'Hit@50': [],\n            'MRR@10': [],\n            'NDCG@10': [],\n            'NDCG@50': []\n        }\n        \n        # Only evaluate users present in both predictions and ground_truth\n        common_users = set(predictions.keys()) & set(ground_truth.keys())\n        \n        for user_id in common_users:\n            preds = predictions[user_id]\n            gt = ground_truth[user_id]\n            \n            # Skip users with no ground truth items\n            if len(gt) == 0:\n                continue\n            \n            # Compute metrics\n            metrics['Hit@5'].append(self.hit_at_k(preds, gt, 5))\n            metrics['Hit@10'].append(self.hit_at_k(preds, gt, 10))\n            metrics['Hit@50'].append(self.hit_at_k(preds, gt, 50))\n            metrics['MRR@10'].append(self.mrr_at_k(preds, gt, 10))\n            metrics['NDCG@10'].append(self.ndcg_at_k(preds, gt, 10))\n            metrics['NDCG@50'].append(self.ndcg_at_k(preds, gt, 50))\n        \n        # Average across all users\n        results = {}\n        for metric_name, values in metrics.items():\n            if len(values) > 0:\n                results[metric_name] = np.mean(values)\n            else:\n                results[metric_name] = 0.0\n        \n        return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport random\nfrom collections import defaultdict\nfrom typing import Dict, List, Set\nfrom tqdm.auto import tqdm\nimport torch\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Removed: from evaluation import RankingEvaluator\n# The RankingEvaluator class is already defined in a previous cell.\n\nclass BaseRecommender:\n    def fit(self):\n        raise NotImplementedError\n\n    def recommend(self, user_id: str, k: int = 10) -> List[str]:\n        raise NotImplementedError\n\n# ============================================================\n# 7. LightGCN — SPARSE, MEMORY-SAFE\n# ============================================================\n\nclass LightGCNRecommender(BaseRecommender):\n    \"\"\"\n    Proper LightGCN baseline:\n    - Builds sparse normalized adjacency A_hat (symmetric normalized)\n    - Propagates embeddings for n_layers\n    - Uses BPR loss\n    - Uses train_user_book only (implicit)\n    \"\"\"\n\n    def __init__(\n        self,\n        train_user_book: Dict[str, List[str]],\n        n_factors: int = 64,\n        n_layers: int = 3,\n        n_epochs: int = 10,\n        lr: float = 0.01,\n        reg: float = 1e-4,\n        n_neg: int = 1,\n        seed: int = 42,\n        device: str = None,\n        batch_size: int = 4096,\n        neg_per_user: int = 1,\n    ):\n        self.train_user_book = {u: list(set(b)) for u, b in train_user_book.items()}\n        self.n_factors = n_factors\n        self.n_layers = n_layers\n        self.n_epochs = n_epochs\n        self.lr = lr\n        self.reg = reg\n        self.n_neg = n_neg\n        self.rng = np.random.default_rng(seed)\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Filled in fit()\n        self.user_ids = []\n        self.book_ids = []\n        self.user_id_map = {}\n        self.book_id_map = {}\n        self.n_users = 0\n        self.n_items = 0\n        self.A_hat = None\n        self.user_emb = None\n        self.item_emb = None\n        self.final_user_emb = None\n        self.final_item_emb = None\n        self.batch_size = batch_size\n        self.neg_per_user = neg_per_user\n\n\n    # -------------------------------------------------\n    # Build sparse normalized adjacency A_hat\n    # -------------------------------------------------\n    def _build_sparse_graph(self):\n        print(\"Building sparse LightGCN graph...\")\n\n        self.user_ids = sorted(self.train_user_book.keys())\n        self.book_ids = sorted({b for books in self.train_user_book.values() for b in books})\n\n        self.user_id_map = {u: i for i, u in enumerate(self.user_ids)}\n        self.book_id_map = {b: i for i, b in enumerate(self.book_ids)}\n\n        U = len(self.user_ids)\n        I = len(self.book_ids)\n        N = U + I\n\n        # Build COO edges for bipartite graph (u<->i)\n        rows, cols = [], []\n        for u, books in self.train_user_book.items():\n            uidx = self.user_id_map[u]\n            for b in books:\n                iidx = self.book_id_map[b]\n                rows.append(uidx)\n                cols.append(U + iidx)\n                rows.append(U + iidx)\n                cols.append(uidx)\n\n        idx = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        val = torch.ones(idx.shape[1], dtype=torch.float32, device=self.device)\n\n        A = torch.sparse_coo_tensor(idx, val, size=(N, N), device=self.device).coalesce()\n\n        # Degree vector: deg[v] = sum_j A[v,j]\n        deg = torch.sparse.sum(A, dim=1).to_dense()\n        deg_inv_sqrt = torch.pow(deg, -0.5)\n        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n\n        # Build normalized values: A_hat[i,j] = A[i,j] * d_i^{-1/2} * d_j^{-1/2}\n        r, c = A.indices()\n        v = A.values()\n        v_norm = v * deg_inv_sqrt[r] * deg_inv_sqrt[c]\n\n        self.A_hat = torch.sparse_coo_tensor(\n            torch.stack([r, c], dim=0),\n            v_norm,\n            size=(N, N),\n            device=self.device,\n        ).coalesce()\n\n        self.n_users = U\n        self.n_items = I\n\n    # -------------------------------------------------\n    # One forward propagation to get final embeddings\n    # -------------------------------------------------\n    def _propagate(self):\n        # Initial embeddings for all nodes\n        all_emb0 = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)  # (U+I, d)\n\n        embs = [all_emb0]\n        x = all_emb0\n        for _ in range(self.n_layers):\n            x = torch.sparse.mm(self.A_hat, x)\n            embs.append(x)\n\n        # LightGCN uses average of layer embeddings\n        all_emb = torch.mean(torch.stack(embs, dim=0), dim=0)  # (U+I, d)\n\n        user_final = all_emb[: self.n_users]\n        item_final = all_emb[self.n_users :]\n        return user_final, item_final\n\n    # -------------------------------------------------\n    # Fit (BPR training)\n    # -------------------------------------------------\n    def fit(self):\n        print(f\"Training LightGCN on {self.device}...\")\n        self._build_sparse_graph()\n\n        self.user_emb = nn.Embedding(self.n_users, self.n_factors).to(self.device)\n        self.item_emb = nn.Embedding(self.n_items, self.n_factors).to(self.device)\n        nn.init.xavier_uniform_(self.user_emb.weight)\n        nn.init.xavier_uniform_(self.item_emb.weight)\n\n        optimizer = optim.Adam(\n            list(self.user_emb.parameters()) + list(self.item_emb.parameters()),\n            lr=self.lr,\n        )\n\n        # For faster negative sampling\n        user_pos_items = {\n            self.user_id_map[u]: set(self.book_id_map[b] for b in books if b in self.book_id_map)\n            for u, books in self.train_user_book.items()\n            if u in self.user_id_map\n        }\n\n        users_list = np.array(list(user_pos_items.keys()), dtype=np.int64)\n        n_users_train = len(users_list)\n\n        for epoch in range(1, self.n_epochs + 1):\n            self.user_emb.train()\n            self.item_emb.train()\n\n            # Shuffle users each epoch\n            self.rng.shuffle(users_list)\n\n            total_loss = 0.0\n            n_steps = 0\n\n            pbar = tqdm(range(0, n_users_train, self.batch_size), desc=f\"LightGCN Epoch {epoch}\")\n            for start in pbar:\n                batch_users = users_list[start : start + self.batch_size]\n                if len(batch_users) == 0:\n                    continue\n\n                # ---- sample positives ----\n                pos_items = []\n                for u in batch_users:\n                    pos_set = user_pos_items[int(u)]\n                    pos_items.append(int(self.rng.choice(list(pos_set))))\n                pos_items = np.array(pos_items, dtype=np.int64)\n\n                # ---- sample negatives (vectorized-ish, with rejection) ----\n                neg_items = np.empty((len(batch_users), self.neg_per_user), dtype=np.int64)\n                for i, u in enumerate(batch_users):\n                    pos_set = user_pos_items[int(u)]\n                    for j in range(self.neg_per_user):\n                        neg = int(self.rng.integers(0, self.n_items))\n                        while neg in pos_set:\n                            neg = int(self.rng.integers(0, self.n_items))\n                        neg_items[i, j] = neg\n\n                # move to GPU\n                batch_users_t = torch.tensor(batch_users, device=self.device, dtype=torch.long)\n                pos_items_t = torch.tensor(pos_items, device=self.device, dtype=torch.long)\n                neg_items_t = torch.tensor(neg_items, device=self.device, dtype=torch.long)  # (B, neg)\n\n                # ---- propagate ONCE per batch ----\n                user_final, item_final = self._propagate()\n\n                uvec = user_final[batch_users_t]                 # (B, d)\n                pvec = item_final[pos_items_t]                   # (B, d)\n                nvec = item_final[neg_items_t]                   # (B, neg, d)\n\n                # scores\n                pos_scores = torch.sum(uvec * pvec, dim=1, keepdim=True)     # (B, 1)\n                neg_scores = torch.sum(uvec.unsqueeze(1) * nvec, dim=2)      # (B, neg)\n\n                # BPR loss\n                # maximize log sigmoid(pos - neg)\n                bpr = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-12))\n\n                # reg loss on *raw* embeddings (standard)\n                reg_val = self.reg * (\n                    self.user_emb(batch_users_t).norm(2).pow(2)\n                    + self.item_emb(pos_items_t).norm(2).pow(2)\n                    + self.item_emb(neg_items_t.view(-1)).norm(2).pow(2)\n                ) / len(batch_users)\n\n                loss = bpr + reg_val\n\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                optimizer.step()\n\n                total_loss += float(loss.item())\n                n_steps += 1\n                pbar.set_postfix(loss=f\"{total_loss / n_steps:.5f}\")\n\n            print(f\"Epoch {epoch}: avg_loss={total_loss / max(n_steps,1):.6f}\")\n\n\n        # Cache final embeddings for fast recommend\n        self.user_emb.eval()\n        self.item_emb.eval()\n        with torch.no_grad():\n            self.final_user_emb, self.final_item_emb = self._propagate()\n\n    # -------------------------------------------------\n    # Recommend\n    # -------------------------------------------------\n    def recommend(self, user_id: str, k: int = 10) -> List[str]:\n        if user_id not in self.user_id_map:\n            return []\n\n        uidx = self.user_id_map[user_id]\n\n        with torch.no_grad():\n            uvec = self.final_user_emb[uidx]  # (d,)\n            scores = torch.matmul(self.final_item_emb, uvec)  # (I,)\n\n        scores = scores.detach().cpu().numpy()\n\n        # filter items seen in TRAIN\n        seen = set(self.book_id_map[b] for b in self.train_user_book[user_id] if b in self.book_id_map)\n        if seen:\n            scores[list(seen)] = -1e9\n\n        topk = np.argpartition(scores, -k)[-k:]\n        topk = topk[np.argsort(scores[topk])[::-1]]\n        return [self.book_ids[i] for i in topk]\n\n\ndef load_json(path: str):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\n# -------------------------------------------------\n# 1. Build User → Book Interaction Table\n# -------------------------------------------------\n\ndef build_user_book_interactions(\n    user_to_review_path: str,\n    book_to_review_path: str,\n) -> Dict[str, List[str]]:\n    \"\"\"\n    Builds:\n        user_id -> [book_id, book_id, ...]\n    \"\"\"\n    user_to_review = load_json(user_to_review_path)\n    book_to_review = load_json(book_to_review_path)\n\n    # Normalize IDs to string\n    review_to_user = {\n        str(x[\"review_id\"]): str(x[\"user_id\"])\n        for x in user_to_review\n    }\n\n    review_to_book = {\n        str(x[\"review_id\"]): str(x[\"book_id\"])\n        for x in book_to_review\n    }\n\n    user_book = defaultdict(list)\n\n    for rid, user_id in review_to_user.items():\n        if rid in review_to_book:\n            book_id = review_to_book[rid]\n            user_book[user_id].append(book_id)\n\n    return user_book\n\n\n# -------------------------------------------------\n# 2. Per-User 70/15/15 Split\n# -------------------------------------------------\n\ndef split_user_interactions(\n    user_book: Dict[str, List[str]],\n    seed: int = 42,\n):\n    \"\"\"\n    Returns:\n        train_user_book\n        val_user_book\n        test_user_book\n    \"\"\"\n    random.seed(seed)\n\n    train = {}\n    val = {}\n    test = {}\n\n    for user, books in user_book.items():\n        books = list(set(books))  # remove duplicates\n        random.shuffle(books)\n\n        n = len(books)\n        if n < 3:\n            train[user] = books\n            val[user] = []\n            test[user] = []\n            continue\n\n        n_train = int(0.7 * n)\n        n_val = int(0.15 * n)\n\n        train[user] = books[:n_train]\n        val[user] = books[n_train : n_train + n_val]\n        test[user] = books[n_train + n_val :]\n\n    return train, val, test\n\n\n# -------------------------------------------------\n# 3. Convert Test Set to Ground Truth Format\n# -------------------------------------------------\n\ndef build_ground_truth(test_user_book: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n    return {\n        user: set(books)\n        for user, books in test_user_book.items()\n        if len(books) > 0\n    }\n\n\n# -------------------------------------------------\n# 4. Generate Predictions for All Users\n# -------------------------------------------------\n\ndef generate_predictions(model, users: List[str], k: int):\n    predictions = {}\n\n    for u in tqdm(users, desc=\"Generating Predictions\"):\n        preds = model.recommend(u, k)\n        predictions[u] = preds\n\n    return predictions\n\n\n# -------------------------------------------------\n# 5. Main Pipeline\n# ------------------------------------------------- #\n\ndef main():\n    # ------------------------------- #\n    # Paths - Updated for Google Colab #\n    # ------------------------------- #\n    USER_TO_REVIEW = \"/content/drive/MyDrive/RokomariBG_Dataset/user_to_review.json\"\n    BOOK_TO_REVIEW = \"/content/drive/MyDrive/RokomariBG_Dataset/book_to_review.json\"\n\n    K = 10\n\n    # ------------------------------- #\n    # Build user-book interactions     #\n    # ------------------------------- #\n    print(\"Loading data...\")\n    user_book = build_user_book_interactions(\n        USER_TO_REVIEW,\n        BOOK_TO_REVIEW,\n    )\n\n    print(f\"Total users with interactions: {len(user_book)}\")\n\n    # Use the RankingEvaluator class defined in previous cell\n    evaluator = RankingEvaluator()\n\n    # ------------------------------- #\n    # Split 70/15/15                   #\n    # ------------------------------- #\n    print(\"Splitting data into train/val/test...\")\n    train_user_book, val_user_book, test_user_book = split_user_interactions(\n        user_book\n    )\n\n    ground_truth = build_ground_truth(test_user_book)\n    test_users = list(ground_truth.keys())\n\n    print(f\"Users in test set: {len(test_users)}\")\n\n    # =====================================================\n    # LIGHTGCN - Updated parameters\n    # =====================================================\n\n    print(\"\\nTraining LightGCN...\")\n\n    lightgcn = LightGCNRecommender(\n        train_user_book=train_user_book,\n        n_factors=64,      # Updated to 64\n        n_layers=2,        # Updated to 2\n        n_epochs=10,\n        lr=0.01,\n        reg=1e-4,\n        batch_size=4096,\n        neg_per_user=1,\n        seed=42\n    )\n\n    lightgcn.fit()\n\n    print(\"\\nGenerating recommendations for test users...\")\n    lightgcn_preds = generate_predictions(lightgcn, test_users, K)\n\n    print(\"Evaluating results...\")\n    lightgcn_metrics = evaluator.evaluate(lightgcn_preds, ground_truth)\n\n    print(\"\\n===== LightGCN Results =====\")\n    for m, v in lightgcn_metrics.items():\n        print(f\"{m}: {v:.4f}\")\n\n\n# -------------------------------------------------\n# Entry Point\n# -------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}