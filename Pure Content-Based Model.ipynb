{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:06:14.479191Z",
     "iopub.status.busy": "2026-02-07T09:06:14.478370Z",
     "iopub.status.idle": "2026-02-07T09:06:15.442572Z",
     "shell.execute_reply": "2026-02-07T09:06:15.441944Z",
     "shell.execute_reply.started": "2026-02-07T09:06:14.479159Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:06:37.627513Z",
     "iopub.status.busy": "2026-02-07T09:06:37.627212Z",
     "iopub.status.idle": "2026-02-07T09:06:49.846625Z",
     "shell.execute_reply": "2026-02-07T09:06:49.845825Z",
     "shell.execute_reply.started": "2026-02-07T09:06:37.627489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "All datasets loaded successfully!\n",
      "Books: 149515\n",
      "Authors: 16601\n",
      "Categories: 1516\n",
      "Publishers: 2757\n",
      "Reviews: 209602\n"
     ]
    }
   ],
   "source": [
    "# Define the base path\n",
    "base_path = Path('/kaggle/input/RokomariBG_Dataset')\n",
    "\n",
    "# Load all JSON files\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Main metadata\n",
    "with open(base_path / 'book.json', 'r', encoding='utf-8') as f:\n",
    "    books_data = json.load(f)\n",
    "    \n",
    "with open(base_path / 'author.json', 'r', encoding='utf-8') as f:\n",
    "    authors_data = json.load(f)\n",
    "    \n",
    "with open(base_path / 'category.json', 'r', encoding='utf-8') as f:\n",
    "    categories_data = json.load(f)\n",
    "    \n",
    "with open(base_path / 'publisher.json', 'r', encoding='utf-8') as f:\n",
    "    publishers_data = json.load(f)\n",
    "    \n",
    "with open(base_path / 'review.json', 'r', encoding='utf-8') as f:\n",
    "    reviews_data = json.load(f)\n",
    "\n",
    "# Relationship tables\n",
    "with open(base_path / 'book_to_category.json', 'r', encoding='utf-8') as f:\n",
    "    book_category = json.load(f)\n",
    "    \n",
    "with open(base_path / 'book_to_author.json', 'r', encoding='utf-8') as f:\n",
    "    book_author = json.load(f)\n",
    "    \n",
    "with open(base_path / 'book_to_publisher.json', 'r', encoding='utf-8') as f:\n",
    "    book_publisher = json.load(f)\n",
    "    \n",
    "with open(base_path / 'book_to_review.json', 'r', encoding='utf-8') as f:\n",
    "    book_review = json.load(f)\n",
    "    \n",
    "with open(base_path / 'user_to_review.json', 'r', encoding='utf-8') as f:\n",
    "    user_review = json.load(f)\n",
    "\n",
    "print(\"All datasets loaded successfully!\")\n",
    "print(f\"Books: {len(books_data)}\")\n",
    "print(f\"Authors: {len(authors_data)}\")\n",
    "print(f\"Categories: {len(categories_data)}\")\n",
    "print(f\"Publishers: {len(publishers_data)}\")\n",
    "print(f\"Reviews: {len(reviews_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:06:52.698693Z",
     "iopub.status.busy": "2026-02-07T09:06:52.698315Z",
     "iopub.status.idle": "2026-02-07T09:06:54.462078Z",
     "shell.execute_reply": "2026-02-07T09:06:54.461438Z",
     "shell.execute_reply.started": "2026-02-07T09:06:52.698660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books before deduplication: 149515\n",
      "Books after deduplication: 127302\n",
      "\n",
      "Dataset Overview:\n",
      "Unique Books: 127302\n",
      "Unique Authors: 16601\n",
      "Unique Categories: 1516\n",
      "Unique Publishers: 2757\n",
      "Total Reviews: 209602\n",
      "Unique Users: 63723\n",
      "\n",
      "Missing values in books:\n",
      "book_id       0\n",
      "book_title    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrames\n",
    "df_books = pd.DataFrame(books_data)\n",
    "df_authors = pd.DataFrame(authors_data)\n",
    "df_categories = pd.DataFrame(categories_data)\n",
    "df_publishers = pd.DataFrame(publishers_data)\n",
    "df_reviews = pd.DataFrame(reviews_data)\n",
    "df_book_category = pd.DataFrame(book_category)\n",
    "df_book_author = pd.DataFrame(book_author)\n",
    "df_book_publisher = pd.DataFrame(book_publisher)\n",
    "df_book_review = pd.DataFrame(book_review)\n",
    "df_user_review = pd.DataFrame(user_review)\n",
    "\n",
    "# Convert all IDs to strings for consistency\n",
    "df_books['book_id'] = df_books['book_id'].astype(str)\n",
    "df_authors['author_id'] = df_authors['author_id'].astype(str)\n",
    "df_categories['category_id'] = df_categories['category_id'].astype(str)\n",
    "df_publishers['publisher_id'] = df_publishers['publisher_id'].astype(str)\n",
    "df_reviews['review_id'] = df_reviews['review_id'].astype(str)\n",
    "\n",
    "df_book_category['book_id'] = df_book_category['book_id'].astype(str)\n",
    "df_book_category['category_id'] = df_book_category['category_id'].astype(str)\n",
    "\n",
    "df_book_author['book_id'] = df_book_author['book_id'].astype(str)\n",
    "df_book_author['author_id'] = df_book_author['author_id'].astype(str)\n",
    "\n",
    "df_book_publisher['book_id'] = df_book_publisher['book_id'].astype(str)\n",
    "df_book_publisher['publisher_id'] = df_book_publisher['publisher_id'].astype(str)\n",
    "\n",
    "df_book_review['book_id'] = df_book_review['book_id'].astype(str)\n",
    "df_book_review['review_id'] = df_book_review['review_id'].astype(str)\n",
    "\n",
    "df_user_review['user_id'] = df_user_review['user_id'].astype(str)\n",
    "df_user_review['review_id'] = df_user_review['review_id'].astype(str)\n",
    "\n",
    "# Remove duplicate books - keep first occurrence\n",
    "print(f\"Books before deduplication: {len(df_books)}\")\n",
    "df_books = df_books.drop_duplicates(subset=['book_id'], keep='first')\n",
    "print(f\"Books after deduplication: {len(df_books)}\")\n",
    "\n",
    "# Basic data info\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Unique Books: {df_books['book_id'].nunique()}\")\n",
    "print(f\"Unique Authors: {df_authors['author_id'].nunique()}\")\n",
    "print(f\"Unique Categories: {df_categories['category_id'].nunique()}\")\n",
    "print(f\"Unique Publishers: {df_publishers['publisher_id'].nunique()}\")\n",
    "print(f\"Total Reviews: {len(df_reviews)}\")\n",
    "print(f\"Unique Users: {df_user_review['user_id'].nunique()}\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "print(\"\\nMissing values in books:\")\n",
    "print(df_books[['book_id', 'book_title']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:07:07.884978Z",
     "iopub.status.busy": "2026-02-07T09:07:07.884691Z",
     "iopub.status.idle": "2026-02-07T09:07:12.612015Z",
     "shell.execute_reply": "2026-02-07T09:07:12.611245Z",
     "shell.execute_reply.started": "2026-02-07T09:07:07.884954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings created successfully!\n",
      "Books with categories: 107680\n",
      "Books with authors: 77444\n",
      "Books with publishers: 94957\n",
      "Books with reviews: 17670\n"
     ]
    }
   ],
   "source": [
    "# Create mapping dictionaries\n",
    "author_map = dict(zip(df_authors['author_id'], df_authors['author']))\n",
    "category_map = dict(zip(df_categories['category_id'], df_categories['category_name']))\n",
    "publisher_map = dict(zip(df_publishers['publisher_id'], df_publishers['publisher_name']))\n",
    "\n",
    "# Aggregate categories per book\n",
    "book_categories = df_book_category.groupby('book_id')['category_id'].apply(list).to_dict()\n",
    "\n",
    "# Aggregate authors per book\n",
    "book_authors = df_book_author.groupby('book_id')['author_id'].apply(list).to_dict()\n",
    "\n",
    "# Aggregate publishers per book\n",
    "book_publishers = df_book_publisher.groupby('book_id')['publisher_id'].apply(list).to_dict()\n",
    "\n",
    "# Aggregate reviews per book\n",
    "book_reviews_map = df_book_review.groupby('book_id')['review_id'].apply(list).to_dict()\n",
    "\n",
    "# Create review text mapping - the field is 'review_detail'\n",
    "review_text_map = dict(zip(df_reviews['review_id'], df_reviews['review_detail']))\n",
    "\n",
    "print(\"Mappings created successfully!\")\n",
    "print(f\"Books with categories: {len(book_categories)}\")\n",
    "print(f\"Books with authors: {len(book_authors)}\")\n",
    "print(f\"Books with publishers: {len(book_publishers)}\")\n",
    "print(f\"Books with reviews: {len(book_reviews_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:10:35.175552Z",
     "iopub.status.busy": "2026-02-07T09:10:35.175256Z",
     "iopub.status.idle": "2026-02-07T09:27:06.918960Z",
     "shell.execute_reply": "2026-02-07T09:27:06.918097Z",
     "shell.execute_reply.started": "2026-02-07T09:10:35.175528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial user-book interactions: 205924\n",
      "Initial unique users: 63723\n",
      "Initial unique books: 17670\n",
      "\n",
      "Total valid books in df_books: 127302\n",
      "\n",
      "After filtering for valid books:\n",
      "Total user-book interactions: 205924\n",
      "Unique users: 63723\n",
      "Unique books: 17670\n",
      "\n",
      "No user filtering applied:\n",
      "Total users: 63723\n",
      "Total interactions: 205924\n",
      "\n",
      "============================================================\n",
      "DATA SPLIT SUMMARY (70/15/15) - NO USER FILTERING\n",
      "============================================================\n",
      "Training set:\n",
      "  Users: 63723\n",
      "  Interactions: 142360 (69.1%)\n",
      "\n",
      "Validation set:\n",
      "  Users: 6227\n",
      "  Interactions: 12799 (6.2%)\n",
      "\n",
      "Test set:\n",
      "  Users: 29368\n",
      "  Interactions: 50765 (24.7%)\n",
      "\n",
      "Total: 63723 users, 205924 interactions\n"
     ]
    }
   ],
   "source": [
    "# Get user-book interactions from reviews\n",
    "user_books = df_user_review.merge(df_book_review, on='review_id')\n",
    "user_books = user_books[['user_id', 'book_id']].drop_duplicates()\n",
    "\n",
    "print(f\"Initial user-book interactions: {len(user_books)}\")\n",
    "print(f\"Initial unique users: {user_books['user_id'].nunique()}\")\n",
    "print(f\"Initial unique books: {user_books['book_id'].nunique()}\")\n",
    "\n",
    "# Convert book_id to string to ensure matching\n",
    "user_books['book_id'] = user_books['book_id'].astype(str)\n",
    "\n",
    "# Filter books that exist in our cleaned books dataset\n",
    "valid_books = set(df_books['book_id'].unique())\n",
    "print(f\"\\nTotal valid books in df_books: {len(valid_books)}\")\n",
    "\n",
    "user_books = user_books[user_books['book_id'].isin(valid_books)]\n",
    "\n",
    "print(f\"\\nAfter filtering for valid books:\")\n",
    "print(f\"Total user-book interactions: {len(user_books)}\")\n",
    "print(f\"Unique users: {user_books['user_id'].nunique()}\")\n",
    "print(f\"Unique books: {user_books['book_id'].nunique()}\")\n",
    "\n",
    "# NO USER FILTERING - Use all users\n",
    "valid_users = user_books['user_id'].unique()\n",
    "\n",
    "print(f\"\\nNo user filtering applied:\")\n",
    "print(f\"Total users: {len(valid_users)}\")\n",
    "print(f\"Total interactions: {len(user_books)}\")\n",
    "\n",
    "# Split each user's interactions into train/val/test (70/15/15)\n",
    "# This ensures every user appears in all three sets\n",
    "np.random.seed(42)\n",
    "\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id in valid_users:\n",
    "    user_interactions = user_books[user_books['user_id'] == user_id].copy()\n",
    "    \n",
    "    # Shuffle user's interactions\n",
    "    user_interactions = user_interactions.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    n_interactions = len(user_interactions)\n",
    "    \n",
    "    # For users with only 1 interaction, put in training\n",
    "    if n_interactions == 1:\n",
    "        train_list.append(user_interactions)\n",
    "        continue\n",
    "    \n",
    "    # For users with 2 interactions, put 1 in train, 1 in test\n",
    "    if n_interactions == 2:\n",
    "        train_list.append(user_interactions[:1])\n",
    "        test_list.append(user_interactions[1:])\n",
    "        continue\n",
    "    \n",
    "    # For users with 3+ interactions, do proper split\n",
    "    train_size = int(0.70 * n_interactions)\n",
    "    val_size = int(0.15 * n_interactions)\n",
    "    \n",
    "    # Ensure at least 1 item in train\n",
    "    if train_size == 0:\n",
    "        train_size = 1\n",
    "    \n",
    "    # Ensure at least 1 item in test if possible\n",
    "    remaining = n_interactions - train_size\n",
    "    if remaining > 0 and val_size >= remaining:\n",
    "        val_size = remaining - 1\n",
    "    \n",
    "    train_list.append(user_interactions[:train_size])\n",
    "    \n",
    "    if val_size > 0:\n",
    "        val_list.append(user_interactions[train_size:train_size + val_size])\n",
    "    \n",
    "    if train_size + val_size < n_interactions:\n",
    "        test_list.append(user_interactions[train_size + val_size:])\n",
    "\n",
    "train_df = pd.concat(train_list, ignore_index=True) if train_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
    "val_df = pd.concat(val_list, ignore_index=True) if val_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
    "test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA SPLIT SUMMARY (70/15/15) - NO USER FILTERING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Users: {train_df['user_id'].nunique()}\")\n",
    "print(f\"  Interactions: {len(train_df)} ({len(train_df)/len(user_books)*100:.1f}%)\")\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Users: {val_df['user_id'].nunique()}\")\n",
    "print(f\"  Interactions: {len(val_df)} ({len(val_df)/len(user_books)*100:.1f}%)\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Users: {test_df['user_id'].nunique()}\")\n",
    "print(f\"  Interactions: {len(test_df)} ({len(test_df)/len(user_books)*100:.1f}%)\")\n",
    "print(f\"\\nTotal: {len(valid_users)} users, {len(user_books)} interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:33:22.322691Z",
     "iopub.status.busy": "2026-02-07T09:33:22.322336Z",
     "iopub.status.idle": "2026-02-07T09:33:23.003932Z",
     "shell.execute_reply": "2026-02-07T09:33:23.003175Z",
     "shell.execute_reply.started": "2026-02-07T09:33:22.322665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features for 127302 books...\n",
      "Author features shape: (127302, 16572)\n",
      "Category features shape: (127302, 1493)\n",
      "Publisher features shape: (127302, 2752)\n"
     ]
    }
   ],
   "source": [
    "# Get all unique books that appear in training data\n",
    "train_books = train_df['book_id'].unique()\n",
    "all_books = df_books[df_books['book_id'].isin(valid_books)]['book_id'].unique()\n",
    "\n",
    "print(f\"Building features for {len(all_books)} books...\")\n",
    "\n",
    "# 1. Author Features (Multi-hot encoding)\n",
    "book_author_lists = []\n",
    "for book_id in all_books:\n",
    "    authors = book_authors.get(book_id, [])\n",
    "    book_author_lists.append(authors)\n",
    "\n",
    "mlb_authors = MultiLabelBinarizer(sparse_output=True)\n",
    "author_features = mlb_authors.fit_transform(book_author_lists)\n",
    "\n",
    "print(f\"Author features shape: {author_features.shape}\")\n",
    "\n",
    "# 2. Category Features (Multi-hot encoding)\n",
    "book_category_lists = []\n",
    "for book_id in all_books:\n",
    "    categories = book_categories.get(book_id, [])\n",
    "    book_category_lists.append(categories)\n",
    "\n",
    "mlb_categories = MultiLabelBinarizer(sparse_output=True)\n",
    "category_features = mlb_categories.fit_transform(book_category_lists)\n",
    "\n",
    "print(f\"Category features shape: {category_features.shape}\")\n",
    "\n",
    "# 3. Publisher Features (Multi-hot encoding)\n",
    "book_publisher_lists = []\n",
    "for book_id in all_books:\n",
    "    publishers = book_publishers.get(book_id, [])\n",
    "    book_publisher_lists.append(publishers)\n",
    "\n",
    "mlb_publishers = MultiLabelBinarizer(sparse_output=True)\n",
    "publisher_features = mlb_publishers.fit_transform(book_publisher_lists)\n",
    "\n",
    "print(f\"Publisher features shape: {publisher_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:33:35.664604Z",
     "iopub.status.busy": "2026-02-07T09:33:35.663999Z",
     "iopub.status.idle": "2026-02-07T09:34:02.284220Z",
     "shell.execute_reply": "2026-02-07T09:34:02.283449Z",
     "shell.execute_reply.started": "2026-02-07T09:33:35.664566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF features from reviews...\n",
      "Review features shape: (127302, 5000)\n",
      "Total vocabulary size: 5000\n"
     ]
    }
   ],
   "source": [
    "# 4. Review Text Features (TF-IDF)\n",
    "print(\"Building TF-IDF features from reviews...\")\n",
    "\n",
    "book_review_texts = []\n",
    "for book_id in all_books:\n",
    "    review_ids = book_reviews_map.get(book_id, [])\n",
    "    reviews = [review_text_map.get(rid, '') for rid in review_ids]\n",
    "    combined_text = ' '.join([r for r in reviews if r])\n",
    "    book_review_texts.append(combined_text)\n",
    "\n",
    "# TF-IDF with parameters to handle large vocabulary\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit features to avoid memory issues\n",
    "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n",
    "    ngram_range=(1, 2), # Use unigrams and bigrams\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words=None     # Keep all words since we're working with Bengali\n",
    ")\n",
    "\n",
    "review_features = tfidf.fit_transform(book_review_texts)\n",
    "\n",
    "print(f\"Review features shape: {review_features.shape}\")\n",
    "print(f\"Total vocabulary size: {len(tfidf.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:34:13.439715Z",
     "iopub.status.busy": "2026-02-07T09:34:13.439134Z",
     "iopub.status.idle": "2026-02-07T09:34:13.531940Z",
     "shell.execute_reply": "2026-02-07T09:34:13.531258Z",
     "shell.execute_reply.started": "2026-02-07T09:34:13.439689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining all features...\n",
      "Combined item features shape: (127302, 25817)\n",
      "Feature dimensions: 25817\n",
      "Created mappings for 127302 books\n"
     ]
    }
   ],
   "source": [
    "# Combine all features\n",
    "print(\"\\nCombining all features...\")\n",
    "\n",
    "# Stack all feature matrices horizontally\n",
    "item_features = hstack([\n",
    "    author_features,\n",
    "    category_features,\n",
    "    publisher_features,\n",
    "    review_features\n",
    "]).tocsr()\n",
    "\n",
    "print(f\"Combined item features shape: {item_features.shape}\")\n",
    "print(f\"Feature dimensions: {item_features.shape[1]}\")\n",
    "\n",
    "# Create book_id to index mapping\n",
    "book_to_idx = {book_id: idx for idx, book_id in enumerate(all_books)}\n",
    "idx_to_book = {idx: book_id for book_id, idx in book_to_idx.items()}\n",
    "\n",
    "print(f\"Created mappings for {len(book_to_idx)} books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:34:23.595338Z",
     "iopub.status.busy": "2026-02-07T09:34:23.595056Z",
     "iopub.status.idle": "2026-02-07T09:47:36.669769Z",
     "shell.execute_reply": "2026-02-07T09:47:36.669046Z",
     "shell.execute_reply.started": "2026-02-07T09:34:23.595313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building user profiles...\n",
      "Created profiles for 63723 users\n",
      "Profile dimension: 25817\n"
     ]
    }
   ],
   "source": [
    "# Build user profiles as average of interacted book features\n",
    "print(\"Building user profiles...\")\n",
    "\n",
    "user_profiles = {}\n",
    "\n",
    "for user_id in train_df['user_id'].unique():\n",
    "    user_books_list = train_df[train_df['user_id'] == user_id]['book_id'].values\n",
    "    \n",
    "    # Get feature vectors for user's books\n",
    "    book_indices = [book_to_idx[bid] for bid in user_books_list if bid in book_to_idx]\n",
    "    \n",
    "    if book_indices:\n",
    "        # Average of book features\n",
    "        user_feature_vectors = item_features[book_indices]\n",
    "        user_profile = np.asarray(user_feature_vectors.mean(axis=0)).flatten()\n",
    "        user_profiles[user_id] = user_profile\n",
    "\n",
    "print(f\"Created profiles for {len(user_profiles)} users\")\n",
    "print(f\"Profile dimension: {user_profiles[list(user_profiles.keys())[0]].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:48:00.901429Z",
     "iopub.status.busy": "2026-02-07T09:48:00.900706Z",
     "iopub.status.idle": "2026-02-07T09:48:00.909303Z",
     "shell.execute_reply": "2026-02-07T09:48:00.908665Z",
     "shell.execute_reply.started": "2026-02-07T09:48:00.901376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation function defined!\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(user_id, user_profile, item_features, book_to_idx, \n",
    "                        train_books_set, top_k=50):\n",
    "    \"\"\"\n",
    "    Generate top-k recommendations for a user\n",
    "    \"\"\"\n",
    "    # Calculate similarity between user profile and all items\n",
    "    user_profile_reshaped = user_profile.reshape(1, -1)\n",
    "    similarities = cosine_similarity(user_profile_reshaped, item_features)[0]\n",
    "    \n",
    "    # Get top-k items\n",
    "    top_indices = np.argsort(similarities)[::-1]\n",
    "    \n",
    "    # Filter out books already interacted with\n",
    "    recommendations = []\n",
    "    for idx in top_indices:\n",
    "        book_id = idx_to_book[idx]\n",
    "        if book_id not in train_books_set:\n",
    "            recommendations.append((book_id, similarities[idx]))\n",
    "            if len(recommendations) >= top_k:\n",
    "                break\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\"Recommendation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:48:30.311622Z",
     "iopub.status.busy": "2026-02-07T09:48:30.311023Z",
     "iopub.status.idle": "2026-02-07T09:48:30.318868Z",
     "shell.execute_reply": "2026-02-07T09:48:30.318136Z",
     "shell.execute_reply.started": "2026-02-07T09:48:30.311595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric functions defined!\n"
     ]
    }
   ],
   "source": [
    "def hit_at_k(recommended, actual, k):\n",
    "    \"\"\"Hit@K: 1 if any recommended item in top-k is in actual, else 0\"\"\"\n",
    "    recommended_k = set([item[0] for item in recommended[:k]])\n",
    "    actual_set = set(actual)\n",
    "    return 1.0 if len(recommended_k & actual_set) > 0 else 0.0\n",
    "\n",
    "def mrr(recommended, actual):\n",
    "    \"\"\"Mean Reciprocal Rank\"\"\"\n",
    "    actual_set = set(actual)\n",
    "    for i, (item, score) in enumerate(recommended):\n",
    "        if item in actual_set:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(recommended, actual, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain at K\"\"\"\n",
    "    recommended_k = [item[0] for item in recommended[:k]]\n",
    "    actual_set = set(actual)\n",
    "    \n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended_k):\n",
    "        if item in actual_set:\n",
    "            dcg += 1.0 / np.log2(i + 2)  # +2 because i is 0-indexed\n",
    "    \n",
    "    # Ideal DCG (if all actual items were at top)\n",
    "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "print(\"Evaluation metric functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:48:43.778078Z",
     "iopub.status.busy": "2026-02-07T09:48:43.777818Z",
     "iopub.status.idle": "2026-02-07T09:55:52.069624Z",
     "shell.execute_reply": "2026-02-07T09:55:52.068702Z",
     "shell.execute_reply.started": "2026-02-07T09:48:43.778057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation evaluation...\n",
      "Total validation users: 6227\n",
      "Validation users with training profiles: 6227\n",
      "Evaluating on 6227 validation users...\n",
      "Processing user 1/6227...\n",
      "Processing user 501/6227...\n",
      "Processing user 1001/6227...\n",
      "Processing user 1501/6227...\n",
      "Processing user 2001/6227...\n",
      "Processing user 2501/6227...\n",
      "Processing user 3001/6227...\n",
      "Processing user 3501/6227...\n",
      "Processing user 4001/6227...\n",
      "Processing user 4501/6227...\n",
      "Processing user 5001/6227...\n",
      "Processing user 5501/6227...\n",
      "Processing user 6001/6227...\n",
      "\n",
      "Validation evaluation complete! Evaluated 6227 users.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting validation evaluation...\")\n",
    "\n",
    "# Metrics storage for validation\n",
    "val_metrics = {\n",
    "    'hit@5': [],\n",
    "    'hit@10': [],\n",
    "    'hit@50': [],\n",
    "    'mrr': [],\n",
    "    'ndcg@10': [],\n",
    "    'ndcg@50': []\n",
    "}\n",
    "\n",
    "# Get validation users\n",
    "val_users_list = val_df['user_id'].unique()\n",
    "print(f\"Total validation users: {len(val_users_list)}\")\n",
    "\n",
    "# Filter to only validation users who also appear in training (so they have profiles)\n",
    "val_users_with_profiles = [u for u in val_users_list if u in user_profiles]\n",
    "print(f\"Validation users with training profiles: {len(val_users_with_profiles)}\")\n",
    "\n",
    "if len(val_users_with_profiles) > 0:\n",
    "    print(f\"Evaluating on {len(val_users_with_profiles)} validation users...\")\n",
    "    \n",
    "    # Evaluation on validation set\n",
    "    for i, user_id in enumerate(val_users_with_profiles):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Processing user {i+1}/{len(val_users_with_profiles)}...\")\n",
    "        \n",
    "        # Get user's training books (to exclude from recommendations)\n",
    "        train_books_set = set(train_df[train_df['user_id'] == user_id]['book_id'].values)\n",
    "        \n",
    "        # Get user's validation books (ground truth)\n",
    "        val_books = val_df[val_df['user_id'] == user_id]['book_id'].values\n",
    "        \n",
    "        if len(val_books) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations\n",
    "        user_profile = user_profiles[user_id]\n",
    "        recommendations = get_recommendations(\n",
    "            user_id, user_profile, item_features, book_to_idx, \n",
    "            train_books_set, top_k=50\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        val_metrics['hit@5'].append(hit_at_k(recommendations, val_books, 5))\n",
    "        val_metrics['hit@10'].append(hit_at_k(recommendations, val_books, 10))\n",
    "        val_metrics['hit@50'].append(hit_at_k(recommendations, val_books, 50))\n",
    "        val_metrics['mrr'].append(mrr(recommendations, val_books))\n",
    "        val_metrics['ndcg@10'].append(ndcg_at_k(recommendations, val_books, 10))\n",
    "        val_metrics['ndcg@50'].append(ndcg_at_k(recommendations, val_books, 50))\n",
    "\n",
    "    print(f\"\\nValidation evaluation complete! Evaluated {len(val_metrics['hit@5'])} users.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No validation users have training profiles. Skipping validation evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:57:48.468637Z",
     "iopub.status.busy": "2026-02-07T09:57:48.467976Z",
     "iopub.status.idle": "2026-02-07T09:57:48.480034Z",
     "shell.execute_reply": "2026-02-07T09:57:48.479448Z",
     "shell.execute_reply.started": "2026-02-07T09:57:48.468608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION SET - EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total Books: 127302\n",
      "  Training Users: 63723\n",
      "  Validation Users: 6227\n",
      "  Training Interactions: 142360\n",
      "  Validation Interactions: 12799\n",
      "  Evaluated Users: 6227\n",
      "\n",
      "Metric          Score     \n",
      "-------------------------\n",
      "Hit@5           0.1734\n",
      "Hit@10          0.2322\n",
      "Hit@50          0.3808\n",
      "MRR             0.1118\n",
      "NDCG@10         0.1085\n",
      "NDCG@50         0.1374\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Validation Hit Coverage:\n",
      "  ‚Ä¢ Users with Hit@5: 1080 (17.3%)\n",
      "  ‚Ä¢ Users with Hit@10: 1446 (23.2%)\n",
      "  ‚Ä¢ Users with Hit@50: 2371 (38.1%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate average validation metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SET - EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total Books: {len(all_books)}\")\n",
    "print(f\"  Training Users: {len(user_profiles)}\")\n",
    "print(f\"  Validation Users: {len(val_users_list)}\")\n",
    "print(f\"  Training Interactions: {len(train_df)}\")\n",
    "print(f\"  Validation Interactions: {len(val_df)}\")\n",
    "\n",
    "if len(val_metrics['hit@5']) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No validation metrics available.\")\n",
    "    print(\"Skipping to test set evaluation...\")\n",
    "else:\n",
    "    print(f\"  Evaluated Users: {len(val_metrics['hit@5'])}\")\n",
    "    \n",
    "    print(f\"\\n{'Metric':<15} {'Score':<10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'Hit@5':<15} {np.mean(val_metrics['hit@5']):.4f}\")\n",
    "    print(f\"{'Hit@10':<15} {np.mean(val_metrics['hit@10']):.4f}\")\n",
    "    print(f\"{'Hit@50':<15} {np.mean(val_metrics['hit@50']):.4f}\")\n",
    "    print(f\"{'MRR':<15} {np.mean(val_metrics['mrr']):.4f}\")\n",
    "    print(f\"{'NDCG@10':<15} {np.mean(val_metrics['ndcg@10']):.4f}\")\n",
    "    print(f\"{'NDCG@50':<15} {np.mean(val_metrics['ndcg@50']):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\nüìä Validation Hit Coverage:\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@5: {sum([1 for x in val_metrics['hit@5'] if x > 0])} ({sum([1 for x in val_metrics['hit@5'] if x > 0])/len(val_metrics['hit@5'])*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@10: {sum([1 for x in val_metrics['hit@10'] if x > 0])} ({sum([1 for x in val_metrics['hit@10'] if x > 0])/len(val_metrics['hit@10'])*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@50: {sum([1 for x in val_metrics['hit@50'] if x > 0])} ({sum([1 for x in val_metrics['hit@50'] if x > 0])/len(val_metrics['hit@50'])*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T09:58:17.024113Z",
     "iopub.status.busy": "2026-02-07T09:58:17.023795Z",
     "iopub.status.idle": "2026-02-07T10:33:55.867739Z",
     "shell.execute_reply": "2026-02-07T10:33:55.867092Z",
     "shell.execute_reply.started": "2026-02-07T09:58:17.024087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting test evaluation...\n",
      "============================================================\n",
      "Total test users: 29368\n",
      "Test users with training profiles: 29368\n",
      "Evaluating on 29368 test users...\n",
      "Processing user 1/29368...\n",
      "Processing user 501/29368...\n",
      "Processing user 1001/29368...\n",
      "Processing user 1501/29368...\n",
      "Processing user 2001/29368...\n",
      "Processing user 2501/29368...\n",
      "Processing user 3001/29368...\n",
      "Processing user 3501/29368...\n",
      "Processing user 4001/29368...\n",
      "Processing user 4501/29368...\n",
      "Processing user 5001/29368...\n",
      "Processing user 5501/29368...\n",
      "Processing user 6001/29368...\n",
      "Processing user 6501/29368...\n",
      "Processing user 7001/29368...\n",
      "Processing user 7501/29368...\n",
      "Processing user 8001/29368...\n",
      "Processing user 8501/29368...\n",
      "Processing user 9001/29368...\n",
      "Processing user 9501/29368...\n",
      "Processing user 10001/29368...\n",
      "Processing user 10501/29368...\n",
      "Processing user 11001/29368...\n",
      "Processing user 11501/29368...\n",
      "Processing user 12001/29368...\n",
      "Processing user 12501/29368...\n",
      "Processing user 13001/29368...\n",
      "Processing user 13501/29368...\n",
      "Processing user 14001/29368...\n",
      "Processing user 14501/29368...\n",
      "Processing user 15001/29368...\n",
      "Processing user 15501/29368...\n",
      "Processing user 16001/29368...\n",
      "Processing user 16501/29368...\n",
      "Processing user 17001/29368...\n",
      "Processing user 17501/29368...\n",
      "Processing user 18001/29368...\n",
      "Processing user 18501/29368...\n",
      "Processing user 19001/29368...\n",
      "Processing user 19501/29368...\n",
      "Processing user 20001/29368...\n",
      "Processing user 20501/29368...\n",
      "Processing user 21001/29368...\n",
      "Processing user 21501/29368...\n",
      "Processing user 22001/29368...\n",
      "Processing user 22501/29368...\n",
      "Processing user 23001/29368...\n",
      "Processing user 23501/29368...\n",
      "Processing user 24001/29368...\n",
      "Processing user 24501/29368...\n",
      "Processing user 25001/29368...\n",
      "Processing user 25501/29368...\n",
      "Processing user 26001/29368...\n",
      "Processing user 26501/29368...\n",
      "Processing user 27001/29368...\n",
      "Processing user 27501/29368...\n",
      "Processing user 28001/29368...\n",
      "Processing user 28501/29368...\n",
      "Processing user 29001/29368...\n",
      "\n",
      "Test evaluation complete! Evaluated 29368 users.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting test evaluation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metrics storage for test\n",
    "test_metrics = {\n",
    "    'hit@5': [],\n",
    "    'hit@10': [],\n",
    "    'hit@50': [],\n",
    "    'mrr': [],\n",
    "    'ndcg@10': [],\n",
    "    'ndcg@50': []\n",
    "}\n",
    "\n",
    "# Get test users\n",
    "test_users_list = test_df['user_id'].unique()\n",
    "print(f\"Total test users: {len(test_users_list)}\")\n",
    "\n",
    "# Filter to only test users who also appear in training (so they have profiles)\n",
    "test_users_with_profiles = [u for u in test_users_list if u in user_profiles]\n",
    "print(f\"Test users with training profiles: {len(test_users_with_profiles)}\")\n",
    "\n",
    "if len(test_users_with_profiles) > 0:\n",
    "    print(f\"Evaluating on {len(test_users_with_profiles)} test users...\")\n",
    "    \n",
    "    # Evaluation on test set\n",
    "    for i, user_id in enumerate(test_users_with_profiles):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Processing user {i+1}/{len(test_users_with_profiles)}...\")\n",
    "        \n",
    "        # Get user's training books (to exclude from recommendations)\n",
    "        train_books_set = set(train_df[train_df['user_id'] == user_id]['book_id'].values)\n",
    "        \n",
    "        # Get user's test books (ground truth)\n",
    "        test_books = test_df[test_df['user_id'] == user_id]['book_id'].values\n",
    "        \n",
    "        if len(test_books) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get recommendations\n",
    "        user_profile = user_profiles[user_id]\n",
    "        recommendations = get_recommendations(\n",
    "            user_id, user_profile, item_features, book_to_idx, \n",
    "            train_books_set, top_k=50\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_metrics['hit@5'].append(hit_at_k(recommendations, test_books, 5))\n",
    "        test_metrics['hit@10'].append(hit_at_k(recommendations, test_books, 10))\n",
    "        test_metrics['hit@50'].append(hit_at_k(recommendations, test_books, 50))\n",
    "        test_metrics['mrr'].append(mrr(recommendations, test_books))\n",
    "        test_metrics['ndcg@10'].append(ndcg_at_k(recommendations, test_books, 10))\n",
    "        test_metrics['ndcg@50'].append(ndcg_at_k(recommendations, test_books, 50))\n",
    "\n",
    "    print(f\"\\nTest evaluation complete! Evaluated {len(test_metrics['hit@5'])} users.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No test users have training profiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T10:34:41.813298Z",
     "iopub.status.busy": "2026-02-07T10:34:41.812712Z",
     "iopub.status.idle": "2026-02-07T10:34:41.837627Z",
     "shell.execute_reply": "2026-02-07T10:34:41.836829Z",
     "shell.execute_reply.started": "2026-02-07T10:34:41.813273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET - EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total Books: 127302\n",
      "  Training Users: 63723\n",
      "  Test Users: 29368\n",
      "  Training Interactions: 142360\n",
      "  Test Interactions: 50765\n",
      "  Evaluated Users: 29368\n",
      "\n",
      "Metric          Score     \n",
      "-------------------------\n",
      "Hit@5           0.2554\n",
      "Hit@10          0.3147\n",
      "Hit@50          0.4373\n",
      "MRR             0.1803\n",
      "NDCG@10         0.1706\n",
      "NDCG@50         0.1968\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Test Hit Coverage:\n",
      "  ‚Ä¢ Users with Hit@5: 7501 (25.5%)\n",
      "  ‚Ä¢ Users with Hit@10: 9242 (31.5%)\n",
      "  ‚Ä¢ Users with Hit@50: 12844 (43.7%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate average test metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET - EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total Books: {len(all_books)}\")\n",
    "print(f\"  Training Users: {len(user_profiles)}\")\n",
    "print(f\"  Test Users: {len(test_users_list)}\")\n",
    "print(f\"  Training Interactions: {len(train_df)}\")\n",
    "print(f\"  Test Interactions: {len(test_df)}\")\n",
    "\n",
    "if len(test_metrics['hit@5']) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No test metrics available.\")\n",
    "else:\n",
    "    print(f\"  Evaluated Users: {len(test_metrics['hit@5'])}\")\n",
    "    \n",
    "    print(f\"\\n{'Metric':<15} {'Score':<10}\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"{'Hit@5':<15} {np.mean(test_metrics['hit@5']):.4f}\")\n",
    "    print(f\"{'Hit@10':<15} {np.mean(test_metrics['hit@10']):.4f}\")\n",
    "    print(f\"{'Hit@50':<15} {np.mean(test_metrics['hit@50']):.4f}\")\n",
    "    print(f\"{'MRR':<15} {np.mean(test_metrics['mrr']):.4f}\")\n",
    "    print(f\"{'NDCG@10':<15} {np.mean(test_metrics['ndcg@10']):.4f}\")\n",
    "    print(f\"{'NDCG@50':<15} {np.mean(test_metrics['ndcg@50']):.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\nüìä Test Hit Coverage:\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@5: {sum([1 for x in test_metrics['hit@5'] if x > 0])} ({sum([1 for x in test_metrics['hit@5'] if x > 0])/len(test_metrics['hit@5'])*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@10: {sum([1 for x in test_metrics['hit@10'] if x > 0])} ({sum([1 for x in test_metrics['hit@10'] if x > 0])/len(test_metrics['hit@10'])*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@50: {sum([1 for x in test_metrics['hit@50'] if x > 0])} ({sum([1 for x in test_metrics['hit@50'] if x > 0])/len(test_metrics['hit@50'])*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T10:34:59.686931Z",
     "iopub.status.busy": "2026-02-07T10:34:59.686654Z",
     "iopub.status.idle": "2026-02-07T10:35:00.836952Z",
     "shell.execute_reply": "2026-02-07T10:35:00.836363Z",
     "shell.execute_reply.started": "2026-02-07T10:34:59.686908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEMONSTRATION: SHOWING ACTUAL RECOMMENDATIONS VS GROUND TRUTH (TEST SET)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "USER 1: USER41435\n",
      "================================================================================\n",
      "\n",
      "üìö TRAINING BOOKS (what user read): 3 books\n",
      "  1. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n",
      "  2. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø English Therapy\n",
      "  3. ‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶æ‡¶∞\n",
      "\n",
      "üéØ GROUND TRUTH (actual test books): 2 books\n",
      "  1. ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶∏‡ßá‡¶ú\n",
      "  2. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø VOCAB THERAPY\n",
      "\n",
      "üí° TOP-10 RECOMMENDATIONS:\n",
      "  1. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏ ‡¶¨‡ßÅ‡¶ï                                    (score: 0.9169) \n",
      "  2. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø VOCAB THERAPY                          (score: 0.8255) ‚úì HIT!\n",
      "  3. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø English Therapy ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú                (score: 0.7619) \n",
      "  4. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶æ‡¶∞‡¶´‡ßá‡¶ï‡ßç‡¶ü ‡ß®‡¶ü‡¶ø ‡¶¨‡¶á                        (score: 0.7164) \n",
      "  5. ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶´‡ßÅ‡¶≤‡¶ï‡ßã‡¶∞‡ßç‡¶∏ (‡¶¨‡¶ø‡¶ó‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ü‡ßÅ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏)                (score: 0.7102) \n",
      "  6. ‡¶õ‡ßã‡¶ü‡¶¶‡ßá‡¶∞ ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø                                          (score: 0.6524) \n",
      "  7. ‡¶∏‡¶æ‡¶á‡¶´‡ßÅ‡¶≤ ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶¨‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶≤‡¶æ‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú                           (score: 0.5830) \n",
      "  8. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá ‡¶â‡¶™‡¶Ø‡ßã‡¶ó‡ßÄ ‡ß® ‡¶ü‡¶ø ‡¶¨‡¶á               (score: 0.5669) \n",
      "  9. ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶ì‡ßü‡ßá ‡¶ü‡ßÅ ‡¶≤‡¶æ‡¶∞‡ßç‡¶® ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂                     (score: 0.5058) \n",
      "  10. How to Speak English Fluently                                (score: 0.5016) \n",
      "\n",
      "üìä USER METRICS:\n",
      "  ‚Ä¢ Hit@10: 1.0\n",
      "  ‚Ä¢ MRR: 0.5000\n",
      "  ‚Ä¢ Hits at positions: [2]\n",
      "\n",
      "================================================================================\n",
      "USER 2: USER12605\n",
      "================================================================================\n",
      "\n",
      "üìö TRAINING BOOKS (what user read): 3 books\n",
      "  1. ‡¶∏‡ßÅ‡¶ñ‡ßá‡¶∞ ‡¶®‡¶æ‡¶ü‡¶æ‡¶á\n",
      "  2. ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡ßá‡¶∞ ‡¶õ‡¶ø‡¶®‡ßç‡¶®‡¶™‡¶§‡ßç‡¶∞ ‡ßß‡¶Æ, ‡ß®‡ßü ‡¶ì ‡ß©‡ßü ‡¶ñ‡¶£‡ßç‡¶°‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®\n",
      "  3. ‡¶ú‡ßã‡¶õ‡¶®‡¶æ‡¶´‡ßÅ‡¶≤\n",
      "\n",
      "üéØ GROUND TRUTH (actual test books): 2 books\n",
      "  1. ‡¶∞‡¶æ‡¶´‡¶â‡¶≤ ‡¶Æ‡¶æ‡¶≤‡¶æ‡¶Æ ‡¶Ü‡¶®‡¶ø‡¶≤ ‡¶Ü‡¶á‡¶Æ‡ßç‡¶Æ‡¶æ‡¶§‡¶ø‡¶≤ ‡¶Ü‡¶≤‡¶æ‡¶Æ\n",
      "  2. ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡ßá‡¶∞ ‡¶õ‡¶ø‡¶®‡ßç‡¶®‡¶™‡¶§‡ßç‡¶∞\n",
      "\n",
      "üí° TOP-10 RECOMMENDATIONS:\n",
      "  1. ‡¶ú‡¶æ‡ßü‡¶®‡¶æ‡¶Æ‡¶æ‡¶ú                                                     (score: 0.5578) \n",
      "  2. ‡¶è‡¶∞‡¶¶‡ßã‡ßü‡¶æ‡¶®                                                      (score: 0.5345) \n",
      "  3. ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡ßá‡¶∞ ‡¶õ‡¶ø‡¶®‡ßç‡¶®‡¶™‡¶§‡ßç‡¶∞                                           (score: 0.5214) ‚úì HIT!\n",
      "  4. ‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶ø ‡¶ó‡¶≤‡ßç‡¶™                                                  (score: 0.4910) \n",
      "  5. ‡¶∏‡¶¨‡ßÅ‡¶ú ‡¶ö‡¶æ‡¶Å‡¶¶‡ßá ‡¶®‡ßÄ‡¶≤ ‡¶ú‡ßã‡¶õ‡¶®‡¶æ                                         (score: 0.4730) \n",
      "  6. ‡¶ï‡¶¨‡¶ø‡¶§‡¶æ‡¶∏‡¶Æ‡¶ó‡ßç‡¶∞ ‡¶Ü‡¶≤‡¶ø ‡¶á‡¶¨‡¶®‡ßÅ ‡¶Ü‡¶¨‡¶ø ‡¶§‡¶æ‡¶≤‡¶ø‡¶¨ (‡¶∞‡¶æ.)                          (score: 0.4661) \n",
      "  7. ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡ßá‡¶∞ ‡¶õ‡¶ø‡¶®‡ßç‡¶®‡¶™‡¶§‡ßç‡¶∞                                           (score: 0.4480) \n",
      "  8. ‡¶®‡ßÄ‡¶≤ ‡¶™‡ßÉ‡¶•‡¶ø‡¶¨‡ßÄ‡¶∞ ‡¶∏‡¶¨‡ßÅ‡¶ú ‡¶Ü‡¶ï‡¶æ‡¶∂                                        (score: 0.4387) \n",
      "  9. ‡¶¨‡¶®‡ßç‡¶ß‡¶®                                                        (score: 0.4381) \n",
      "  10. ‡¶Ü‡¶ï‡¶æ‡¶∂ ‡¶õ‡ßã‡¶Å‡ßü‡¶æ ‡¶∏‡ßç‡¶¨‡¶™‡ßç‡¶®                                            (score: 0.4318) \n",
      "\n",
      "üìä USER METRICS:\n",
      "  ‚Ä¢ Hit@10: 1.0\n",
      "  ‚Ä¢ MRR: 0.3333\n",
      "  ‚Ä¢ Hits at positions: [3]\n",
      "\n",
      "================================================================================\n",
      "USER 3: USER26066\n",
      "================================================================================\n",
      "\n",
      "üìö TRAINING BOOKS (what user read): 1 books\n",
      "  1. ‡¶∏‡¶Æ‡¶ï‡¶æ‡¶≤‡ßÄ‡¶® ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡¶®‡ßá‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®\n",
      "\n",
      "üéØ GROUND TRUTH (actual test books): 1 books\n",
      "  1. ‡¶∂‡¶ø‡¶∂‡ßÅ‡¶∞ ‡¶Æ‡¶®‡¶®‡ßá ‡¶à‡¶Æ‡¶æ‡¶®\n",
      "\n",
      "üí° TOP-10 RECOMMENDATIONS:\n",
      "  1. ‡¶∂‡¶ø‡¶∂‡ßÅ‡¶∞ ‡¶Æ‡¶®‡¶®‡ßá ‡¶à‡¶Æ‡¶æ‡¶®                                              (score: 0.7799) ‚úì HIT!\n",
      "  2. ‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶® ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶™‡¶æ‡¶≤‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡ßç‡¶Æ‡¶æ‡¶∞‡ßç‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®          (score: 0.7608) \n",
      "  3. ‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶®‡¶É ‡¶∏‡ßç‡¶¨‡¶™‡ßç‡¶® ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡ßã‡¶®‡¶æ                                     (score: 0.6662) \n",
      "  4. ‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶®‡ßá‡¶∞ ‡¶¨‡ßü‡¶É‡¶∏‡¶®‡ßç‡¶ß‡¶ø‡¶ï‡¶æ‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶®‡¶∏‡ßç‡¶§‡¶§‡ßç‡¶§‡ßç‡¶¨                            (score: 0.6455) \n",
      "  5. ‡¶∂‡ßç‡¶∞‡ßá‡¶∑‡ßç‡¶† ‡¶â‡¶™‡¶π‡¶æ‡¶∞                                                (score: 0.6325) \n",
      "  6. ‡¶ó‡ßÅ‡¶° ‡¶™‚Äç‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç                                             (score: 0.6325) \n",
      "  7. ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç                                                  (score: 0.6325) \n",
      "  8. ‡¶∏‡ßá‡¶∞‡¶æ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®                                     (score: 0.6003) \n",
      "  9. ‡¶Æ‡¶æ‡ßü‡ßá‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡¶ø‡¶Ç                                          (score: 0.5976) \n",
      "  10. ‡¶∏‡¶®‡ßç‡¶§‡¶æ‡¶®‡ßá‡¶∞ ‡¶≤‡¶æ‡¶≤‡¶®                                                (score: 0.5657) \n",
      "\n",
      "üìä USER METRICS:\n",
      "  ‚Ä¢ Hit@10: 1.0\n",
      "  ‚Ä¢ MRR: 1.0000\n",
      "  ‚Ä¢ Hits at positions: [1]\n",
      "\n",
      "================================================================================\n",
      "USER 4: USER16803\n",
      "================================================================================\n",
      "\n",
      "üìö TRAINING BOOKS (what user read): 4 books\n",
      "  1. ‡¶ï‡¶æ‡¶∏‡¶æ‡¶∏‡ßÅ‡¶≤ ‡¶Ü‡¶Æ‡ßç‡¶¨‡¶ø‡ßü‡¶æ (‡¶∏‡¶¨ ‡¶ñ‡¶£‡ßç‡¶° ‡¶è‡¶ï‡¶§‡ßç‡¶∞‡ßá)\n",
      "  2. ‡¶™‡¶¶‡ßç‡¶Æ‡¶æ ‡¶∏‡ßá‡¶§‡ßÅ\n",
      "  3. ‡¶∂‡¶∞‡ßç‡¶ü ‡¶ü‡ßá‡¶ï‡¶®‡¶ø‡¶ï ‡¶¨‡ßá‡¶∏‡¶ø‡¶ï ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶æ‡¶∞\n",
      "  ... and 1 more\n",
      "\n",
      "üéØ GROUND TRUTH (actual test books): 2 books\n",
      "  1. ‡¶ó‡¶æ‡¶≠‡ßÄ ‡¶¨‡¶ø‡¶§‡ßç‡¶§‡¶æ‡¶®‡ßç‡¶§\n",
      "  2. ‡¶õ‡¶¶‡ßç‡¶Æ ‡¶™‡¶ø‡¶∂‡¶æ‡¶ö\n",
      "\n",
      "üí° TOP-10 RECOMMENDATIONS:\n",
      "  1. ‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶æ‡¶∞                                      (score: 0.4587) \n",
      "  2. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂ ‡¶•‡ßá‡¶∞‡¶æ‡¶™‡¶ø ‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏ ‡¶¨‡ßÅ‡¶ï                                    (score: 0.4570) \n",
      "  3. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø English Therapy ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú                (score: 0.4396) \n",
      "  4. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø English Therapy                        (score: 0.4352) \n",
      "  5. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø VOCAB THERAPY                          (score: 0.3833) \n",
      "  6. ‡¶á‡¶Ç‡¶≤‡¶ø‡¶∂‡ßá ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡¶¶‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶æ‡¶∞‡¶´‡ßá‡¶ï‡ßç‡¶ü ‡ß®‡¶ü‡¶ø ‡¶¨‡¶á                        (score: 0.3723) \n",
      "  7. ‡¶ò‡¶∞‡ßá ‡¶¨‡¶∏‡ßá Spoken English                                       (score: 0.3607) \n",
      "  8. ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶´‡ßÅ‡¶≤‡¶ï‡ßã‡¶∞‡ßç‡¶∏ (‡¶¨‡¶ø‡¶ó‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ü‡ßÅ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏)                (score: 0.3600) \n",
      "  9. ‡¶®‡¶¨‡¶ø ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶ó‡¶≤‡ßç‡¶™                                              (score: 0.3537) \n",
      "  10. ‡¶Ü‡¶ß‡ßÅ‡¶®‡¶ø‡¶ï ‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶§‡ßç‡¶Ø‡¶ß‡¶æ‡¶∞‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞ ‡¶Æ‡¶π‡¶æ‡¶®‡¶æ‡ßü‡¶ï                             (score: 0.3417) \n",
      "\n",
      "üìä USER METRICS:\n",
      "  ‚Ä¢ Hit@10: 0.0\n",
      "  ‚Ä¢ MRR: 0.0000\n",
      "\n",
      "================================================================================\n",
      "USER 5: USER55093\n",
      "================================================================================\n",
      "\n",
      "üìö TRAINING BOOKS (what user read): 1 books\n",
      "  1. ‡¶¶‡ßà‡¶®‡¶®‡ßç‡¶¶‡¶ø‡¶® ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ\n",
      "\n",
      "üéØ GROUND TRUTH (actual test books): 1 books\n",
      "  1. ‡¶™‡ßç‡¶∞‡¶æ‡¶ï‡ßç‡¶§‡¶®\n",
      "\n",
      "üí° TOP-10 RECOMMENDATIONS:\n",
      "  1. ‡¶´‡¶æ‡¶§‡¶æ‡¶ì‡ßü‡¶æ ‡¶ì ‡¶Æ‡¶æ‡¶∏‡¶æ‡¶á‡¶≤ ‡ß©‡ßü ‡¶ñ‡¶£‡ßç‡¶°                                     (score: 0.8660) \n",
      "  2. ‡¶´‡¶æ‡¶§‡¶æ‡¶ì‡ßü‡¶æ ‡¶ì ‡¶Æ‡¶æ‡¶∏‡¶æ‡¶á‡¶≤ ‡ß¨‡¶∑‡ßç‡¶† ‡¶ñ‡¶£‡ßç‡¶°                                   (score: 0.8660) \n",
      "  3. ‡¶´‡¶æ‡¶§‡¶æ‡¶ì‡ßü‡¶æ ‡¶ì ‡¶Æ‡¶æ‡¶∏‡¶æ‡¶á‡¶≤ ‡ß™‡¶∞‡ßç‡¶• ‡¶ñ‡¶£‡ßç‡¶°                                   (score: 0.8660) \n",
      "  4. ‡¶´‡¶æ‡¶§‡¶æ‡¶ì‡ßü‡¶æ ‡¶ì ‡¶Æ‡¶æ‡¶∏‡¶æ‡¶á‡¶≤ ‡ß´‡¶Æ ‡¶ñ‡¶£‡ßç‡¶°                                     (score: 0.8660) \n",
      "  5. ‡¶´‡¶æ‡¶§‡¶æ‡¶ì‡ßü‡¶æ ‡¶ì ‡¶Æ‡¶æ‡¶∏‡¶æ‡¶á‡¶≤ ‡ßß‡¶Æ ‡¶ì ‡ß®‡ßü ‡¶ñ‡¶£‡ßç‡¶°                                (score: 0.8253) \n",
      "  6. ‡¶§‡¶æ‡¶´‡¶∏‡ßÄ‡¶∞‡ßá ‡¶§‡¶æ‡¶¨‡¶æ‡¶∞‡ßÄ ‡¶∂‡¶∞‡ßÄ‡¶´ ‡ß™‡¶∞‡ßç‡¶• ‡¶ñ‡¶£‡ßç‡¶°                                (score: 0.7071) \n",
      "  7. ‡¶§‡¶æ‡¶´‡¶∏‡ßÄ‡¶∞‡ßá ‡¶§‡¶æ‡¶¨‡¶æ‡¶∞‡ßÄ ‡¶∂‡¶∞‡ßÄ‡¶´ ‡ß¨‡¶∑‡ßç‡¶† ‡¶ñ‡¶£‡ßç‡¶°                                (score: 0.7071) \n",
      "  8. ‡¶∏‡ßÅ‡¶®‡¶æ‡¶®‡ßÅ ‡¶á‡¶¨‡¶®‡ßá ‡¶Æ‡¶æ‡¶ú‡¶æ‡¶π‡ßç                                           (score: 0.7071) \n",
      "  9. ‡¶´‡¶ø‡¶ï‡ßç‚Äå‡¶π‡ßÅ‡¶∏ ‡¶∏‡ßÅ‡¶®‡¶æ‡¶®‡¶ø ‡¶ì‡ßü‡¶æ‡¶≤ ‡¶Ü‡¶õ‡¶æ‡¶∞ ‡ßß‡¶Æ ‡¶ñ‡¶£‡ßç‡¶°                            (score: 0.7071) \n",
      "  10. ‡¶á‡¶∏‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶¶‡ßá‡¶∂ ‡¶®‡¶ø‡¶∑‡ßá‡¶ß                                           (score: 0.7071) \n",
      "\n",
      "üìä USER METRICS:\n",
      "  ‚Ä¢ Hit@10: 0.0\n",
      "  ‚Ä¢ MRR: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DEMONSTRATION: SHOWING ACTUAL RECOMMENDATIONS VS GROUND TRUTH (TEST SET)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select a few random test users to demonstrate\n",
    "if len(test_users_with_profiles) > 0:\n",
    "    np.random.seed(42)\n",
    "    demo_users = np.random.choice(test_users_with_profiles, min(5, len(test_users_with_profiles)), replace=False)\n",
    "    \n",
    "    for user_idx, user_id in enumerate(demo_users):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"USER {user_idx + 1}: {user_id}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Get user's training books\n",
    "        train_books_list = train_df[train_df['user_id'] == user_id]['book_id'].values\n",
    "        print(f\"\\nüìö TRAINING BOOKS (what user read): {len(train_books_list)} books\")\n",
    "        for i, book_id in enumerate(train_books_list[:3]):  # Show first 3\n",
    "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
    "            print(f\"  {i+1}. {book_info['book_title']}\")\n",
    "        if len(train_books_list) > 3:\n",
    "            print(f\"  ... and {len(train_books_list) - 3} more\")\n",
    "        \n",
    "        # Get user's test books (ground truth)\n",
    "        test_books_list = test_df[test_df['user_id'] == user_id]['book_id'].values\n",
    "        print(f\"\\nüéØ GROUND TRUTH (actual test books): {len(test_books_list)} books\")\n",
    "        for i, book_id in enumerate(test_books_list):\n",
    "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
    "            print(f\"  {i+1}. {book_info['book_title']}\")\n",
    "        \n",
    "        # Get recommendations\n",
    "        user_profile = user_profiles[user_id]\n",
    "        train_books_set = set(train_books_list)\n",
    "        recommendations = get_recommendations(\n",
    "            user_id, user_profile, item_features, book_to_idx,\n",
    "            train_books_set, top_k=10\n",
    "        )\n",
    "        \n",
    "        # Display top-10 recommendations\n",
    "        print(f\"\\nüí° TOP-10 RECOMMENDATIONS:\")\n",
    "        hits = []\n",
    "        for i, (book_id, score) in enumerate(recommendations[:10]):\n",
    "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
    "            is_hit = \"‚úì HIT!\" if book_id in test_books_list else \"\"\n",
    "            print(f\"  {i+1}. {book_info['book_title'][:60]:60s} (score: {score:.4f}) {is_hit}\")\n",
    "            if book_id in test_books_list:\n",
    "                hits.append(i+1)\n",
    "        \n",
    "        # Show metrics for this user\n",
    "        hit10 = 1.0 if len(hits) > 0 else 0.0\n",
    "        mrr_val = (1.0 / hits[0]) if hits else 0.0\n",
    "        \n",
    "        print(f\"\\nüìä USER METRICS:\")\n",
    "        print(f\"  ‚Ä¢ Hit@10: {hit10}\")\n",
    "        print(f\"  ‚Ä¢ MRR: {mrr_val:.4f}\")\n",
    "        if hits:\n",
    "            print(f\"  ‚Ä¢ Hits at positions: {hits}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No test users available for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T10:35:14.222489Z",
     "iopub.status.busy": "2026-02-07T10:35:14.221686Z",
     "iopub.status.idle": "2026-02-07T10:35:14.249062Z",
     "shell.execute_reply": "2026-02-07T10:35:14.248426Z",
     "shell.execute_reply.started": "2026-02-07T10:35:14.222453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERALL PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VALIDATION SET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä VALIDATION HIT COVERAGE STATISTICS:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Total Validation Users: 6227\n",
      "  ‚Ä¢ Users with Hit@5: 1080 (17.3%)\n",
      "  ‚Ä¢ Users with Hit@10: 1446 (23.2%)\n",
      "  ‚Ä¢ Users with Hit@50: 2371 (38.1%)\n",
      "\n",
      "üìà VALIDATION AVERAGE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Hit@5:    0.1734\n",
      "  ‚Ä¢ Hit@10:   0.2322\n",
      "  ‚Ä¢ Hit@50:   0.3808\n",
      "  ‚Ä¢ MRR:      0.1118\n",
      "  ‚Ä¢ NDCG@10:  0.1085\n",
      "  ‚Ä¢ NDCG@50:  0.1374\n",
      "\n",
      "================================================================================\n",
      "TEST SET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä TEST HIT COVERAGE STATISTICS:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Total Test Users: 29368\n",
      "  ‚Ä¢ Users with Hit@5: 7501 (25.5%)\n",
      "  ‚Ä¢ Users with Hit@10: 9242 (31.5%)\n",
      "  ‚Ä¢ Users with Hit@50: 12844 (43.7%)\n",
      "\n",
      "üìà TEST AVERAGE METRICS:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Hit@5:    0.2554\n",
      "  ‚Ä¢ Hit@10:   0.3147\n",
      "  ‚Ä¢ Hit@50:   0.4373\n",
      "  ‚Ä¢ MRR:      0.1803\n",
      "  ‚Ä¢ NDCG@10:  0.1706\n",
      "  ‚Ä¢ NDCG@50:  0.1968\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EVALUATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(val_metrics['hit@5']) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION SET SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Validation statistics\n",
    "    users_with_hit5_val = sum([1 for x in val_metrics['hit@5'] if x > 0])\n",
    "    users_with_hit10_val = sum([1 for x in val_metrics['hit@10'] if x > 0])\n",
    "    users_with_hit50_val = sum([1 for x in val_metrics['hit@50'] if x > 0])\n",
    "    total_val_users = len(val_metrics['hit@5'])\n",
    "    \n",
    "    print(f\"\\nüìä VALIDATION HIT COVERAGE STATISTICS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  ‚Ä¢ Total Validation Users: {total_val_users}\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@5: {users_with_hit5_val} ({users_with_hit5_val/total_val_users*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@10: {users_with_hit10_val} ({users_with_hit10_val/total_val_users*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@50: {users_with_hit50_val} ({users_with_hit50_val/total_val_users*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà VALIDATION AVERAGE METRICS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  ‚Ä¢ Hit@5:    {np.mean(val_metrics['hit@5']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Hit@10:   {np.mean(val_metrics['hit@10']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Hit@50:   {np.mean(val_metrics['hit@50']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ MRR:      {np.mean(val_metrics['mrr']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ NDCG@10:  {np.mean(val_metrics['ndcg@10']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ NDCG@50:  {np.mean(val_metrics['ndcg@50']):.4f}\")\n",
    "\n",
    "if len(test_metrics['hit@5']) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST SET SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test statistics\n",
    "    users_with_hit5_test = sum([1 for x in test_metrics['hit@5'] if x > 0])\n",
    "    users_with_hit10_test = sum([1 for x in test_metrics['hit@10'] if x > 0])\n",
    "    users_with_hit50_test = sum([1 for x in test_metrics['hit@50'] if x > 0])\n",
    "    total_test_users = len(test_metrics['hit@5'])\n",
    "    \n",
    "    print(f\"\\nüìä TEST HIT COVERAGE STATISTICS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  ‚Ä¢ Total Test Users: {total_test_users}\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@5: {users_with_hit5_test} ({users_with_hit5_test/total_test_users*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@10: {users_with_hit10_test} ({users_with_hit10_test/total_test_users*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Users with Hit@50: {users_with_hit50_test} ({users_with_hit50_test/total_test_users*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà TEST AVERAGE METRICS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  ‚Ä¢ Hit@5:    {np.mean(test_metrics['hit@5']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Hit@10:   {np.mean(test_metrics['hit@10']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Hit@50:   {np.mean(test_metrics['hit@50']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ MRR:      {np.mean(test_metrics['mrr']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ NDCG@10:  {np.mean(test_metrics['ndcg@10']):.4f}\")\n",
    "    print(f\"  ‚Ä¢ NDCG@50:  {np.mean(test_metrics['ndcg@50']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T10:35:29.015131Z",
     "iopub.status.busy": "2026-02-07T10:35:29.014409Z",
     "iopub.status.idle": "2026-02-07T10:35:29.021871Z",
     "shell.execute_reply": "2026-02-07T10:35:29.021177Z",
     "shell.execute_reply.started": "2026-02-07T10:35:29.015097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONTENT FEATURE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä FEATURE DISTRIBUTION IN MODEL:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Author features:      16572 (64.19%)\n",
      "  ‚Ä¢ Category features:     1493 ( 5.78%)\n",
      "  ‚Ä¢ Publisher features:    2752 (10.66%)\n",
      "  ‚Ä¢ Review features:       5000 (19.37%)\n",
      "  ‚Ä¢ TOTAL:                25817 (100.00%)\n",
      "\n",
      "üîç FEATURE SPACE ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚Ä¢ Feature sparsity: 0.9987\n",
      "  ‚Ä¢ Non-zero elements: 4,212,492\n",
      "  ‚Ä¢ Average non-zero per book: 33.09\n",
      "\n",
      "================================================================================\n",
      "üéâ CONTENT-BASED RECOMMENDATION SYSTEM COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONTENT FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate feature importance based on feature ranges\n",
    "author_importance = author_features.shape[1]\n",
    "category_importance = category_features.shape[1]\n",
    "publisher_importance = publisher_features.shape[1]\n",
    "review_importance = review_features.shape[1]\n",
    "\n",
    "total_features = author_importance + category_importance + publisher_importance + review_importance\n",
    "\n",
    "print(f\"\\nüìä FEATURE DISTRIBUTION IN MODEL:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  ‚Ä¢ Author features:     {author_importance:6d} ({author_importance/total_features*100:5.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Category features:   {category_importance:6d} ({category_importance/total_features*100:5.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Publisher features:  {publisher_importance:6d} ({publisher_importance/total_features*100:5.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Review features:     {review_importance:6d} ({review_importance/total_features*100:5.2f}%)\")\n",
    "print(f\"  ‚Ä¢ TOTAL:               {total_features:6d} (100.00%)\")\n",
    "\n",
    "print(f\"\\nüîç FEATURE SPACE ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  ‚Ä¢ Feature sparsity: {1 - (item_features.nnz / (item_features.shape[0] * item_features.shape[1])):.4f}\")\n",
    "print(f\"  ‚Ä¢ Non-zero elements: {item_features.nnz:,}\")\n",
    "print(f\"  ‚Ä¢ Average non-zero per book: {item_features.nnz / item_features.shape[0]:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ CONTENT-BASED RECOMMENDATION SYSTEM COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9434225,
     "sourceId": 14760119,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
