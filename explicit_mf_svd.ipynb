{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge scikit-surprise -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8de2054-94ba-45af-bf5d-6cb515c5c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\envs\\recsys2\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from surprise import Dataset, Reader, SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utility Loader\n",
    "# ---------------------------------------\n",
    "\n",
    "def load_json(path: str):\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFRecommender():\n",
    "    \"\"\"\n",
    "    Explicit Matrix Factorization using SVD (via Surprise).\n",
    "    Train-only ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_user_book_rating: Dict[str, Dict[str, float]]):\n",
    "        \"\"\"\n",
    "        Input format:\n",
    "            train_user_book_rating[user][book] = rating\n",
    "        \"\"\"\n",
    "        self.train_data = train_user_book_rating\n",
    "\n",
    "    def fit(self):\n",
    "        print(\"Training Explicit MF (SVD)...\")\n",
    "\n",
    "        records = []\n",
    "        for user, book_dict in self.train_data.items():\n",
    "            for book, rating in book_dict.items():\n",
    "                records.append([user, book, float(rating)])\n",
    "\n",
    "        reader = Reader(rating_scale=(1, 5))\n",
    "        self.data = Dataset.load_from_df(\n",
    "            pd.DataFrame(records, columns=[\"user\", \"item\", \"rating\"]),\n",
    "            reader,\n",
    "        )\n",
    "\n",
    "        trainset = self.data.build_full_trainset()\n",
    "\n",
    "        self.model = SVD(random_state=42)\n",
    "        self.model.fit(trainset)\n",
    "\n",
    "        self.all_items = set({b for u in self.train_data for b in self.train_data[u]})\n",
    "\n",
    "    def recommend(self, user_id: str, k: int = 10, n_candidates: int = 1000):\n",
    "        if user_id not in self.train_data:\n",
    "            return []\n",
    "    \n",
    "        seen = set(self.train_data[user_id].keys())\n",
    "    \n",
    "        # Sample candidates instead of full catalog\n",
    "        unseen_items = list(self.all_items - seen)\n",
    "    \n",
    "        if len(unseen_items) > n_candidates:\n",
    "            candidates = random.sample(unseen_items, n_candidates)\n",
    "        else:\n",
    "            candidates = unseen_items\n",
    "    \n",
    "        scores = []\n",
    "        for book in candidates:\n",
    "            est = self.model.predict(user_id, book).est\n",
    "            scores.append((book, est))\n",
    "    \n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [b for b, _ in scores[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates ranking-based recommendation metrics.\n",
    "    Computes: Hit@K, MRR@K, NDCG@K for various K values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def hit_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        Hit@K: 1 if at least one relevant item is in top-K, else 0\n",
    "        \"\"\"\n",
    "        top_k = predictions[:k]\n",
    "        return 1.0 if any(item in ground_truth for item in top_k) else 0.0\n",
    "    \n",
    "    def mrr_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        MRR@K: Reciprocal rank of first relevant item in top-K\n",
    "        \"\"\"\n",
    "        top_k = predictions[:k]\n",
    "        for rank, item in enumerate(top_k, start=1):\n",
    "            if item in ground_truth:\n",
    "                return 1.0 / rank\n",
    "        return 0.0\n",
    "    \n",
    "    def dcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        DCG@K: Discounted Cumulative Gain\n",
    "        \"\"\"\n",
    "        dcg = 0.0\n",
    "        top_k = predictions[:k]\n",
    "        for rank, item in enumerate(top_k, start=1):\n",
    "            if item in ground_truth:\n",
    "                dcg += 1.0 / np.log2(rank + 1)\n",
    "        return dcg\n",
    "    \n",
    "    def idcg_at_k(self, ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        IDCG@K: Ideal DCG (best possible DCG)\n",
    "        \"\"\"\n",
    "        ideal_k = min(len(ground_truth), k)\n",
    "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_k + 1))\n",
    "        return idcg\n",
    "    \n",
    "    def ndcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
    "        \"\"\"\n",
    "        NDCG@K: Normalized Discounted Cumulative Gain\n",
    "        \"\"\"\n",
    "        dcg = self.dcg_at_k(predictions, ground_truth, k)\n",
    "        idcg = self.idcg_at_k(ground_truth, k)\n",
    "        \n",
    "        if idcg == 0.0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dcg / idcg\n",
    "    \n",
    "    def evaluate(self, predictions: Dict[str, List[str]], ground_truth: Dict[str, Set[str]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate predictions against ground truth.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Dict mapping user_id -> list of recommended item_ids (ranked)\n",
    "            ground_truth: Dict mapping user_id -> set of relevant item_ids\n",
    "            \n",
    "        Returns:\n",
    "            Dict of metric_name -> average_score\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'Hit@5': [],\n",
    "            'Hit@10': [],\n",
    "            'Hit@50': [],\n",
    "            'MRR@10': [],\n",
    "            'NDCG@10': [],\n",
    "            'NDCG@50': []\n",
    "        }\n",
    "        \n",
    "        # Only evaluate users present in both predictions and ground_truth\n",
    "        common_users = set(predictions.keys()) & set(ground_truth.keys())\n",
    "        \n",
    "        for user_id in common_users:\n",
    "            preds = predictions[user_id]\n",
    "            gt = ground_truth[user_id]\n",
    "            \n",
    "            # Skip users with no ground truth items\n",
    "            if len(gt) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Compute metrics\n",
    "            metrics['Hit@5'].append(self.hit_at_k(preds, gt, 5))\n",
    "            metrics['Hit@10'].append(self.hit_at_k(preds, gt, 10))\n",
    "            metrics['Hit@50'].append(self.hit_at_k(preds, gt, 50))\n",
    "            metrics['MRR@10'].append(self.mrr_at_k(preds, gt, 10))\n",
    "            metrics['NDCG@10'].append(self.ndcg_at_k(preds, gt, 10))\n",
    "            metrics['NDCG@50'].append(self.ndcg_at_k(preds, gt, 50))\n",
    "        \n",
    "        # Average across all users\n",
    "        results = {}\n",
    "        for metric_name, values in metrics.items():\n",
    "            if len(values) > 0:\n",
    "                results[metric_name] = np.mean(values)\n",
    "            else:\n",
    "                results[metric_name] = 0.0\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_book_interactions(\n",
    "    user_to_review_path: str,\n",
    "    book_to_review_path: str,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "        user_id -> [book_id, book_id, ...]\n",
    "    \"\"\"\n",
    "    user_to_review = load_json(user_to_review_path)\n",
    "    book_to_review = load_json(book_to_review_path)\n",
    "\n",
    "    # Normalize IDs to string\n",
    "    review_to_user = {\n",
    "        str(x[\"review_id\"]): str(x[\"user_id\"])\n",
    "        for x in user_to_review\n",
    "    }\n",
    "\n",
    "    review_to_book = {\n",
    "        str(x[\"review_id\"]): str(x[\"book_id\"])\n",
    "        for x in book_to_review\n",
    "    }\n",
    "\n",
    "    user_book = defaultdict(list)\n",
    "\n",
    "    for rid, user_id in review_to_user.items():\n",
    "        if rid in review_to_book:\n",
    "            book_id = review_to_book[rid]\n",
    "            user_book[user_id].append(book_id)\n",
    "\n",
    "    return user_book\n",
    "\n",
    "\n",
    "def split_user_interactions(\n",
    "    user_book: Dict[str, List[str]],\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        train_user_book\n",
    "        val_user_book\n",
    "        test_user_book\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    train = {}\n",
    "    val = {}\n",
    "    test = {}\n",
    "\n",
    "    for user, books in user_book.items():\n",
    "        books = list(set(books))  # remove duplicates\n",
    "        random.shuffle(books)\n",
    "\n",
    "        n = len(books)\n",
    "        if n < 3:\n",
    "            train[user] = books\n",
    "            val[user] = []\n",
    "            test[user] = []\n",
    "            continue\n",
    "\n",
    "        n_train = int(0.7 * n)\n",
    "        n_val = int(0.15 * n)\n",
    "\n",
    "        train[user] = books[:n_train]\n",
    "        val[user] = books[n_train : n_train + n_val]\n",
    "        test[user] = books[n_train + n_val :]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def build_ground_truth(test_user_book: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    return {\n",
    "        user: set(books)\n",
    "        for user, books in test_user_book.items()\n",
    "        if len(books) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_predictions(model, users: List[str], k: int):\n",
    "    predictions = {}\n",
    "\n",
    "    for u in tqdm(users, desc=\"Generating Predictions\"):\n",
    "        preds = model.recommend(u, k)\n",
    "        predictions[u] = preds\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def build_train_user_book_rating(\n",
    "    train_user_book,\n",
    "    user_to_review_path,\n",
    "    book_to_review_path,\n",
    "    review_path,\n",
    "):\n",
    "    user_to_review = load_json(user_to_review_path)\n",
    "    book_to_review = load_json(book_to_review_path)\n",
    "    reviews = load_json(review_path)\n",
    "\n",
    "    review_to_user = {str(x[\"review_id\"]): str(x[\"user_id\"]) for x in user_to_review}\n",
    "    review_to_book = {str(x[\"review_id\"]): str(x[\"book_id\"]) for x in book_to_review}\n",
    "    review_to_rating = {str(x[\"review_id\"]): float(x[\"user_rating\"]) for x in reviews}\n",
    "\n",
    "    train_ratings = defaultdict(dict)\n",
    "\n",
    "    for rid, user in review_to_user.items():\n",
    "        if rid in review_to_book and rid in review_to_rating:\n",
    "            book = review_to_book[rid]\n",
    "            rating = review_to_rating[rid]\n",
    "\n",
    "            if rating > 0 and user in train_user_book and book in train_user_book[user]:\n",
    "                train_ratings[user][book] = rating\n",
    "\n",
    "    return train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users with interactions: 63721\n",
      "Users in test set: 15427\n",
      "\n",
      "Training Explicit MF (SVD)...\n",
      "Training Explicit MF (SVD)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 15427/15427 [02:35<00:00, 99.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Explicit MF Results =====\n",
      "Hit@5: 0.0033\n",
      "Hit@10: 0.0063\n",
      "Hit@50: 0.0063\n",
      "MRR@10: 0.0020\n",
      "NDCG@10: 0.0019\n",
      "NDCG@50: 0.0019\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # -------------------------------\n",
    "    # Paths\n",
    "    # -------------------------------\n",
    "\n",
    "    DATA_FOLDER = \"RokomariBG_Dataset/\"\n",
    "    USER_TO_REVIEW = DATA_FOLDER+\"user_to_review.json.gz\"\n",
    "    BOOK_TO_REVIEW = DATA_FOLDER+\"book_to_review.json.gz\"\n",
    "    REVIEW = DATA_FOLDER + \"review.json.gz\"\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    # -------------------------------\n",
    "    # Build user-book interactions\n",
    "    # -------------------------------\n",
    "    user_book = build_user_book_interactions(\n",
    "        USER_TO_REVIEW,\n",
    "        BOOK_TO_REVIEW,\n",
    "    )\n",
    "\n",
    "    print(f\"Total users with interactions: {len(user_book)}\")\n",
    "\n",
    "    evaluator = RankingEvaluator()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Split 70/15/15\n",
    "    # -------------------------------\n",
    "    train_user_book, val_user_book, test_user_book = split_user_interactions(\n",
    "        user_book\n",
    "    )\n",
    "\n",
    "    ground_truth = build_ground_truth(test_user_book)\n",
    "    test_users = list(ground_truth.keys())\n",
    "\n",
    "    print(f\"Users in test set: {len(test_users)}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # EXPLICIT MF (SVD)\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nTraining Explicit MF (SVD)...\")\n",
    "\n",
    "    train_user_book_rating = build_train_user_book_rating(\n",
    "        train_user_book=train_user_book,\n",
    "        user_to_review_path=USER_TO_REVIEW,\n",
    "        book_to_review_path=BOOK_TO_REVIEW,\n",
    "        review_path=REVIEW,\n",
    "    )\n",
    "\n",
    "    explicit_mf = ExplicitMFRecommender(train_user_book_rating)\n",
    "    explicit_mf.fit()\n",
    "\n",
    "    explicit_preds = generate_predictions(explicit_mf, test_users, K)\n",
    "    explicit_metrics = evaluator.evaluate(explicit_preds, ground_truth)\n",
    "\n",
    "    print(\"\\n===== Explicit MF Results =====\")\n",
    "    for m, v in explicit_metrics.items():\n",
    "        print(f\"{m}: {v:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9c8a0-4a09-439a-a591-25eb44c4f4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
