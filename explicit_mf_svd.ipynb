{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge scikit-surprise -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from surprise import Dataset, Reader, SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utility Loader\n",
    "# ---------------------------------------\n",
    "\n",
    "def load_json(path: str):\n",
    "    with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMFRecommender():\n",
    "    \"\"\"\n",
    "    Explicit Matrix Factorization using SVD (via Surprise).\n",
    "    Train-only ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_user_book_rating: Dict[str, Dict[str, float]]):\n",
    "        \"\"\"\n",
    "        Input format:\n",
    "            train_user_book_rating[user][book] = rating\n",
    "        \"\"\"\n",
    "        self.train_data = train_user_book_rating\n",
    "\n",
    "    def fit(self):\n",
    "        print(\"Training Explicit MF (SVD)...\")\n",
    "\n",
    "        records = []\n",
    "        for user, book_dict in self.train_data.items():\n",
    "            for book, rating in book_dict.items():\n",
    "                records.append([user, book, float(rating)])\n",
    "\n",
    "        reader = Reader(rating_scale=(1, 5))\n",
    "        self.data = Dataset.load_from_df(\n",
    "            pd.DataFrame(records, columns=[\"user\", \"item\", \"rating\"]),\n",
    "            reader,\n",
    "        )\n",
    "\n",
    "        trainset = self.data.build_full_trainset()\n",
    "\n",
    "        self.model = SVD(random_state=42)\n",
    "        self.model.fit(trainset)\n",
    "\n",
    "        self.all_items = set({b for u in self.train_data for b in self.train_data[u]})\n",
    "\n",
    "    def recommend(self, user_id: str, k: int = 10, n_candidates: int = 1000):\n",
    "        if user_id not in self.train_data:\n",
    "            return []\n",
    "    \n",
    "        seen = set(self.train_data[user_id].keys())\n",
    "    \n",
    "        # Sample candidates instead of full catalog\n",
    "        unseen_items = list(self.all_items - seen)\n",
    "    \n",
    "        if len(unseen_items) > n_candidates:\n",
    "            candidates = random.sample(unseen_items, n_candidates)\n",
    "        else:\n",
    "            candidates = unseen_items\n",
    "    \n",
    "        scores = []\n",
    "        for book in candidates:\n",
    "            est = self.model.predict(user_id, book).est\n",
    "            scores.append((book, est))\n",
    "    \n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [b for b, _ in scores[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "evaluator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluation class for Top-N Recommendation.\n",
    "\n",
    "    Supports:\n",
    "        - Hit@K\n",
    "        - MRR@K\n",
    "        - NDCG@K\n",
    "\n",
    "    Input format:\n",
    "        predictions: Dict[user_id, List[item_id]]\n",
    "        ground_truth: Dict[user_id, Set[item_id]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int\n",
    "            Cutoff rank for evaluation (e.g., 5, 10, 20)\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    # -------------------------\n",
    "    # Hit@K\n",
    "    # -------------------------\n",
    "    def hit_at_k(self, predictions: Dict, ground_truth: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Computes Hit@K\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float : Average Hit@K over users\n",
    "        \"\"\"\n",
    "        hits = []\n",
    "\n",
    "        for user in ground_truth:\n",
    "            if user not in predictions:\n",
    "                continue\n",
    "\n",
    "            top_k = predictions[user][:self.k]\n",
    "            gt_items = ground_truth[user]\n",
    "\n",
    "            hit = 1.0 if any(item in gt_items for item in top_k) else 0.0\n",
    "            hits.append(hit)\n",
    "\n",
    "        return float(np.mean(hits)) if hits else 0.0\n",
    "\n",
    "    # -------------------------\n",
    "    # MRR@K\n",
    "    # -------------------------\n",
    "    def mrr_at_k(self, predictions: Dict, ground_truth: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Computes Mean Reciprocal Rank (MRR@K)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float : Average MRR over users\n",
    "        \"\"\"\n",
    "        rr_scores = []\n",
    "\n",
    "        for user in ground_truth:\n",
    "            if user not in predictions:\n",
    "                continue\n",
    "\n",
    "            top_k = predictions[user][:self.k]\n",
    "            gt_items = ground_truth[user]\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank, item in enumerate(top_k, start=1):\n",
    "                if item in gt_items:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "\n",
    "            rr_scores.append(rr)\n",
    "\n",
    "        return float(np.mean(rr_scores)) if rr_scores else 0.0\n",
    "\n",
    "    # -------------------------\n",
    "    # NDCG@K\n",
    "    # -------------------------\n",
    "    def ndcg_at_k(self, predictions: Dict, ground_truth: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Computes Normalized Discounted Cumulative Gain (NDCG@K)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float : Average NDCG@K over users\n",
    "        \"\"\"\n",
    "        ndcg_scores = []\n",
    "\n",
    "        for user in tqdm(ground_truth, desc=\"Evaluating\"):\n",
    "            if user not in predictions:\n",
    "                continue\n",
    "\n",
    "            top_k = predictions[user][:self.k]\n",
    "            gt_items = ground_truth[user]\n",
    "\n",
    "            dcg = 0.0\n",
    "            for i, item in enumerate(top_k):\n",
    "                if item in gt_items:\n",
    "                    dcg += 1.0 / math.log2(i + 2)\n",
    "\n",
    "            ideal_hits = min(len(gt_items), self.k)\n",
    "            idcg = sum(1.0 / math.log2(i + 2) for i in range(ideal_hits))\n",
    "\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "        return float(np.mean(ndcg_scores)) if ndcg_scores else 0.0\n",
    "\n",
    "    # -------------------------\n",
    "    # Combined Evaluation\n",
    "    # -------------------------\n",
    "    def evaluate(self, predictions: Dict, ground_truth: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Computes all metrics together.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "        \"\"\"\n",
    "        return {\n",
    "            f\"Hit@{self.k}\": self.hit_at_k(predictions, ground_truth),\n",
    "            f\"MRR@{self.k}\": self.mrr_at_k(predictions, ground_truth),\n",
    "            f\"NDCG@{self.k}\": self.ndcg_at_k(predictions, ground_truth),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_book_interactions(\n",
    "    user_to_review_path: str,\n",
    "    book_to_review_path: str,\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Builds:\n",
    "        user_id -> [book_id, book_id, ...]\n",
    "    \"\"\"\n",
    "    user_to_review = load_json(user_to_review_path)\n",
    "    book_to_review = load_json(book_to_review_path)\n",
    "\n",
    "    # Normalize IDs to string\n",
    "    review_to_user = {\n",
    "        str(x[\"review_id\"]): str(x[\"user_id\"])\n",
    "        for x in user_to_review\n",
    "    }\n",
    "\n",
    "    review_to_book = {\n",
    "        str(x[\"review_id\"]): str(x[\"book_id\"])\n",
    "        for x in book_to_review\n",
    "    }\n",
    "\n",
    "    user_book = defaultdict(list)\n",
    "\n",
    "    for rid, user_id in review_to_user.items():\n",
    "        if rid in review_to_book:\n",
    "            book_id = review_to_book[rid]\n",
    "            user_book[user_id].append(book_id)\n",
    "\n",
    "    return user_book\n",
    "\n",
    "\n",
    "def split_user_interactions(\n",
    "    user_book: Dict[str, List[str]],\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        train_user_book\n",
    "        val_user_book\n",
    "        test_user_book\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    train = {}\n",
    "    val = {}\n",
    "    test = {}\n",
    "\n",
    "    for user, books in user_book.items():\n",
    "        books = list(set(books))  # remove duplicates\n",
    "        random.shuffle(books)\n",
    "\n",
    "        n = len(books)\n",
    "        if n < 3:\n",
    "            train[user] = books\n",
    "            val[user] = []\n",
    "            test[user] = []\n",
    "            continue\n",
    "\n",
    "        n_train = int(0.7 * n)\n",
    "        n_val = int(0.15 * n)\n",
    "\n",
    "        train[user] = books[:n_train]\n",
    "        val[user] = books[n_train : n_train + n_val]\n",
    "        test[user] = books[n_train + n_val :]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def build_ground_truth(test_user_book: Dict[str, List[str]]) -> Dict[str, Set[str]]:\n",
    "    return {\n",
    "        user: set(books)\n",
    "        for user, books in test_user_book.items()\n",
    "        if len(books) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_predictions(model, users: List[str], k: int):\n",
    "    predictions = {}\n",
    "\n",
    "    for u in tqdm(users, desc=\"Generating Predictions\"):\n",
    "        preds = model.recommend(u, k)\n",
    "        predictions[u] = preds\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def build_train_user_book_rating(\n",
    "    train_user_book,\n",
    "    user_to_review_path,\n",
    "    book_to_review_path,\n",
    "    review_path,\n",
    "):\n",
    "    user_to_review = load_json(user_to_review_path)\n",
    "    book_to_review = load_json(book_to_review_path)\n",
    "    reviews = load_json(review_path)\n",
    "\n",
    "    review_to_user = {str(x[\"review_id\"]): str(x[\"user_id\"]) for x in user_to_review}\n",
    "    review_to_book = {str(x[\"review_id\"]): str(x[\"book_id\"]) for x in book_to_review}\n",
    "    review_to_rating = {str(x[\"review_id\"]): float(x[\"user_rating\"]) for x in reviews}\n",
    "\n",
    "    train_ratings = defaultdict(dict)\n",
    "\n",
    "    for rid, user in review_to_user.items():\n",
    "        if rid in review_to_book and rid in review_to_rating:\n",
    "            book = review_to_book[rid]\n",
    "            rating = review_to_rating[rid]\n",
    "\n",
    "            if rating > 0 and user in train_user_book and book in train_user_book[user]:\n",
    "                train_ratings[user][book] = rating\n",
    "\n",
    "    return train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users with interactions: 63721\n",
      "Users in test set: 15427\n",
      "\n",
      "Training Explicit MF (SVD)...\n",
      "Training Explicit MF (SVD)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions:  85%|███████████████████████████████████████████▏       | 13061/15427 [01:41<00:19, 121.39it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # -------------------------------\n",
    "    # Paths\n",
    "    # -------------------------------\n",
    "    DATA_FOLDER = r\"E:/RokomariBG_Dataset\"\n",
    "    USER_TO_REVIEW = DATA_FOLDER + r\"/user_to_review.json.gz\"\n",
    "    BOOK_TO_REVIEW = DATA_FOLDER + r\"/book_to_review.json.gz\"\n",
    "    REVIEW = DATA_FOLDER + r\"/review.json.gz\"\n",
    "\n",
    "    K = 10\n",
    "\n",
    "    # -------------------------------\n",
    "    # Build user-book interactions\n",
    "    # -------------------------------\n",
    "    user_book = build_user_book_interactions(\n",
    "        USER_TO_REVIEW,\n",
    "        BOOK_TO_REVIEW,\n",
    "    )\n",
    "\n",
    "    print(f\"Total users with interactions: {len(user_book)}\")\n",
    "\n",
    "    evaluator = RankingEvaluator(k=K)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Split 70/15/15\n",
    "    # -------------------------------\n",
    "    train_user_book, val_user_book, test_user_book = split_user_interactions(\n",
    "        user_book\n",
    "    )\n",
    "\n",
    "    ground_truth = build_ground_truth(test_user_book)\n",
    "    test_users = list(ground_truth.keys())\n",
    "\n",
    "    print(f\"Users in test set: {len(test_users)}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # EXPLICIT MF (SVD)\n",
    "    # =====================================================\n",
    "\n",
    "    print(\"\\nTraining Explicit MF (SVD)...\")\n",
    "\n",
    "    train_user_book_rating = build_train_user_book_rating(\n",
    "        train_user_book=train_user_book,\n",
    "        user_to_review_path=USER_TO_REVIEW,\n",
    "        book_to_review_path=BOOK_TO_REVIEW,\n",
    "        review_path=REVIEW,\n",
    "    )\n",
    "\n",
    "    explicit_mf = ExplicitMFRecommender(train_user_book_rating)\n",
    "    explicit_mf.fit()\n",
    "\n",
    "    explicit_preds = generate_predictions(explicit_mf, test_users, K)\n",
    "    explicit_metrics = evaluator.evaluate(explicit_preds, ground_truth)\n",
    "\n",
    "    print(\"\\n===== Explicit MF Results =====\")\n",
    "    for m, v in explicit_metrics.items():\n",
    "        print(f\"{m}: {v:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d515024-5dcd-44d2-9ea5-b7e3b85c238f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
