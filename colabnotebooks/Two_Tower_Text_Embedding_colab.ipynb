{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/backlashblitz/Bangla-Book-Recommendation-Dataset/blob/main/colabnotebooks/Two_Tower_Text_Embedding_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYpcRAylJ8lc"
      },
      "outputs": [],
      "source": [
        "import os, shutil, subprocess\n",
        "\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"huggingface_hub\"], check=True)\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "REPO_ID = \"DevnilMaster1/Bangla-Book-Recommendation-Dataset\"\n",
        "DATA_FOLDER = \"RokomariBG_Dataset\"\n",
        "\n",
        "# Download the full dataset repo snapshot\n",
        "print(\"Downloading dataset from HuggingFace Hub...\")\n",
        "local_repo = snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\")\n",
        "print(f\"Repo downloaded to: {local_repo}\")\n",
        "\n",
        "FILES_NEEDED = [\n",
        "    \"author.json\",\n",
        "    \"book.json\",\n",
        "    \"category.json\",\n",
        "    \"publisher.json\",\n",
        "    \"review.json\",\n",
        "    \"book_to_author.json\",\n",
        "    \"book_to_category.json\",\n",
        "    \"book_to_publisher.json\",\n",
        "    \"book_to_review.json\",\n",
        "    \"user_to_review.json\",\n",
        "]\n",
        "\n",
        "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
        "\n",
        "# Walk the downloaded repo and find each needed file\n",
        "found = {}\n",
        "for root, dirs, files in os.walk(local_repo):\n",
        "    for fname in files:\n",
        "        if fname in FILES_NEEDED and fname not in found:\n",
        "            found[fname] = os.path.join(root, fname)\n",
        "\n",
        "for filename in FILES_NEEDED:\n",
        "    dest = os.path.join(DATA_FOLDER, filename)\n",
        "    if filename in found:\n",
        "        shutil.copy(found[filename], dest)\n",
        "        print(f\"âœ… Saved: {filename}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Not found in repo: {filename}\")\n",
        "\n",
        "base_dir = DATA_FOLDER\n",
        "print(\"\\nðŸŽ‰ Environment Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHaXcGO4KA9F"
      },
      "outputs": [],
      "source": [
        "# two_tower_rokomari.py\n",
        "# pip install torch sentence-transformers pandas numpy tqdm scikit-learn\n",
        "\n",
        "import os, json, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Paths:\n",
        "    author_json: str\n",
        "    book_json: str\n",
        "    category_json: str\n",
        "    publisher_json: str\n",
        "    review_json: str\n",
        "    book_to_author_json: str\n",
        "    book_to_category_json: str\n",
        "    book_to_publisher_json: str\n",
        "    book_to_review_json: str\n",
        "    user_to_review_json: str\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # embedding sizes\n",
        "    id_emb_dim: int = 128\n",
        "    text_emb_dim: int = 256  # multilingual-e5-base outputs 768; we can project down\n",
        "    out_dim: int = 256\n",
        "\n",
        "    # tower MLP hyperparams\n",
        "    item_mlp_layers: int = 2\n",
        "    user_mlp_layers: int = 2\n",
        "    item_hidden_dim: int = 256\n",
        "    user_hidden_dim: int = 256\n",
        "    dropout: float = 0.1\n",
        "    use_layernorm: bool = True\n",
        "\n",
        "\n",
        "    # training\n",
        "    batch_size: int = 256\n",
        "    epochs: int = 20\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 1e-5\n",
        "    max_history: int = 50  # for user history pooling\n",
        "\n",
        "    # eval\n",
        "    k_list: Tuple[int, ...] = (10, 20)\n",
        "    num_neg_eval: int = 200  # sampled negatives per user for fast eval\n",
        "\n",
        "    # text model\n",
        "    text_model_name: str = \"intfloat/multilingual-e5-large-instruct\"\n",
        "    text_cache_dir: str = \"./text_cache\"\n",
        "\n",
        "    # early stopping / selection\n",
        "    patience: int = 2  # stop if no val NDCG@10 improvement for this many epochs\n",
        "\n",
        "    # evaluation\n",
        "    eval_ks: Tuple[int, ...] = (5, 10, 50)\n",
        "    mrr_k: int = 10\n",
        "    ndcg_ks: Tuple[int, ...] = (10, 50)\n",
        "\n",
        "    # ---- Signal toggles (leave-one-out ablations) ----\n",
        "    use_interaction: bool = True     # user-item interaction / history pooling signal\n",
        "    use_side: bool = True            # side features = text + numeric\n",
        "    use_relations: bool = True       # relational knowledge = author/cat/pub\n",
        "    ablation_name: str = \"full\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Utilities: load JSON arrays\n",
        "# ----------------------------\n",
        "def load_json_array(path: str) -> List[dict]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def safe_str(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return str(x)\n",
        "\n",
        "def normalize_numeric(df: pd.DataFrame, cols: List[str]) -> Tuple[pd.DataFrame, Dict[str, Tuple[float,float]]]:\n",
        "    stats = {}\n",
        "    for c in cols:\n",
        "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "        mean = float(v.mean(skipna=True)) if v.notna().any() else 0.0\n",
        "        std = float(v.std(skipna=True)) if v.notna().any() else 1.0\n",
        "        if std == 0.0: std = 1.0\n",
        "        df[c] = (v.fillna(mean) - mean) / std\n",
        "        stats[c] = (mean, std)\n",
        "    return df, stats\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Build interaction table: (user_id, book_id) via review_id join\n",
        "# ----------------------------\n",
        "def build_interactions(paths: Paths) -> pd.DataFrame:\n",
        "    reviews = pd.DataFrame(load_json_array(paths.review_json))\n",
        "    reviews[\"review_id\"] = pd.to_numeric(reviews[\"review_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    u2r = pd.DataFrame(load_json_array(paths.user_to_review_json))\n",
        "    b2r = pd.DataFrame(load_json_array(paths.book_to_review_json))\n",
        "\n",
        "    u2r[\"review_id\"] = pd.to_numeric(u2r[\"review_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    b2r[\"review_id\"] = pd.to_numeric(b2r[\"review_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    b2r[\"book_id\"] = b2r[\"book_id\"].astype(str)\n",
        "\n",
        "    tmp = u2r.merge(reviews, on=\"review_id\", how=\"left\")\n",
        "    inter = tmp.merge(b2r[[\"book_id\",\"review_id\"]], on=\"review_id\", how=\"inner\")\n",
        "\n",
        "    inter = inter.dropna(subset=[\"user_id\", \"book_id\"])\n",
        "    inter[\"user_id\"] = inter[\"user_id\"].astype(str)\n",
        "\n",
        "    inter[\"verified_purchase\"] = inter.get(\"verified_purchase\", False).fillna(False).astype(bool)\n",
        "    inter[\"user_rating\"] = pd.to_numeric(inter.get(\"user_rating\", np.nan), errors=\"coerce\")\n",
        "\n",
        "    w = np.ones(len(inter), dtype=np.float32)\n",
        "    w += inter[\"verified_purchase\"].astype(np.float32) * 0.5\n",
        "    r = inter[\"user_rating\"].fillna(0).to_numpy()\n",
        "    r = np.clip(r / 5.0, 0, 1).astype(np.float32)\n",
        "    w += 0.2 * r\n",
        "    inter[\"weight\"] = w\n",
        "\n",
        "    inter = inter.sort_values(\"weight\", ascending=False).drop_duplicates([\"user_id\",\"book_id\"], keep=\"first\")\n",
        "    return inter[[\"user_id\",\"book_id\",\"weight\",\"review_id\"]].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Build item (book) features\n",
        "# ----------------------------\n",
        "def build_book_tables(paths: Paths) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    books = pd.DataFrame(load_json_array(paths.book_json))\n",
        "    authors = pd.DataFrame(load_json_array(paths.author_json))\n",
        "    cats = pd.DataFrame(load_json_array(paths.category_json))\n",
        "    pubs = pd.DataFrame(load_json_array(paths.publisher_json))\n",
        "    reviews = pd.DataFrame(load_json_array(paths.review_json))\n",
        "\n",
        "    books[\"book_id\"] = books[\"book_id\"].astype(str)\n",
        "    books = books.drop_duplicates(subset=[\"book_id\"], keep=\"first\").reset_index(drop=True)\n",
        "    if \"author_id\" in books.columns:\n",
        "        books[\"author_id\"] = books[\"author_id\"].astype(str)\n",
        "    if \"publisher_id\" in books.columns:\n",
        "        books[\"publisher_id\"] = books[\"publisher_id\"].astype(str)\n",
        "    if \"category_id\" in books.columns:\n",
        "        books[\"category_id\"] = books[\"category_id\"].astype(str)\n",
        "\n",
        "    if \"author_id\" in authors.columns:\n",
        "        authors[\"author_id\"] = authors[\"author_id\"].astype(str)\n",
        "        authors = authors.drop_duplicates(subset=[\"author_id\"], keep=\"first\").reset_index(drop=True)\n",
        "    if \"publisher_id\" in authors.columns:\n",
        "        authors[\"publisher_id\"] = authors[\"publisher_id\"].astype(str)\n",
        "\n",
        "    cats[\"category_id\"] = cats[\"category_id\"].astype(str)\n",
        "    cats = cats.drop_duplicates(subset=[\"category_id\"], keep=\"first\").reset_index(drop=True)\n",
        "    pubs[\"publisher_id\"] = pubs[\"publisher_id\"].astype(str)\n",
        "    pubs = pubs.drop_duplicates(subset=[\"publisher_id\"], keep=\"first\").reset_index(drop=True)\n",
        "\n",
        "    reviews[\"review_id\"] = pd.to_numeric(reviews[\"review_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return books, authors, cats, pubs, reviews\n",
        "\n",
        "\n",
        "def build_multi_maps(paths: Paths) -> Tuple[Dict[str, List[str]], Dict[str, List[str]], Dict[str, str]]:\n",
        "    b2a = pd.DataFrame(load_json_array(paths.book_to_author_json))\n",
        "    b2c = pd.DataFrame(load_json_array(paths.book_to_category_json))\n",
        "    b2p = pd.DataFrame(load_json_array(paths.book_to_publisher_json))\n",
        "\n",
        "    b2a[\"book_id\"] = b2a[\"book_id\"].astype(str)\n",
        "    b2a[\"author_id\"] = b2a[\"author_id\"].astype(str)\n",
        "\n",
        "    b2c[\"book_id\"] = b2c[\"book_id\"].astype(str)\n",
        "    b2c[\"category_id\"] = b2c[\"category_id\"].astype(str)\n",
        "\n",
        "    b2p[\"book_id\"] = b2p[\"book_id\"].astype(str)\n",
        "    b2p[\"publisher_id\"] = b2p[\"publisher_id\"].astype(str)\n",
        "\n",
        "    book_to_authors = b2a.groupby(\"book_id\")[\"author_id\"].apply(list).to_dict()\n",
        "    book_to_cats = b2c.groupby(\"book_id\")[\"category_id\"].apply(list).to_dict()\n",
        "    book_to_pub = b2p.groupby(\"book_id\")[\"publisher_id\"].first().to_dict()\n",
        "    return book_to_authors, book_to_cats, book_to_pub\n",
        "\n",
        "\n",
        "def make_book_text(\n",
        "    book_row: pd.Series,\n",
        "    authors_by_id: Dict[str, dict],\n",
        "    cats_by_id: Dict[str, dict],\n",
        "    pubs_by_id: Dict[str, dict],\n",
        "    author_ids: List[str],\n",
        "    cat_ids: List[str],\n",
        "    pub_id: Optional[str]\n",
        ") -> str:\n",
        "    title = safe_str(book_row.get(\"title\"))\n",
        "    summary = safe_str(book_row.get(\"book_summary\"))\n",
        "\n",
        "    author_texts = []\n",
        "    for aid in author_ids[:3]:\n",
        "        a = authors_by_id.get(aid)\n",
        "        if not a: continue\n",
        "        author_texts.append(safe_str(a.get(\"author\")))\n",
        "        author_texts.append(safe_str(a.get(\"bio\")))\n",
        "        author_texts.append(safe_str(a.get(\"known_for_tokens\")))\n",
        "\n",
        "    cat_texts = []\n",
        "    for cid in cat_ids[:3]:\n",
        "        c = cats_by_id.get(cid)\n",
        "        if not c: continue\n",
        "        cat_texts.append(safe_str(c.get(\"category_name\")))\n",
        "        cat_texts.append(safe_str(c.get(\"category_description\")))\n",
        "\n",
        "    pub_text = \"\"\n",
        "    if pub_id is not None:\n",
        "        p = pubs_by_id.get(pub_id)\n",
        "        if p:\n",
        "            pub_text = \" \".join([safe_str(p.get(\"publisher_name\")), safe_str(p.get(\"publisher_description\"))])\n",
        "\n",
        "    blob = \" \".join([title, summary, \" \".join(author_texts), \" \".join(cat_texts), pub_text]).strip()\n",
        "    return f\"passage: {blob}\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Text embedding cache\n",
        "# ----------------------------\n",
        "def compute_or_load_text_embeddings(\n",
        "    model_name: str,\n",
        "    cache_path: str,\n",
        "    texts: List[str],\n",
        "    batch_size: int = 64\n",
        ") -> np.ndarray:\n",
        "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
        "    if os.path.exists(cache_path):\n",
        "        return np.load(cache_path)\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embs = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    np.save(cache_path, embs)\n",
        "    return embs\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Torch Dataset\n",
        "# ----------------------------\n",
        "class InteractionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, inter_df: pd.DataFrame, user2idx: Dict[str,int], item2idx: Dict[str,int]):\n",
        "        self.u = inter_df[\"user_id\"].map(user2idx).to_numpy(dtype=np.int64)\n",
        "        self.i = inter_df[\"book_id\"].map(item2idx).to_numpy(dtype=np.int64)\n",
        "        self.w = inter_df[\"weight\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.u)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.u[idx], self.i[idx], self.w[idx]\n",
        "\n",
        "def build_mlp(in_dim: int, hidden_dim: int, out_dim: int, num_layers: int, dropout: float, use_layernorm: bool) -> nn.Module:\n",
        "    if num_layers < 1: raise ValueError(\"num_layers must be >= 1\")\n",
        "    layers: List[nn.Module] = []\n",
        "    if num_layers == 1:\n",
        "        layers.append(nn.Linear(in_dim, out_dim))\n",
        "        return nn.Sequential(*layers)\n",
        "    d = in_dim\n",
        "    for _ in range(num_layers - 1):\n",
        "        layers.append(nn.Linear(d, hidden_dim))\n",
        "        if use_layernorm: layers.append(nn.LayerNorm(hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        if dropout > 0: layers.append(nn.Dropout(dropout))\n",
        "        d = hidden_dim\n",
        "    layers.append(nn.Linear(d, out_dim))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Two-tower model\n",
        "# ----------------------------\n",
        "class TwoTower(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_authors, num_cats, num_pubs, dense_dim, text_in_dim, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.user_emb = nn.Embedding(num_users, cfg.id_emb_dim)\n",
        "        self.item_emb = nn.Embedding(num_items, cfg.id_emb_dim)\n",
        "        self.author_emb = nn.Embedding(max(1, num_authors), cfg.id_emb_dim)\n",
        "        self.cat_emb = nn.Embedding(max(1, num_cats), cfg.id_emb_dim)\n",
        "        self.pub_emb = nn.Embedding(max(1, num_pubs), cfg.id_emb_dim)\n",
        "        self.global_user = nn.Parameter(torch.zeros(cfg.out_dim))\n",
        "        self.text_proj = nn.Linear(text_in_dim, cfg.text_emb_dim) if cfg.use_side else None\n",
        "\n",
        "        item_in = cfg.id_emb_dim\n",
        "        if cfg.use_relations: item_in += cfg.id_emb_dim * 3\n",
        "        if cfg.use_side: item_in += dense_dim + cfg.text_emb_dim\n",
        "\n",
        "        self.item_mlp = build_mlp(item_in, cfg.item_hidden_dim, cfg.out_dim, cfg.item_mlp_layers, cfg.dropout, cfg.use_layernorm)\n",
        "        user_in = cfg.id_emb_dim + cfg.out_dim\n",
        "        self.user_mlp = build_mlp(user_in, cfg.user_hidden_dim, cfg.out_dim, cfg.user_mlp_layers, cfg.dropout, cfg.use_layernorm)\n",
        "\n",
        "    def encode_item(self, item_idx, pub_idx, author_idx_list, cat_idx_list, dense, text_emb):\n",
        "        it = self.item_emb(item_idx)\n",
        "        pb = self.pub_emb(pub_idx)\n",
        "        a_emb = self.author_emb(author_idx_list)\n",
        "        c_emb = self.cat_emb(cat_idx_list)\n",
        "        a_mask = (author_idx_list != 0).float().unsqueeze(-1)\n",
        "        c_mask = (cat_idx_list != 0).float().unsqueeze(-1)\n",
        "        a_pool = (a_emb * a_mask).sum(dim=1) / a_mask.sum(dim=1).clamp_min(1.0)\n",
        "        c_pool = (c_emb * c_mask).sum(dim=1) / c_mask.sum(dim=1).clamp_min(1.0)\n",
        "        parts = [it]\n",
        "        if self.cfg.use_relations: parts.extend([pb, a_pool, c_pool])\n",
        "        if self.cfg.use_side:\n",
        "            parts.extend([dense, self.text_proj(text_emb)])\n",
        "        x = torch.cat(parts, dim=1)\n",
        "        return F.normalize(self.item_mlp(x), dim=1)\n",
        "\n",
        "    def encode_user(self, user_idx, hist_item_vec):\n",
        "        u = self.user_emb(user_idx)\n",
        "        x = torch.cat([u, hist_item_vec], dim=1)\n",
        "        v = self.user_mlp(x)\n",
        "        if not self.cfg.use_interaction:\n",
        "            v = self.global_user.unsqueeze(0).expand(user_idx.size(0), -1)\n",
        "        return F.normalize(v, dim=1)\n",
        "\n",
        "\n",
        "def build_book_side_indices(book_ids, b2a, b2c, b2p, a2idx, c2idx, p2idx, max_authors=5, max_cats=5):\n",
        "    B = len(book_ids)\n",
        "    auth = np.zeros((B, max_authors), dtype=np.int64)\n",
        "    cats = np.zeros((B, max_cats), dtype=np.int64)\n",
        "    pubs = np.zeros((B,), dtype=np.int64)\n",
        "    for j, bid in enumerate(book_ids):\n",
        "        a_list = b2a.get(bid, [])[:max_authors]\n",
        "        c_list = b2c.get(bid, [])[:max_cats]\n",
        "        p = b2p.get(bid, None)\n",
        "        for k, aid in enumerate(a_list): auth[j, k] = a2idx.get(aid, 0)\n",
        "        for k, cid in enumerate(c_list): cats[j, k] = c2idx.get(cid, 0)\n",
        "        pubs[j] = p2idx.get(p, 0) if p is not None else 0\n",
        "    return auth, cats, pubs\n",
        "\n",
        "def build_user_histories(train_df, max_k):\n",
        "    hist = {}\n",
        "    for u, g in train_df.groupby(\"u_idx\"): hist[u] = g[\"i_idx\"].tolist()[-max_k:]\n",
        "    return hist\n",
        "\n",
        "def mrr_at_k(ranked_items, pos_item, k):\n",
        "    topk = ranked_items[:k]\n",
        "    return 1.0 / (topk.index(pos_item) + 1) if pos_item in topk else 0.0\n",
        "\n",
        "def ndcg_at_k(ranked_items, pos_item, k):\n",
        "    topk = ranked_items[:k]\n",
        "    return 1.0 / math.log2(topk.index(pos_item) + 2) if pos_item in topk else 0.0\n",
        "\n",
        "def evaluate_split(model, df, user_hist, all_item_vec, cfg, num_items):\n",
        "    model.eval()\n",
        "    hits, ndcgs, mrrs = {k: [] for k in cfg.eval_ks}, {k: [] for k in cfg.ndcg_ks}, []\n",
        "    with torch.no_grad():\n",
        "        for row in tqdm(df.itertuples(index=False), desc=\"Eval\", leave=False):\n",
        "            u, pos = int(row.u_idx), int(row.i_idx)\n",
        "            h_items = user_hist.get(u, [])\n",
        "            if not h_items:\n",
        "                h_vec = torch.zeros((1, cfg.out_dim), device=cfg.device)\n",
        "            else:\n",
        "                h_vec = all_item_vec[torch.tensor(h_items, device=cfg.device, dtype=torch.long)].mean(dim=0, keepdim=True)\n",
        "            uvec = model.encode_user(torch.tensor([u], device=cfg.device), h_vec)\n",
        "            negs = set()\n",
        "            while len(negs) < cfg.num_neg_eval:\n",
        "                ni = random.randrange(num_items)\n",
        "                if ni != pos: negs.add(ni)\n",
        "            cand = [pos] + list(negs)\n",
        "            cvec = all_item_vec[torch.tensor(cand, device=cfg.device)]\n",
        "            scores = (uvec @ cvec.t()).squeeze(0).cpu().numpy()\n",
        "            order = np.argsort(-scores)\n",
        "            ranked = [cand[i] for i in order]\n",
        "            for k in cfg.eval_ks: hits[k].append(1.0 if pos in ranked[:k] else 0.0)\n",
        "            for k in cfg.ndcg_ks: ndcgs[k].append(ndcg_at_k(ranked, pos, k))\n",
        "            mrrs.append(mrr_at_k(ranked, pos, cfg.mrr_k))\n",
        "    return {**{f\"Hit@{k}\": np.mean(hits[k]) for k in cfg.eval_ks}, **{f\"NDCG@{k}\": np.mean(ndcgs[k]) for k in cfg.ndcg_ks}, f\"MRR@{cfg.mrr_k}\": np.mean(mrrs)}\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main(paths: Paths, cfg: TrainConfig):\n",
        "    set_seed(cfg.seed)\n",
        "    inter = build_interactions(paths)\n",
        "    books, authors, cats, pubs, _ = build_book_tables(paths)\n",
        "    b2a, b2c, b2p = build_multi_maps(paths)\n",
        "\n",
        "    min_user_inter = 3\n",
        "    user_counts = inter[\"user_id\"].value_counts()\n",
        "    inter = inter[inter[\"user_id\"].isin(user_counts[user_counts >= min_user_inter].index)].copy()\n",
        "\n",
        "    keep_books = set(inter[\"book_id\"].unique())\n",
        "    books = books[books[\"book_id\"].isin(keep_books)].copy()\n",
        "    book_ids, user_ids = books[\"book_id\"].unique().tolist(), inter[\"user_id\"].unique().tolist()\n",
        "\n",
        "    user2idx, item2idx = {u:i for i,u in enumerate(user_ids)}, {b:i for i,b in enumerate(book_ids)}\n",
        "    author2idx = {aid:i+1 for i,aid in enumerate(sorted({a for b in book_ids for a in b2a.get(b, [])}))}\n",
        "    cat2idx = {cid:i+1 for i,cid in enumerate(sorted({c for b in book_ids for c in b2c.get(b, [])}))}\n",
        "    pub2idx = {pid:i+1 for i,pid in enumerate(sorted({b2p.get(b) for b in book_ids if b2p.get(b)}))}\n",
        "\n",
        "    numeric_cols = [c for c in [\"price\",\"offer_price\",\"pages\",\"rating_value\",\"rating_count\",\"review_count\",\"wished_customer_count\",\"stock_quantity\"] if c in books.columns]\n",
        "    if not numeric_cols: books[\"dummy_num\"] = 0.0; numeric_cols = [\"dummy_num\"]\n",
        "    books_num = books[[\"book_id\"] + numeric_cols].copy()\n",
        "    books_num, _ = normalize_numeric(books_num, numeric_cols)\n",
        "    dense_mat = books_num.set_index(\"book_id\").loc[book_ids].to_numpy(dtype=np.float32)\n",
        "\n",
        "    if cfg.use_side:\n",
        "        authors_by_id = {row[\"author_id\"]: row for row in authors.to_dict(\"records\") if \"author_id\" in row}\n",
        "        cats_by_id = {row[\"category_id\"]: row for row in cats.to_dict(\"records\")}\n",
        "        pubs_by_id = {row[\"publisher_id\"]: row for row in pubs.to_dict(\"records\")}\n",
        "        book_by_id = books.set_index(\"book_id\").to_dict(\"index\")\n",
        "        book_texts = [make_book_text(pd.Series(book_by_id[bid]), authors_by_id, cats_by_id, pubs_by_id, b2a.get(bid, []), b2c.get(bid, []), b2p.get(bid)) for bid in tqdm(book_ids, desc=\"Assemble text\")]\n",
        "        text_cache = os.path.join(cfg.text_cache_dir, f\"book_text_{cfg.text_model_name.replace('/','_')}_{cfg.ablation_name}.npy\")\n",
        "        text_embs = compute_or_load_text_embeddings(cfg.text_model_name, text_cache, book_texts).astype(np.float32)\n",
        "    else:\n",
        "        text_embs = np.zeros((len(book_ids), cfg.text_emb_dim), dtype=np.float32)\n",
        "\n",
        "    auth_idx, cat_idx, pub_idx = build_book_side_indices(book_ids, b2a, b2c, b2p, author2idx, cat2idx, pub2idx)\n",
        "\n",
        "    train_df, temp_df = train_test_split(inter, test_size=0.30, random_state=cfg.seed)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=cfg.seed)\n",
        "    test_df = test_df[test_df[\"user_id\"].isin(user_counts[user_counts <= 10].index)].copy()\n",
        "\n",
        "    for df in [train_df, val_df, test_df]:\n",
        "        df[\"u_idx\"], df[\"i_idx\"] = df[\"user_id\"].map(user2idx), df[\"book_id\"].map(item2idx)\n",
        "\n",
        "    user_hist = build_user_histories(train_df, cfg.max_history)\n",
        "    dense_t, text_t, auth_t, cat_t, pub_t = [torch.from_numpy(x).to(cfg.device).float() if x.dtype == np.float32 else torch.from_numpy(x).to(cfg.device) for x in [dense_mat, text_embs, auth_idx, cat_idx, pub_idx]]\n",
        "\n",
        "    model = TwoTower(len(user_ids), len(book_ids), len(author2idx)+1, len(cat2idx)+1, len(pub2idx)+1, dense_t.shape[1], text_t.shape[1], cfg).to(cfg.device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    train_loader = torch.utils.data.DataLoader(InteractionDataset(train_df, user2idx, item2idx), batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    best_val, best_state, patience_ctr = -1.0, None, 0\n",
        "    for epoch in range(1, cfg.epochs+1):\n",
        "        model.train(); total_loss = 0.0\n",
        "        for u_idx, pos_i, w in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "            u_idx, pos_i, w = u_idx.to(cfg.device), pos_i.to(cfg.device), w.to(cfg.device)\n",
        "            pos_vec = model.encode_item(pos_i, pub_t[pos_i], auth_t[pos_i], cat_t[pos_i], dense_t[pos_i], text_t[pos_i])\n",
        "            h_vecs = []\n",
        "            for u in u_idx.cpu().numpy():\n",
        "                h_items = user_hist.get(u, [])\n",
        "                if not h_items: h_vecs.append(torch.zeros((1, cfg.out_dim), device=cfg.device))\n",
        "                else:\n",
        "                    hi = torch.tensor(h_items, device=cfg.device)\n",
        "                    h_vecs.append(model.encode_item(hi, pub_t[hi], auth_t[hi], cat_t[hi], dense_t[hi], text_t[hi]).mean(dim=0, keepdim=True))\n",
        "            user_vec = model.encode_user(u_idx, torch.cat(h_vecs, dim=0))\n",
        "            loss = (F.cross_entropy(user_vec @ pos_vec.t(), torch.arange(cfg.batch_size, device=cfg.device), reduction=\"none\") * w).mean()\n",
        "            opt.zero_grad(); loss.backward(); opt.step(); total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            all_i = torch.arange(len(book_ids), device=cfg.device)\n",
        "            all_item_vec = model.encode_item(all_i, pub_t[all_i], auth_t[all_i], cat_t[all_i], dense_t[all_i], text_t[all_i])\n",
        "        val_m = evaluate_split(model, val_df, user_hist, all_item_vec, cfg, len(book_ids))\n",
        "        print(f\"Epoch {epoch} loss: {total_loss/len(train_loader):.4f} | Val NDCG@10: {val_m['NDCG@10']:.4f}\")\n",
        "        if val_m['NDCG@10'] > best_val: best_val, best_state, patience_ctr = val_m['NDCG@10'], {k: v.cpu().clone() for k, v in model.state_dict().items()}, 0\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "            if patience_ctr >= cfg.patience: break\n",
        "\n",
        "    if best_state: model.load_state_dict(best_state)\n",
        "    with torch.no_grad(): all_item_vec = model.encode_item(all_i, pub_t[all_i], auth_t[all_i], cat_t[all_i], dense_t[all_i], text_t[all_i])\n",
        "    test_m = evaluate_split(model, test_df, user_hist, all_item_vec, cfg, len(book_ids))\n",
        "    print(\"\\n=== TEST RESULTS ===\"); [print(f\"{k:8}: {v:.4f}\") for k, v in test_m.items()]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = DATA_FOLDER\n",
        "    paths = Paths(author_json=os.path.join(base_dir, \"author.json\"), book_json=os.path.join(base_dir, \"book.json\"), category_json=os.path.join(base_dir, \"category.json\"), publisher_json=os.path.join(base_dir, \"publisher.json\"), review_json=os.path.join(base_dir, \"review.json\"), book_to_author_json=os.path.join(base_dir, \"book_to_author.json\"), book_to_category_json=os.path.join(base_dir, \"book_to_category.json\"), book_to_publisher_json=os.path.join(base_dir, \"book_to_publisher.json\"), book_to_review_json=os.path.join(base_dir, \"book_to_review.json\"), user_to_review_json=os.path.join(base_dir, \"user_to_review.json\"))\n",
        "    main(paths, TrainConfig())"
      ]
    }
  ]
}