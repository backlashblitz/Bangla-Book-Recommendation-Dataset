{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/backlashblitz/Bangla-Book-Recommendation-Dataset/blob/main/colabnotebooks/LightGCN_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGCN\n",
        "**Bangla Book Recommendation Dataset**\n",
        "\n",
        "â–¶ï¸ **Just click `Runtime â†’ Run all` to get started!**"
      ],
      "metadata": {
        "id": "TYuT1kRnfNDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "os.system(\"pip install -q huggingface_hub tqdm\")\n",
        "\n",
        "REPO_ID = \"DevnilMaster1/Bangla-Book-Recommendation-Dataset\"\n",
        "DATA_FOLDER = \"RokomariBG_Dataset\"\n",
        "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
        "\n",
        "FILES_NEEDED = ['user_to_review.json', 'book_to_review.json']\n",
        "\n",
        "for filename in FILES_NEEDED:\n",
        "    dest = os.path.join(DATA_FOLDER, filename)\n",
        "    if not os.path.exists(dest):\n",
        "        try:\n",
        "            downloaded_path = hf_hub_download(\n",
        "                repo_id=REPO_ID,\n",
        "                filename=f\"{DATA_FOLDER}/{filename}\",\n",
        "                repo_type=\"dataset\"\n",
        "            )\n",
        "            shutil.copy(downloaded_path, dest)\n",
        "            print(f\"âœ… Saved: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error downloading {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"âœ… Already present: {filename}\")\n",
        "\n",
        "user_to_review_path = os.path.join(DATA_FOLDER, \"user_to_review.json\")\n",
        "book_to_review_path = os.path.join(DATA_FOLDER, \"book_to_review.json\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Environment Ready!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "-mm6dPhtd5WP",
        "outputId": "9bf6b65f-6cd9-437d-8eea-b112b423eb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Already present: user_to_review.json\n",
            "âœ… Already present: book_to_review.json\n",
            "\n",
            "ðŸŽ‰ Environment Ready!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "class RankingEvaluator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def hit_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
        "        top_k = predictions[:k]\n",
        "        return 1.0 if any(item in ground_truth for item in top_k) else 0.0\n",
        "\n",
        "    def mrr_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
        "        top_k = predictions[:k]\n",
        "        for rank, item in enumerate(top_k, start=1):\n",
        "            if item in ground_truth:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "    def dcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
        "        dcg = 0.0\n",
        "        top_k = predictions[:k]\n",
        "        for rank, item in enumerate(top_k, start=1):\n",
        "            if item in ground_truth:\n",
        "                dcg += 1.0 / np.log2(rank + 1)\n",
        "        return dcg\n",
        "\n",
        "    def idcg_at_k(self, ground_truth: Set[str], k: int) -> float:\n",
        "        ideal_k = min(len(ground_truth), k)\n",
        "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, ideal_k + 1))\n",
        "        return idcg\n",
        "\n",
        "    def ndcg_at_k(self, predictions: List[str], ground_truth: Set[str], k: int) -> float:\n",
        "        dcg = self.dcg_at_k(predictions, ground_truth, k)\n",
        "        idcg = self.idcg_at_k(ground_truth, k)\n",
        "        if idcg == 0.0:\n",
        "            return 0.0\n",
        "        return dcg / idcg\n",
        "\n",
        "    def evaluate(self, predictions: Dict[str, List[str]], ground_truth: Dict[str, Set[str]]) -> Dict[str, float]:\n",
        "        metrics = {\n",
        "            'Hit@5': [],\n",
        "            'Hit@10': [],\n",
        "            'Hit@50': [],\n",
        "            'MRR@10': [],\n",
        "            'NDCG@10': [],\n",
        "            'NDCG@50': []\n",
        "        }\n",
        "        common_users = set(predictions.keys()) & set(ground_truth.keys())\n",
        "        for user_id in common_users:\n",
        "            preds = predictions[user_id]\n",
        "            gt = ground_truth[user_id]\n",
        "            if len(gt) == 0:\n",
        "                continue\n",
        "            metrics['Hit@5'].append(self.hit_at_k(preds, gt, 5))\n",
        "            metrics['Hit@10'].append(self.hit_at_k(preds, gt, 10))\n",
        "            metrics['Hit@50'].append(self.hit_at_k(preds, gt, 50))\n",
        "            metrics['MRR@10'].append(self.mrr_at_k(preds, gt, 10))\n",
        "            metrics['NDCG@10'].append(self.ndcg_at_k(preds, gt, 10))\n",
        "            metrics['NDCG@50'].append(self.ndcg_at_k(preds, gt, 50))\n",
        "        results = {}\n",
        "        for metric_name, values in metrics.items():\n",
        "            results[metric_name] = np.mean(values) if len(values) > 0 else 0.0\n",
        "        return results"
      ],
      "metadata": {
        "trusted": true,
        "id": "yLcqeszvd5WT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class BaseRecommender:\n",
        "    def fit(self):\n",
        "        raise NotImplementedError\n",
        "    def recommend(self, user_id: str, k: int = 10) -> List[str]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class LightGCNRecommender(BaseRecommender):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_user_book: Dict[str, List[str]],\n",
        "        n_factors: int = 64,\n",
        "        n_layers: int = 3,\n",
        "        n_epochs: int = 10,\n",
        "        lr: float = 0.01,\n",
        "        reg: float = 1e-4,\n",
        "        n_neg: int = 1,\n",
        "        seed: int = 42,\n",
        "        device: str = None,\n",
        "        batch_size: int = 4096,\n",
        "        neg_per_user: int = 1,\n",
        "    ):\n",
        "        self.train_user_book = {u: list(set(b)) for u, b in train_user_book.items()}\n",
        "        self.n_factors = n_factors\n",
        "        self.n_layers = n_layers\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr\n",
        "        self.reg = reg\n",
        "        self.n_neg = n_neg\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.user_ids = []\n",
        "        self.book_ids = []\n",
        "        self.user_id_map = {}\n",
        "        self.book_id_map = {}\n",
        "        self.n_users = 0\n",
        "        self.n_items = 0\n",
        "        self.A_hat = None\n",
        "        self.user_emb = None\n",
        "        self.item_emb = None\n",
        "        self.final_user_emb = None\n",
        "        self.final_item_emb = None\n",
        "        self.batch_size = batch_size\n",
        "        self.neg_per_user = neg_per_user\n",
        "\n",
        "    def _build_sparse_graph(self):\n",
        "        print(\"Building sparse LightGCN graph...\")\n",
        "        self.user_ids = sorted(self.train_user_book.keys())\n",
        "        self.book_ids = sorted({b for books in self.train_user_book.values() for b in books})\n",
        "        self.user_id_map = {u: i for i, u in enumerate(self.user_ids)}\n",
        "        self.book_id_map = {b: i for i, b in enumerate(self.book_ids)}\n",
        "        U, I = len(self.user_ids), len(self.book_ids)\n",
        "        N = U + I\n",
        "        rows, cols = [], []\n",
        "        for u, books in self.train_user_book.items():\n",
        "            uidx = self.user_id_map[u]\n",
        "            for b in books:\n",
        "                iidx = self.book_id_map[b]\n",
        "                rows.append(uidx); cols.append(U + iidx)\n",
        "                rows.append(U + iidx); cols.append(uidx)\n",
        "        idx = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n",
        "        val = torch.ones(idx.shape[1], dtype=torch.float32, device=self.device)\n",
        "        A = torch.sparse_coo_tensor(idx, val, size=(N, N), device=self.device).coalesce()\n",
        "        deg = torch.sparse.sum(A, dim=1).to_dense()\n",
        "        deg_inv_sqrt = torch.pow(deg, -0.5)\n",
        "        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
        "        r, c = A.indices()\n",
        "        v_norm = A.values() * deg_inv_sqrt[r] * deg_inv_sqrt[c]\n",
        "        self.A_hat = torch.sparse_coo_tensor(torch.stack([r, c], dim=0), v_norm, size=(N, N), device=self.device).coalesce()\n",
        "        self.n_users, self.n_items = U, I\n",
        "\n",
        "    def _propagate(self):\n",
        "        all_emb0 = torch.cat([self.user_emb.weight, self.item_emb.weight], dim=0)\n",
        "        embs = [all_emb0]\n",
        "        x = all_emb0\n",
        "        for _ in range(self.n_layers):\n",
        "            x = torch.sparse.mm(self.A_hat, x)\n",
        "            embs.append(x)\n",
        "        all_emb = torch.mean(torch.stack(embs, dim=0), dim=0)\n",
        "        return all_emb[: self.n_users], all_emb[self.n_users :]\n",
        "\n",
        "    def fit(self):\n",
        "        print(f\"Training LightGCN on {self.device}...\")\n",
        "        self._build_sparse_graph()\n",
        "        self.user_emb = nn.Embedding(self.n_users, self.n_factors).to(self.device)\n",
        "        self.item_emb = nn.Embedding(self.n_items, self.n_factors).to(self.device)\n",
        "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
        "        optimizer = optim.Adam(list(self.user_emb.parameters()) + list(self.item_emb.parameters()), lr=self.lr)\n",
        "        user_pos_items = {self.user_id_map[u]: set(self.book_id_map[b] for b in books if b in self.book_id_map) for u, books in self.train_user_book.items() if u in self.user_id_map}\n",
        "        users_list = np.array(list(user_pos_items.keys()), dtype=np.int64)\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "            self.rng.shuffle(users_list)\n",
        "            total_loss, n_steps = 0.0, 0\n",
        "            pbar = tqdm(range(0, len(users_list), self.batch_size), desc=f\"LightGCN Epoch {epoch}\")\n",
        "            for start in pbar:\n",
        "                batch_users = users_list[start : start + self.batch_size]\n",
        "                if len(batch_users) == 0: continue\n",
        "                pos_items = [int(self.rng.choice(list(user_pos_items[int(u)]))) for u in batch_users]\n",
        "                neg_items = np.empty((len(batch_users), self.neg_per_user), dtype=np.int64)\n",
        "                for i, u in enumerate(batch_users):\n",
        "                    pos_set = user_pos_items[int(u)]\n",
        "                    for j in range(self.neg_per_user):\n",
        "                        neg = int(self.rng.integers(0, self.n_items))\n",
        "                        while neg in pos_set: neg = int(self.rng.integers(0, self.n_items))\n",
        "                        neg_items[i, j] = neg\n",
        "                batch_users_t = torch.tensor(batch_users, device=self.device, dtype=torch.long)\n",
        "                pos_items_t = torch.tensor(pos_items, device=self.device, dtype=torch.long)\n",
        "                neg_items_t = torch.tensor(neg_items, device=self.device, dtype=torch.long)\n",
        "                u_f, i_f = self._propagate()\n",
        "                uvec, pvec, nvec = u_f[batch_users_t], i_f[pos_items_t], i_f[neg_items_t]\n",
        "                pos_scores = torch.sum(uvec * pvec, dim=1, keepdim=True)\n",
        "                neg_scores = torch.sum(uvec.unsqueeze(1) * nvec, dim=2)\n",
        "                bpr = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-12))\n",
        "                reg_val = self.reg * (self.user_emb(batch_users_t).norm(2).pow(2) + self.item_emb(pos_items_t).norm(2).pow(2) + self.item_emb(neg_items_t.view(-1)).norm(2).pow(2)) / len(batch_users)\n",
        "                loss = bpr + reg_val\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss.backward(); optimizer.step()\n",
        "                total_loss += float(loss.item()); n_steps += 1\n",
        "                pbar.set_postfix(loss=f\"{total_loss / n_steps:.5f}\")\n",
        "            print(f\"Epoch {epoch}: avg_loss={total_loss / max(n_steps,1):.6f}\")\n",
        "        with torch.no_grad(): self.final_user_emb, self.final_item_emb = self._propagate()\n",
        "\n",
        "    def recommend(self, user_id: str, k: int = 10) -> List[str]:\n",
        "        if user_id not in self.user_id_map: return []\n",
        "        uidx = self.user_id_map[user_id]\n",
        "        with torch.no_grad():\n",
        "            scores = torch.matmul(self.final_item_emb, self.final_user_emb[uidx]).cpu().numpy()\n",
        "        seen = [self.book_id_map[b] for b in self.train_user_book[user_id] if b in self.book_id_map]\n",
        "        if seen: scores[seen] = -1e9\n",
        "        topk = np.argpartition(scores, -k)[-k:]\n",
        "        return [self.book_ids[i] for i in topk[np.argsort(scores[topk])[::-1]]]\n",
        "\n",
        "def load_json(path: str):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
        "\n",
        "def build_user_book_interactions(u_path, b_path):\n",
        "    u_to_r, b_to_r = load_json(u_path), load_json(b_path)\n",
        "    r_to_u = {str(x[\"review_id\"]): str(x[\"user_id\"]) for x in u_to_r}\n",
        "    r_to_b = {str(x[\"review_id\"]): str(x[\"book_id\"]) for x in b_to_r}\n",
        "    ub = defaultdict(list)\n",
        "    for rid, uid in r_to_u.items():\n",
        "        if rid in r_to_b: ub[uid].append(r_to_b[rid])\n",
        "    return ub\n",
        "\n",
        "def split_user_interactions(ub, seed=42):\n",
        "    random.seed(seed)\n",
        "    tr, va, te = {}, {}, {}\n",
        "    for u, b in ub.items():\n",
        "        b = list(set(b)); random.shuffle(b); n = len(b)\n",
        "        if n < 3: tr[u], va[u], te[u] = b, [], []; continue\n",
        "        nt, nv = int(0.7*n), int(0.15*n)\n",
        "        tr[u], va[u], te[u] = b[:nt], b[nt:nt+nv], b[nt+nv:]\n",
        "    return tr, va, te\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ub = build_user_book_interactions(user_to_review_path, book_to_review_path)\n",
        "    tr, va, te = split_user_interactions(ub)\n",
        "    gt = {u: set(b) for u, b in te.items() if b}\n",
        "    model = LightGCNRecommender(tr, n_factors=64, n_layers=2, n_epochs=10)\n",
        "    model.fit()\n",
        "    preds = {u: model.recommend(u, 10) for u in tqdm(gt.keys(), desc=\"Predicting\")}\n",
        "    metrics = RankingEvaluator().evaluate(preds, gt)\n",
        "    print(\"\\n===== LightGCN Results =====\")\n",
        "    for m, v in metrics.items(): print(f\"{m}: {v:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "BXvk9AGbd5WU",
        "outputId": "36e553c3-ab66-40c3-f2f2-985e5c4af11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LightGCN on cuda...\n",
            "Building sparse LightGCN graph...\n",
            "Epoch 10: avg_loss=0.018455\n",
            "\n",
            "===== LightGCN Results =====\n",
            "Hit@5: 0.1831\n",
            "Hit@10: 0.2218\n",
            "Hit@50: 0.2218\n",
            "MRR@10: 0.1355\n",
            "NDCG@10: 0.1237\n",
            "NDCG@50: 0.1236\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}