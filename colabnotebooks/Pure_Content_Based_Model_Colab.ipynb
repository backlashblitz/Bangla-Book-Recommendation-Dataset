{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/backlashblitz/Bangla-Book-Recommendation-Dataset/blob/main/colabnotebooks/Pure_Content_Based_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqHk3dFCZANb"
      },
      "source": [
        "# Pure_Content_Based_Model\n",
        "**Bangla Book Recommendation Dataset**\n",
        "\n",
        "‚ñ∂Ô∏è **Just click `Runtime ‚Üí Run all` to get started!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKOPqWv5uqx7"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install and Import Libraries\n",
        "!pip install -q datasets huggingface-hub\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import hstack, csr_matrix, vstack\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# Memory monitoring function\n",
        "def print_memory_usage():\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"Memory usage: {memory.percent}% ({memory.used / 1e9:.2f} GB / {memory.total / 1e9:.2f} GB)\")\n",
        "\n",
        "# Force garbage collection\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95hhw_YrQyl9"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_ID = \"DevnilMaster1/Bangla-Book-Recommendation-Dataset\"\n",
        "DATA_FOLDER = \"RokomariBG_Dataset\"\n",
        "\n",
        "# Download the entire repository\n",
        "download_root_dir = huggingface_hub.snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "# The actual data we want is inside the DATA_FOLDER within the downloaded root.\n",
        "local_dir = os.path.join(download_root_dir, DATA_FOLDER)\n",
        "\n",
        "print(f\"Dataset root downloaded to: {download_root_dir}\")\n",
        "print(f\"Attempting to access data from: {local_dir}\")\n",
        "\n",
        "# Verify that the expected data folder exists\n",
        "if not os.path.isdir(local_dir):\n",
        "    # This might happen if DATA_FOLDER itself is the root of the dataset in the repo\n",
        "    # or if the folder structure is different. Let's check for JSON files directly in download_root_dir\n",
        "    if os.path.isdir(download_root_dir) and any(f.endswith(\".json\") for f in os.listdir(download_root_dir)):\n",
        "        local_dir = download_root_dir\n",
        "        print(f\"Warning: '{DATA_FOLDER}' not found as subfolder. Found JSON files directly in download root. Setting local_dir to: {local_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Data folder '{DATA_FOLDER}' not found at '{local_dir}' and no JSON files found directly in '{download_root_dir}'. Please check repository structure or network.\")\n",
        "\n",
        "print(\"Files available in local_dir:\")\n",
        "for f in os.listdir(local_dir):\n",
        "    print(f\"  {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnym_xxsSDv8"
      },
      "outputs": [],
      "source": [
        "# Define the base path (pointing to the HuggingFace downloaded dataset)\n",
        "from pathlib import Path\n",
        "\n",
        "# local_dir is already set in the previous cell to point to the correct data folder\n",
        "base_path = Path(local_dir)\n",
        "\n",
        "# Load all JSON files\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "# Main metadata\n",
        "with open(base_path / 'book.json', 'r', encoding='utf-8') as f:\n",
        "    books_data = json.load(f)\n",
        "\n",
        "with open(base_path / 'author.json', 'r', encoding='utf-8') as f:\n",
        "    authors_data = json.load(f)\n",
        "\n",
        "with open(base_path / 'category.json', 'r', encoding='utf-8') as f:\n",
        "    categories_data = json.load(f)\n",
        "\n",
        "with open(base_path / 'publisher.json', 'r', encoding='utf-8') as f:\n",
        "    publishers_data = json.load(f)\n",
        "\n",
        "with open(base_path / 'review.json', 'r', encoding='utf-8') as f:\n",
        "    reviews_data = json.load(f)\n",
        "\n",
        "# Relationship tables\n",
        "with open(base_path / 'book_to_category.json', 'r', encoding='utf-8') as f:\n",
        "    book_category = json.load(f)\n",
        "\n",
        "with open(base_path / 'book_to_author.json', 'r', encoding='utf-8') as f:\n",
        "    book_author = json.load(f)\n",
        "\n",
        "with open(base_path / 'book_to_publisher.json', 'r', encoding='utf-8') as f:\n",
        "    book_publisher = json.load(f)\n",
        "\n",
        "with open(base_path / 'book_to_review.json', 'r', encoding='utf-8') as f:\n",
        "    book_review = json.load(f)\n",
        "\n",
        "with open(base_path / 'user_to_review.json', 'r', encoding='utf-8') as f:\n",
        "    user_review = json.load(f)\n",
        "\n",
        "print(\"All datasets loaded successfully!\")\n",
        "print(f\"Books: {len(books_data)}\")\n",
        "print(f\"Authors: {len(authors_data)}\")\n",
        "print(f\"Categories: {len(categories_data)}\")\n",
        "print(f\"Publishers: {len(publishers_data)}\")\n",
        "print(f\"Reviews: {len(reviews_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6ROf0qmQ0GZ"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrames\n",
        "df_books = pd.DataFrame(books_data)\n",
        "df_authors = pd.DataFrame(authors_data)\n",
        "df_categories = pd.DataFrame(categories_data)\n",
        "df_publishers = pd.DataFrame(publishers_data)\n",
        "df_reviews = pd.DataFrame(reviews_data)\n",
        "df_book_category = pd.DataFrame(book_category)\n",
        "df_book_author = pd.DataFrame(book_author)\n",
        "df_book_publisher = pd.DataFrame(book_publisher)\n",
        "df_book_review = pd.DataFrame(book_review)\n",
        "df_user_review = pd.DataFrame(user_review)\n",
        "\n",
        "# Convert all IDs to strings for consistency\n",
        "df_books['book_id'] = df_books['book_id'].astype(str)\n",
        "df_authors['author_id'] = df_authors['author_id'].astype(str)\n",
        "df_categories['category_id'] = df_categories['category_id'].astype(str)\n",
        "df_publishers['publisher_id'] = df_publishers['publisher_id'].astype(str)\n",
        "df_reviews['review_id'] = df_reviews['review_id'].astype(str)\n",
        "\n",
        "df_book_category['book_id'] = df_book_category['book_id'].astype(str)\n",
        "df_book_category['category_id'] = df_book_category['category_id'].astype(str)\n",
        "\n",
        "df_book_author['book_id'] = df_book_author['book_id'].astype(str)\n",
        "df_book_author['author_id'] = df_book_author['author_id'].astype(str)\n",
        "\n",
        "df_book_publisher['book_id'] = df_book_publisher['book_id'].astype(str)\n",
        "df_book_publisher['publisher_id'] = df_book_publisher['publisher_id'].astype(str)\n",
        "\n",
        "df_book_review['book_id'] = df_book_review['book_id'].astype(str)\n",
        "df_book_review['review_id'] = df_book_review['review_id'].astype(str)\n",
        "\n",
        "df_user_review['user_id'] = df_user_review['user_id'].astype(str)\n",
        "df_user_review['review_id'] = df_user_review['review_id'].astype(str)\n",
        "\n",
        "# Remove duplicate books - keep first occurrence\n",
        "print(f\"Books before deduplication: {len(df_books)}\")\n",
        "df_books = df_books.drop_duplicates(subset=['book_id'], keep='first')\n",
        "print(f\"Books after deduplication: {len(df_books)}\")\n",
        "\n",
        "# Basic data info\n",
        "print(\"\\nDataset Overview:\")\n",
        "print(f\"Unique Books: {df_books['book_id'].nunique()}\")\n",
        "print(f\"Unique Authors: {df_authors['author_id'].nunique()}\")\n",
        "print(f\"Unique Categories: {df_categories['category_id'].nunique()}\")\n",
        "print(f\"Unique Publishers: {df_publishers['publisher_id'].nunique()}\")\n",
        "print(f\"Total Reviews: {len(df_reviews)}\")\n",
        "print(f\"Unique Users: {df_user_review['user_id'].nunique()}\")\n",
        "\n",
        "# Check for missing values in key columns\n",
        "print(\"\\nMissing values in books:\")\n",
        "print(df_books[['book_id', 'book_title']].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpTqzweEQ2jm"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Create Mappings\n",
        "# Create mapping dictionaries\n",
        "author_map = dict(zip(df_authors['author_id'], df_authors['author']))\n",
        "category_map = dict(zip(df_categories['category_id'], df_categories['category_name']))\n",
        "publisher_map = dict(zip(df_publishers['publisher_id'], df_publishers['publisher_name']))\n",
        "\n",
        "# Aggregate categories per book\n",
        "book_categories = df_book_category.groupby('book_id')['category_id'].apply(list).to_dict()\n",
        "\n",
        "# Aggregate authors per book\n",
        "book_authors = df_book_author.groupby('book_id')['author_id'].apply(list).to_dict()\n",
        "\n",
        "# Aggregate publishers per book\n",
        "book_publishers = df_book_publisher.groupby('book_id')['publisher_id'].apply(list).to_dict()\n",
        "\n",
        "# Aggregate reviews per book\n",
        "book_reviews_map = df_book_review.groupby('book_id')['review_id'].apply(list).to_dict()\n",
        "\n",
        "# Create review text mapping - the field is 'review_detail'\n",
        "review_text_map = dict(zip(df_reviews['review_id'], df_reviews['review_detail']))\n",
        "\n",
        "print(\"Mappings created successfully!\")\n",
        "print(f\"Books with categories: {len(book_categories)}\")\n",
        "print(f\"Books with authors: {len(book_authors)}\")\n",
        "print(f\"Books with publishers: {len(book_publishers)}\")\n",
        "print(f\"Books with reviews: {len(book_reviews_map)}\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1PXfc80Q3Md"
      },
      "outputs": [],
      "source": [
        "# Get user-book interactions from reviews\n",
        "user_books = df_user_review.merge(df_book_review, on='review_id')\n",
        "user_books = user_books[['user_id', 'book_id']].drop_duplicates()\n",
        "\n",
        "print(f\"Initial user-book interactions: {len(user_books)}\")\n",
        "print(f\"Initial unique users: {user_books['user_id'].nunique()}\")\n",
        "print(f\"Initial unique books: {user_books['book_id'].nunique()}\")\n",
        "\n",
        "# Convert book_id to string to ensure matching\n",
        "user_books['book_id'] = user_books['book_id'].astype(str)\n",
        "\n",
        "# Filter books that exist in our cleaned books dataset\n",
        "valid_books = set(df_books['book_id'].unique())\n",
        "print(f\"\\nTotal valid books in df_books: {len(valid_books)}\")\n",
        "\n",
        "user_books = user_books[user_books['book_id'].isin(valid_books)]\n",
        "\n",
        "print(f\"\\nAfter filtering for valid books:\")\n",
        "print(f\"Total user-book interactions: {len(user_books)}\")\n",
        "print(f\"Unique users: {user_books['user_id'].nunique()}\")\n",
        "print(f\"Unique books: {user_books['book_id'].nunique()}\")\n",
        "\n",
        "# NO USER FILTERING - Use all users\n",
        "valid_users = user_books['user_id'].unique()\n",
        "\n",
        "print(f\"\\nNo user filtering applied:\")\n",
        "print(f\"Total users: {len(valid_users)}\")\n",
        "print(f\"Total interactions: {len(user_books)}\")\n",
        "\n",
        "# Split each user's interactions into train/val/test (70/15/15)\n",
        "# This ensures every user appears in all three sets\n",
        "np.random.seed(42)\n",
        "\n",
        "train_list = []\n",
        "val_list = []\n",
        "test_list = []\n",
        "\n",
        "# PRE-GROUP by user_id ‚Äî avoids slow per-user filtering inside the loop\n",
        "grouped = user_books.groupby('user_id')\n",
        "\n",
        "for user_id, user_interactions in grouped:\n",
        "    # Shuffle user's interactions\n",
        "    user_interactions = user_interactions.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    n_interactions = len(user_interactions)\n",
        "\n",
        "    # For users with only 1 interaction, put in training\n",
        "    if n_interactions == 1:\n",
        "        train_list.append(user_interactions)\n",
        "        continue\n",
        "\n",
        "    # For users with 2 interactions, put 1 in train, 1 in test\n",
        "    if n_interactions == 2:\n",
        "        train_list.append(user_interactions[:1])\n",
        "        test_list.append(user_interactions[1:])\n",
        "        continue\n",
        "\n",
        "    # For users with 3+ interactions, do proper split\n",
        "    train_size = int(0.70 * n_interactions)\n",
        "    val_size = int(0.15 * n_interactions)\n",
        "\n",
        "    # Ensure at least 1 item in train\n",
        "    if train_size == 0:\n",
        "        train_size = 1\n",
        "\n",
        "    # Ensure at least 1 item in test if possible\n",
        "    remaining = n_interactions - train_size\n",
        "    if remaining > 0 and val_size >= remaining:\n",
        "        val_size = remaining - 1\n",
        "\n",
        "    train_list.append(user_interactions[:train_size])\n",
        "\n",
        "    if val_size > 0:\n",
        "        val_list.append(user_interactions[train_size:train_size + val_size])\n",
        "\n",
        "    if train_size + val_size < n_interactions:\n",
        "        test_list.append(user_interactions[train_size + val_size:])\n",
        "\n",
        "train_df = pd.concat(train_list, ignore_index=True) if train_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
        "val_df = pd.concat(val_list, ignore_index=True) if val_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
        "test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame(columns=['user_id', 'book_id'])\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DATA SPLIT SUMMARY (70/15/15) - NO USER FILTERING\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Training set:\")\n",
        "print(f\"  Users: {train_df['user_id'].nunique()}\")\n",
        "print(f\"  Interactions: {len(train_df)} ({len(train_df)/len(user_books)*100:.1f}%)\")\n",
        "print(f\"\\nValidation set:\")\n",
        "print(f\"  Users: {val_df['user_id'].nunique()}\")\n",
        "print(f\"  Interactions: {len(val_df)} ({len(val_df)/len(user_books)*100:.1f}%)\")\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Users: {test_df['user_id'].nunique()}\")\n",
        "print(f\"  Interactions: {len(test_df)} ({len(test_df)/len(user_books)*100:.1f}%)\")\n",
        "print(f\"\\nTotal: {len(valid_users)} users, {len(user_books)} interactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvw9Ch6nQ4jo"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Build Feature Matrices (with memory optimization)\n",
        "# Get all unique books that appear in training data\n",
        "train_books = train_df['book_id'].unique()\n",
        "all_books = df_books[df_books['book_id'].isin(valid_books)]['book_id'].unique()\n",
        "\n",
        "print(f\"Building features for {len(all_books)} books...\")\n",
        "print_memory_usage()\n",
        "\n",
        "# 1. Author Features (Multi-hot encoding)\n",
        "print(\"Processing author features...\")\n",
        "book_author_lists = []\n",
        "for book_id in all_books:\n",
        "    authors = book_authors.get(book_id, [])\n",
        "    book_author_lists.append(authors)\n",
        "\n",
        "mlb_authors = MultiLabelBinarizer(sparse_output=True)\n",
        "author_features = mlb_authors.fit_transform(book_author_lists)\n",
        "\n",
        "print(f\"Author features shape: {author_features.shape}\")\n",
        "print_memory_usage()\n",
        "clean_memory()\n",
        "\n",
        "# 2. Category Features (Multi-hot encoding)\n",
        "print(\"Processing category features...\")\n",
        "book_category_lists = []\n",
        "for book_id in all_books:\n",
        "    categories = book_categories.get(book_id, [])\n",
        "    book_category_lists.append(categories)\n",
        "\n",
        "mlb_categories = MultiLabelBinarizer(sparse_output=True)\n",
        "category_features = mlb_categories.fit_transform(book_category_lists)\n",
        "\n",
        "print(f\"Category features shape: {category_features.shape}\")\n",
        "print_memory_usage()\n",
        "clean_memory()\n",
        "\n",
        "# 3. Publisher Features (Multi-hot encoding)\n",
        "print(\"Processing publisher features...\")\n",
        "book_publisher_lists = []\n",
        "for book_id in all_books:\n",
        "    publishers = book_publishers.get(book_id, [])\n",
        "    book_publisher_lists.append(publishers)\n",
        "\n",
        "mlb_publishers = MultiLabelBinarizer(sparse_output=True)\n",
        "publisher_features = mlb_publishers.fit_transform(book_publisher_lists)\n",
        "\n",
        "print(f\"Publisher features shape: {publisher_features.shape}\")\n",
        "print_memory_usage()\n",
        "clean_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du5CDWTLQ6T3"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Build TF-IDF Features (with memory optimization)\n",
        "print(\"Building TF-IDF features from reviews...\")\n",
        "\n",
        "# Process in batches to save memory\n",
        "batch_size = 10000\n",
        "book_review_texts = []\n",
        "\n",
        "print(\"Creating review text corpus...\")\n",
        "for i, book_id in enumerate(all_books):\n",
        "    if i % 10000 == 0 and i > 0:\n",
        "        print(f\"Processed {i}/{len(all_books)} books for review texts\")\n",
        "\n",
        "    review_ids = book_reviews_map.get(book_id, [])\n",
        "    reviews = [review_text_map.get(rid, '') for rid in review_ids]\n",
        "    combined_text = ' '.join([r for r in reviews if r])\n",
        "    book_review_texts.append(combined_text)\n",
        "\n",
        "print(\"Review text corpus created\")\n",
        "print_memory_usage()\n",
        "\n",
        "# TF-IDF with parameters to handle large vocabulary\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,  # Limit features to avoid memory issues\n",
        "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
        "    max_df=0.8,         # Ignore terms that appear in more than 80% of documents\n",
        "    ngram_range=(1, 2), # Use unigrams and bigrams\n",
        "    strip_accents='unicode',\n",
        "    lowercase=True,\n",
        "    stop_words=None     # Keep all words since we're working with Bengali\n",
        ")\n",
        "\n",
        "print(\"Fitting TF-IDF (this may take a while)...\")\n",
        "review_features = tfidf.fit_transform(book_review_texts)\n",
        "\n",
        "print(f\"Review features shape: {review_features.shape}\")\n",
        "print(f\"Total vocabulary size: {len(tfidf.vocabulary_)}\")\n",
        "print_memory_usage()\n",
        "\n",
        "# Clear review texts to free memory\n",
        "del book_review_texts\n",
        "clean_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyKt36rhQ7sL"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Combine All Features\n",
        "print(\"\\nCombining all features...\")\n",
        "\n",
        "# Stack all feature matrices horizontally\n",
        "item_features = hstack([\n",
        "    author_features,\n",
        "    category_features,\n",
        "    publisher_features,\n",
        "    review_features\n",
        "]).tocsr()\n",
        "\n",
        "print(f\"Combined item features shape: {item_features.shape}\")\n",
        "print(f\"Feature dimensions: {item_features.shape[1]}\")\n",
        "print_memory_usage()\n",
        "\n",
        "# Create book_id to index mapping\n",
        "book_to_idx = {book_id: idx for idx, book_id in enumerate(all_books)}\n",
        "idx_to_book = {idx: book_id for book_id, idx in book_to_idx.items()}\n",
        "\n",
        "print(f\"Created mappings for {len(book_to_idx)} books\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K__M4plQ-Pt"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Build User Profiles (with memory optimization)\n",
        "print(\"Building user profiles...\")\n",
        "\n",
        "user_profiles = {}\n",
        "total_users = len(train_df['user_id'].unique())\n",
        "processed = 0\n",
        "\n",
        "# Process users in batches to show progress\n",
        "for user_id in train_df['user_id'].unique():\n",
        "    processed += 1\n",
        "    if processed % 5000 == 0:\n",
        "        print(f\"Processed {processed}/{total_users} users\")\n",
        "        print_memory_usage()\n",
        "        # Force garbage collection every 5000 users\n",
        "        gc.collect()\n",
        "\n",
        "    user_books_list = train_df[train_df['user_id'] == user_id]['book_id'].values\n",
        "\n",
        "    # Get feature vectors for user's books\n",
        "    book_indices = [book_to_idx[bid] for bid in user_books_list if bid in book_to_idx]\n",
        "\n",
        "    if book_indices:\n",
        "        # Average of book features - USING float32 TO SAVE MEMORY\n",
        "        user_feature_vectors = item_features[book_indices]\n",
        "        user_profile = np.asarray(user_feature_vectors.mean(axis=0)).flatten().astype(np.float32)\n",
        "        user_profiles[user_id] = user_profile\n",
        "\n",
        "print(f\"Created profiles for {len(user_profiles)} users\")\n",
        "print(f\"Profile dimension: {list(user_profiles.values())[0].shape[0] if user_profiles else 0}\")\n",
        "print_memory_usage()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU-AH6sbQ_tH"
      },
      "outputs": [],
      "source": [
        "# Cell 10: Define Recommendation Function\n",
        "def get_recommendations(user_id, user_profile, item_features, book_to_idx,\n",
        "                        train_books_set, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate top-k recommendations for a user\n",
        "    \"\"\"\n",
        "    # Calculate similarity between user profile and all items\n",
        "    user_profile_reshaped = user_profile.reshape(1, -1)\n",
        "    similarities = cosine_similarity(user_profile_reshaped, item_features)[0]\n",
        "\n",
        "    # Get top-k items\n",
        "    top_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    # Filter out books already interacted with\n",
        "    recommendations = []\n",
        "    for idx in top_indices:\n",
        "        book_id = idx_to_book[idx]\n",
        "        if book_id not in train_books_set:\n",
        "            recommendations.append((book_id, similarities[idx]))\n",
        "            if len(recommendations) >= top_k:\n",
        "                break\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "print(\"Recommendation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiwQRfh0RBAM"
      },
      "outputs": [],
      "source": [
        "# Cell 11: Define Evaluation Metrics\n",
        "def hit_at_k(recommended, actual, k):\n",
        "    \"\"\"Hit@K: 1 if any recommended item in top-k is in actual, else 0\"\"\"\n",
        "    recommended_k = set([item[0] for item in recommended[:k]])\n",
        "    actual_set = set(actual)\n",
        "    return 1.0 if len(recommended_k & actual_set) > 0 else 0.0\n",
        "\n",
        "def mrr(recommended, actual):\n",
        "    \"\"\"Mean Reciprocal Rank\"\"\"\n",
        "    actual_set = set(actual)\n",
        "    for i, (item, score) in enumerate(recommended):\n",
        "        if item in actual_set:\n",
        "            return 1.0 / (i + 1)\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(recommended, actual, k):\n",
        "    \"\"\"Normalized Discounted Cumulative Gain at K\"\"\"\n",
        "    recommended_k = [item[0] for item in recommended[:k]]\n",
        "    actual_set = set(actual)\n",
        "\n",
        "    dcg = 0.0\n",
        "    for i, item in enumerate(recommended_k):\n",
        "        if item in actual_set:\n",
        "            dcg += 1.0 / np.log2(i + 2)  # +2 because i is 0-indexed\n",
        "\n",
        "    # Ideal DCG (if all actual items were at top)\n",
        "    idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(actual), k))])\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "print(\"Evaluation metric functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DJGx4w2RCNI"
      },
      "outputs": [],
      "source": [
        "# Cell 12: Validation Set Evaluation\n",
        "print(\"Starting validation evaluation...\")\n",
        "\n",
        "# Metrics storage for validation\n",
        "val_metrics = {\n",
        "    'hit@5': [],\n",
        "    'hit@10': [],\n",
        "    'hit@50': [],\n",
        "    'mrr': [],\n",
        "    'ndcg@10': [],\n",
        "    'ndcg@50': []\n",
        "}\n",
        "\n",
        "# Get validation users\n",
        "val_users_list = val_df['user_id'].unique()\n",
        "print(f\"Total validation users: {len(val_users_list)}\")\n",
        "\n",
        "# Filter to only validation users who also appear in training (so they have profiles)\n",
        "val_users_with_profiles = [u for u in val_users_list if u in user_profiles]\n",
        "print(f\"Validation users with training profiles: {len(val_users_with_profiles)}\")\n",
        "\n",
        "if len(val_users_with_profiles) > 0:\n",
        "    print(f\"Evaluating on {len(val_users_with_profiles)} validation users...\")\n",
        "\n",
        "    # Evaluation on validation set - with batch processing for progress\n",
        "    batch_size_val = 500\n",
        "    for batch_start in range(0, len(val_users_with_profiles), batch_size_val):\n",
        "        batch_end = min(batch_start + batch_size_val, len(val_users_with_profiles))\n",
        "        batch_users = val_users_with_profiles[batch_start:batch_end]\n",
        "\n",
        "        print(f\"Processing validation users {batch_start+1}-{batch_end}/{len(val_users_with_profiles)}...\")\n",
        "\n",
        "        for user_id in batch_users:\n",
        "            # Get user's training books (to exclude from recommendations)\n",
        "            train_books_set = set(train_df[train_df['user_id'] == user_id]['book_id'].values)\n",
        "\n",
        "            # Get user's validation books (ground truth)\n",
        "            val_books = val_df[val_df['user_id'] == user_id]['book_id'].values\n",
        "\n",
        "            if len(val_books) == 0:\n",
        "                continue\n",
        "\n",
        "            # Get recommendations\n",
        "            user_profile = user_profiles[user_id]\n",
        "            recommendations = get_recommendations(\n",
        "                user_id, user_profile, item_features, book_to_idx,\n",
        "                train_books_set, top_k=50\n",
        "            )\n",
        "\n",
        "            # Calculate metrics\n",
        "            val_metrics['hit@5'].append(hit_at_k(recommendations, val_books, 5))\n",
        "            val_metrics['hit@10'].append(hit_at_k(recommendations, val_books, 10))\n",
        "            val_metrics['hit@50'].append(hit_at_k(recommendations, val_books, 50))\n",
        "            val_metrics['mrr'].append(mrr(recommendations, val_books))\n",
        "            val_metrics['ndcg@10'].append(ndcg_at_k(recommendations, val_books, 10))\n",
        "            val_metrics['ndcg@50'].append(ndcg_at_k(recommendations, val_books, 50))\n",
        "\n",
        "        # Clean memory after each batch\n",
        "        clean_memory()\n",
        "\n",
        "    print(f\"\\nValidation evaluation complete! Evaluated {len(val_metrics['hit@5'])} users.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No validation users have training profiles. Skipping validation evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM0_LjS3RDke"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Display Validation Results\n",
        "# Calculate average validation metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SET - EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Total Books: {len(all_books)}\")\n",
        "print(f\"  Training Users: {len(user_profiles)}\")\n",
        "print(f\"  Validation Users: {len(val_users_list)}\")\n",
        "print(f\"  Training Interactions: {len(train_df)}\")\n",
        "print(f\"  Validation Interactions: {len(val_df)}\")\n",
        "\n",
        "if len(val_metrics['hit@5']) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No validation metrics available.\")\n",
        "    print(\"Skipping to test set evaluation...\")\n",
        "else:\n",
        "    print(f\"  Evaluated Users: {len(val_metrics['hit@5'])}\")\n",
        "\n",
        "    print(f\"\\n{'Metric':<15} {'Score':<10}\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"{'Hit@5':<15} {np.mean(val_metrics['hit@5']):.4f}\")\n",
        "    print(f\"{'Hit@10':<15} {np.mean(val_metrics['hit@10']):.4f}\")\n",
        "    print(f\"{'Hit@50':<15} {np.mean(val_metrics['hit@50']):.4f}\")\n",
        "    print(f\"{'MRR':<15} {np.mean(val_metrics['mrr']):.4f}\")\n",
        "    print(f\"{'NDCG@10':<15} {np.mean(val_metrics['ndcg@10']):.4f}\")\n",
        "    print(f\"{'NDCG@50':<15} {np.mean(val_metrics['ndcg@50']):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Additional statistics\n",
        "    print(f\"\\nüìä Validation Hit Coverage:\")\n",
        "    users_hit5 = sum([1 for x in val_metrics['hit@5'] if x > 0])\n",
        "    users_hit10 = sum([1 for x in val_metrics['hit@10'] if x > 0])\n",
        "    users_hit50 = sum([1 for x in val_metrics['hit@50'] if x > 0])\n",
        "    total_val_eval = len(val_metrics['hit@5'])\n",
        "\n",
        "    print(f\"  ‚Ä¢ Users with Hit@5: {users_hit5} ({users_hit5/total_val_eval*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@10: {users_hit10} ({users_hit10/total_val_eval*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@50: {users_hit50} ({users_hit50/total_val_eval*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yT9Jca9RFHK"
      },
      "outputs": [],
      "source": [
        "# Cell 14: Test Set Evaluation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting test evaluation...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Metrics storage for test\n",
        "test_metrics = {\n",
        "    'hit@5': [],\n",
        "    'hit@10': [],\n",
        "    'hit@50': [],\n",
        "    'mrr': [],\n",
        "    'ndcg@10': [],\n",
        "    'ndcg@50': []\n",
        "}\n",
        "\n",
        "# Get test users\n",
        "test_users_list = test_df['user_id'].unique()\n",
        "print(f\"Total test users: {len(test_users_list)}\")\n",
        "\n",
        "# Filter to only test users who also appear in training (so they have profiles)\n",
        "test_users_with_profiles = [u for u in test_users_list if u in user_profiles]\n",
        "print(f\"Test users with training profiles: {len(test_users_with_profiles)}\")\n",
        "\n",
        "if len(test_users_with_profiles) > 0:\n",
        "    print(f\"Evaluating on {len(test_users_with_profiles)} test users...\")\n",
        "\n",
        "    # Evaluation on test set - with batch processing\n",
        "    batch_size_test = 500\n",
        "    for batch_start in range(0, len(test_users_with_profiles), batch_size_test):\n",
        "        batch_end = min(batch_start + batch_size_test, len(test_users_with_profiles))\n",
        "        batch_users = test_users_with_profiles[batch_start:batch_end]\n",
        "\n",
        "        print(f\"Processing test users {batch_start+1}-{batch_end}/{len(test_users_with_profiles)}...\")\n",
        "\n",
        "        for user_id in batch_users:\n",
        "            # Get user's training books (to exclude from recommendations)\n",
        "            train_books_set = set(train_df[train_df['user_id'] == user_id]['book_id'].values)\n",
        "\n",
        "            # Get user's test books (ground truth)\n",
        "            test_books = test_df[test_df['user_id'] == user_id]['book_id'].values\n",
        "\n",
        "            if len(test_books) == 0:\n",
        "                continue\n",
        "\n",
        "            # Get recommendations\n",
        "            user_profile = user_profiles[user_id]\n",
        "            recommendations = get_recommendations(\n",
        "                user_id, user_profile, item_features, book_to_idx,\n",
        "                train_books_set, top_k=50\n",
        "            )\n",
        "\n",
        "            # Calculate metrics\n",
        "            test_metrics['hit@5'].append(hit_at_k(recommendations, test_books, 5))\n",
        "            test_metrics['hit@10'].append(hit_at_k(recommendations, test_books, 10))\n",
        "            test_metrics['hit@50'].append(hit_at_k(recommendations, test_books, 50))\n",
        "            test_metrics['mrr'].append(mrr(recommendations, test_books))\n",
        "            test_metrics['ndcg@10'].append(ndcg_at_k(recommendations, test_books, 10))\n",
        "            test_metrics['ndcg@50'].append(ndcg_at_k(recommendations, test_books, 50))\n",
        "\n",
        "        # Clean memory after each batch\n",
        "        clean_memory()\n",
        "\n",
        "    print(f\"\\nTest evaluation complete! Evaluated {len(test_metrics['hit@5'])} users.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No test users have training profiles.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE3bqkAKRHbK"
      },
      "outputs": [],
      "source": [
        "# Cell 15: Display Test Results\n",
        "# Calculate average test metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST SET - EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Total Books: {len(all_books)}\")\n",
        "print(f\"  Training Users: {len(user_profiles)}\")\n",
        "print(f\"  Test Users: {len(test_users_list)}\")\n",
        "print(f\"  Training Interactions: {len(train_df)}\")\n",
        "print(f\"  Test Interactions: {len(test_df)}\")\n",
        "\n",
        "if len(test_metrics['hit@5']) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No test metrics available.\")\n",
        "else:\n",
        "    print(f\"  Evaluated Users: {len(test_metrics['hit@5'])}\")\n",
        "\n",
        "    print(f\"\\n{'Metric':<15} {'Score':<10}\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"{'Hit@5':<15} {np.mean(test_metrics['hit@5']):.4f}\")\n",
        "    print(f\"{'Hit@10':<15} {np.mean(test_metrics['hit@10']):.4f}\")\n",
        "    print(f\"{'Hit@50':<15} {np.mean(test_metrics['hit@50']):.4f}\")\n",
        "    print(f\"{'MRR':<15} {np.mean(test_metrics['mrr']):.4f}\")\n",
        "    print(f\"{'NDCG@10':<15} {np.mean(test_metrics['ndcg@10']):.4f}\")\n",
        "    print(f\"{'NDCG@50':<15} {np.mean(test_metrics['ndcg@50']):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Additional statistics\n",
        "    print(f\"\\nüìä Test Hit Coverage:\")\n",
        "    users_hit5 = sum([1 for x in test_metrics['hit@5'] if x > 0])\n",
        "    users_hit10 = sum([1 for x in test_metrics['hit@10'] if x > 0])\n",
        "    users_hit50 = sum([1 for x in test_metrics['hit@50'] if x > 0])\n",
        "    total_test_eval = len(test_metrics['hit@5'])\n",
        "\n",
        "    print(f\"  ‚Ä¢ Users with Hit@5: {users_hit5} ({users_hit5/total_test_eval*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@10: {users_hit10} ({users_hit10/total_test_eval*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@50: {users_hit50} ({users_hit50/total_test_eval*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgcC8D2XRJeQ"
      },
      "outputs": [],
      "source": [
        "# Cell 16: Demonstration - Show Recommendations\n",
        "print(\"=\"*80)\n",
        "print(\"DEMONSTRATION: SHOWING ACTUAL RECOMMENDATIONS VS GROUND TRUTH (TEST SET)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select a few random test users to demonstrate\n",
        "if len(test_users_with_profiles) > 0:\n",
        "    np.random.seed(42)\n",
        "    demo_users = np.random.choice(test_users_with_profiles, min(5, len(test_users_with_profiles)), replace=False)\n",
        "\n",
        "    for user_idx, user_id in enumerate(demo_users):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"USER {user_idx + 1}: {user_id}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Get user's training books\n",
        "        train_books_list = train_df[train_df['user_id'] == user_id]['book_id'].values\n",
        "        print(f\"\\nüìö TRAINING BOOKS (what user read): {len(train_books_list)} books\")\n",
        "        for i, book_id in enumerate(train_books_list[:3]):  # Show first 3\n",
        "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
        "            print(f\"  {i+1}. {book_info['book_title']}\")\n",
        "        if len(train_books_list) > 3:\n",
        "            print(f\"  ... and {len(train_books_list) - 3} more\")\n",
        "\n",
        "        # Get user's test books (ground truth)\n",
        "        test_books_list = test_df[test_df['user_id'] == user_id]['book_id'].values\n",
        "        print(f\"\\nüéØ GROUND TRUTH (actual test books): {len(test_books_list)} books\")\n",
        "        for i, book_id in enumerate(test_books_list):\n",
        "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
        "            print(f\"  {i+1}. {book_info['book_title']}\")\n",
        "\n",
        "        # Get recommendations\n",
        "        user_profile = user_profiles[user_id]\n",
        "        train_books_set = set(train_books_list)\n",
        "        recommendations = get_recommendations(\n",
        "            user_id, user_profile, item_features, book_to_idx,\n",
        "            train_books_set, top_k=10\n",
        "        )\n",
        "\n",
        "        # Display top-10 recommendations\n",
        "        print(f\"\\nüí° TOP-10 RECOMMENDATIONS:\")\n",
        "        hits = []\n",
        "        for i, (book_id, score) in enumerate(recommendations[:10]):\n",
        "            book_info = df_books[df_books['book_id'] == book_id].iloc[0]\n",
        "            is_hit = \"‚úì HIT!\" if book_id in test_books_list else \"\"\n",
        "            print(f\"  {i+1}. {book_info['book_title'][:60]:60s} (score: {score:.4f}) {is_hit}\")\n",
        "            if book_id in test_books_list:\n",
        "                hits.append(i+1)\n",
        "\n",
        "        # Show metrics for this user\n",
        "        hit10 = 1.0 if len(hits) > 0 else 0.0\n",
        "        mrr_val = (1.0 / hits[0]) if hits else 0.0\n",
        "\n",
        "        print(f\"\\nüìä USER METRICS:\")\n",
        "        print(f\"  ‚Ä¢ Hit@10: {hit10}\")\n",
        "        print(f\"  ‚Ä¢ MRR: {mrr_val:.4f}\")\n",
        "        if hits:\n",
        "            print(f\"  ‚Ä¢ Hits at positions: {hits}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No test users available for demonstration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFTerXA-RSW2"
      },
      "outputs": [],
      "source": [
        "# Cell 17: Overall Performance Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OVERALL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if len(val_metrics.get('hit@5', [])) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VALIDATION SET SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Validation statistics\n",
        "    users_with_hit5_val = sum([1 for x in val_metrics['hit@5'] if x > 0])\n",
        "    users_with_hit10_val = sum([1 for x in val_metrics['hit@10'] if x > 0])\n",
        "    users_with_hit50_val = sum([1 for x in val_metrics['hit@50'] if x > 0])\n",
        "    total_val_users = len(val_metrics['hit@5'])\n",
        "\n",
        "    print(f\"\\nüìä VALIDATION HIT COVERAGE STATISTICS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  ‚Ä¢ Total Validation Users: {total_val_users}\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@5: {users_with_hit5_val} ({users_with_hit5_val/total_val_users*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@10: {users_with_hit10_val} ({users_with_hit10_val/total_val_users*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@50: {users_with_hit50_val} ({users_with_hit50_val/total_val_users*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüìà VALIDATION AVERAGE METRICS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  ‚Ä¢ Hit@5:    {np.mean(val_metrics['hit@5']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ Hit@10:   {np.mean(val_metrics['hit@10']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ Hit@50:   {np.mean(val_metrics['hit@50']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ MRR:      {np.mean(val_metrics['mrr']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ NDCG@10:  {np.mean(val_metrics['ndcg@10']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ NDCG@50:  {np.mean(val_metrics['ndcg@50']):.4f}\")\n",
        "\n",
        "if len(test_metrics.get('hit@5', [])) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TEST SET SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Test statistics\n",
        "    users_with_hit5_test = sum([1 for x in test_metrics['hit@5'] if x > 0])\n",
        "    users_with_hit10_test = sum([1 for x in test_metrics['hit@10'] if x > 0])\n",
        "    users_with_hit50_test = sum([1 for x in test_metrics['hit@50'] if x > 0])\n",
        "    total_test_users = len(test_metrics['hit@5'])\n",
        "\n",
        "    print(f\"\\nüìä TEST HIT COVERAGE STATISTICS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  ‚Ä¢ Total Test Users: {total_test_users}\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@5: {users_with_hit5_test} ({users_with_hit5_test/total_test_users*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@10: {users_with_hit10_test} ({users_with_hit10_test/total_test_users*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Users with Hit@50: {users_with_hit50_test} ({users_with_hit50_test/total_test_users*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüìà TEST AVERAGE METRICS:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  ‚Ä¢ Hit@5:    {np.mean(test_metrics['hit@5']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ Hit@10:   {np.mean(test_metrics['hit@10']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ Hit@50:   {np.mean(test_metrics['hit@50']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ MRR:      {np.mean(test_metrics['mrr']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ NDCG@10:  {np.mean(test_metrics['ndcg@10']):.4f}\")\n",
        "    print(f\"  ‚Ä¢ NDCG@50:  {np.mean(test_metrics['ndcg@50']):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ EVALUATION COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}